{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1592528591681},{"_id":"source/.DS_Store","hash":"405ea0049338e59584d21a004b47ffa859b6cf05","modified":1597175485378},{"_id":"themes/next/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1592530545778},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1592528591681},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1592528591684},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1592528591684},{"_id":"themes/next/_config.yml","hash":"b8fd32f8713aced3dba4bfb1eb1314a0c13d745f","modified":1597175569142},{"_id":"themes/next/README.md","hash":"9b4b7d66aca47f9c65d6321b14eef48d95c4dff1","modified":1592528591684},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1592528591685},{"_id":"themes/next/package.json","hash":"62fad6de02adbbba9fb096cbe2dcc15fe25f2435","modified":1592528591717},{"_id":"themes/next/gulpfile.js","hash":"1b4fc262b89948937b9e3794de812a7c1f2f3592","modified":1592528591689},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1592528591684},{"_id":"source/_data/footer.swig","hash":"1909878f0e1d4049c00a186121115ddaa3fc7432","modified":1597175534398},{"_id":"source/_posts/agile.md","hash":"761c3302fbcbb59f60364fd8b74621d5a12ce1ac","modified":1596639594069},{"_id":"source/_data/styles.styl","hash":"3745396338b9cad38e65bddae6fd3c0f7730e5f3","modified":1597175247575},{"_id":"source/_drafts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1590269503016},{"_id":"source/_posts/.DS_Store","hash":"73eb1a16090ab5148480d3ef6c784a388e9d2d7c","modified":1596640210153},{"_id":"source/_posts/helloworld.md","hash":"b614e5d915d981b84fe8e8fd2efda5b65965d4d5","modified":1596639885431},{"_id":"source/_posts/neuralnet.md","hash":"fd19ef49629db7b41086f35cd03cc391a41698ee","modified":1595989639353},{"_id":"source/_posts/online-resources.md","hash":"8a6308e6533cc2413ccb84a40abef2aae0ad313a","modified":1596640323682},{"_id":"source/_posts/project-ideas.md","hash":"4bca32d2468c7965c09904eb2a144b9bea89283f","modified":1596638539354},{"_id":"source/_posts/rl-bandit.md","hash":"579b552fab8cae02a1f98405e50137de444e8207","modified":1596639771622},{"_id":"source/_posts/interview_prep.md","hash":"827020dfb38c94938a21db3f55390717dbbfe90f","modified":1596639686160},{"_id":"source/_posts/rl-mdp.md","hash":"4ffc94c5825788961e0c1a395a91ec5606dd3d85","modified":1596639781676},{"_id":"source/_posts/香蕉.md","hash":"3c1df6a5409d1bffc018146584c6b00d458473a2","modified":1596639898294},{"_id":"source/_posts/杨绛语录.md","hash":"5a3b087aebd10df2a08bd57b085369dd18184ff2","modified":1596639891545},{"_id":"source/_posts/rl-env.md","hash":"7420f9b0b792b0b4650b614b795c00e4d0697b33","modified":1594325061469},{"_id":"source/_posts/rl-policy-valuefunct.md","hash":"18366ae3fb6e9a65fb15f0cf9988aa4878293278","modified":1596639798458},{"_id":"source/tags/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1592525543970},{"_id":"source/categories/index.md","hash":"4bd11cfc2178308802d27a9e00ff4e38a7ec64cb","modified":1595266527254},{"_id":"source/about/index.md","hash":"e1101f55b65fd073f032ea53186a0a99308ac6a3","modified":1595266436745},{"_id":"source/categories/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1592526931488},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1592528591686},{"_id":"themes/next/docs/DATA-FILES.md","hash":"cddbdc91ee9e65c37a50bec12194f93d36161616","modified":1592528591686},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1592528591686},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"94dc3404ccb0e5f663af2aa883c1af1d6eae553d","modified":1592528591686},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1592528591686},{"_id":"themes/next/docs/MATH.md","hash":"d645b025ec7fb9fbf799b9bb76af33b9f5b9ed93","modified":1592528591687},{"_id":"themes/next/languages/ar.yml","hash":"9815e84e53d750c8bcbd9193c2d44d8d910e3444","modified":1592528591689},{"_id":"themes/next/languages/de.yml","hash":"74c59f2744217003b717b59d96e275b54635abf5","modified":1592528591690},{"_id":"themes/next/languages/default.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1592528591690},{"_id":"themes/next/languages/en.yml","hash":"45bc5118828bdc72dcaa25282cd367c8622758cb","modified":1592528591690},{"_id":"themes/next/languages/es.yml","hash":"c64cf05f356096f1464b4b1439da3c6c9b941062","modified":1592528591690},{"_id":"themes/next/languages/fa.yml","hash":"3676b32fda37e122f3c1a655085a1868fb6ad66b","modified":1592528591690},{"_id":"themes/next/languages/fr.yml","hash":"752bf309f46a2cd43890b82300b342d7218d625f","modified":1592528591690},{"_id":"themes/next/languages/hu.yml","hash":"b1ebb77a5fd101195b79f94de293bcf9001d996f","modified":1592528591690},{"_id":"themes/next/languages/it.yml","hash":"44759f779ce9c260b895532de1d209ad4bd144bf","modified":1592528591691},{"_id":"themes/next/languages/ja.yml","hash":"0cf0baa663d530f22ff380a051881216d6adcdd8","modified":1592528591691},{"_id":"themes/next/languages/id.yml","hash":"572ed855d47aafe26f58c73b1394530754881ec2","modified":1592528591690},{"_id":"themes/next/languages/ko.yml","hash":"0feea9e43cd399f3610b94d755a39fff1d371e97","modified":1592528591691},{"_id":"themes/next/languages/nl.yml","hash":"5af3473d9f22897204afabc08bb984b247493330","modified":1592528591691},{"_id":"themes/next/languages/pt-BR.yml","hash":"67555b1ba31a0242b12fc6ce3add28531160e35b","modified":1592528591692},{"_id":"themes/next/languages/pt.yml","hash":"718d131f42f214842337776e1eaddd1e9a584054","modified":1592528591692},{"_id":"themes/next/languages/ru.yml","hash":"e993d5ca072f7f6887e30fc0c19b4da791ca7a88","modified":1592528591692},{"_id":"themes/next/languages/tr.yml","hash":"fe793f4c2608e3f85f0b872fd0ac1fb93e6155e2","modified":1592528591692},{"_id":"themes/next/languages/uk.yml","hash":"3a6d635b1035423b22fc86d9455dba9003724de9","modified":1592528591692},{"_id":"themes/next/languages/vi.yml","hash":"93393b01df148dcbf0863f6eee8e404e2d94ef9e","modified":1592528591693},{"_id":"themes/next/languages/zh-CN.yml","hash":"a1f15571ee7e1e84e3cc0985c3ec4ba1a113f6f8","modified":1592528591693},{"_id":"themes/next/languages/zh-HK.yml","hash":"3789f94010f948e9f23e21235ef422a191753c65","modified":1592528591693},{"_id":"themes/next/languages/zh-TW.yml","hash":"8c09da7c4ec3fca2c6ee897b2eea260596a2baa1","modified":1592528591693},{"_id":"themes/next/layout/_layout.swig","hash":"6a6e92a4664cdb981890a27ac11fd057f44de1d5","modified":1592528591694},{"_id":"themes/next/layout/archive.swig","hash":"e4e31317a8df68f23156cfc49e9b1aa9a12ad2ed","modified":1592528591716},{"_id":"themes/next/layout/category.swig","hash":"1bde61cf4d2d171647311a0ac2c5c7933f6a53b0","modified":1592528591716},{"_id":"themes/next/layout/index.swig","hash":"7f403a18a68e6d662ae3e154b2c1d3bbe0801a23","modified":1592528591716},{"_id":"themes/next/layout/page.swig","hash":"db581bdeac5c75fabb0f17d7c5e746e47f2a9168","modified":1592528591716},{"_id":"themes/next/layout/post.swig","hash":"2f6d992ced7e067521fdce05ffe4fd75481f41c5","modified":1592528591717},{"_id":"themes/next/layout/tag.swig","hash":"0dfb653bd5de980426d55a0606d1ab122bd8c017","modified":1592528591717},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"8b6e4b2c9cfcb969833092bdeaed78534082e3e6","modified":1592528591687},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"c7a994b9542040317d8f99affa1405c143a94a38","modified":1592528591685},{"_id":"source/tags/index.md","hash":"989e97f90403764bece2987e20461f522eb2d60e","modified":1595266465946},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1592528591727},{"_id":"source/_posts/rl-bandit/1equation.png","hash":"6c5270d865056b0b1e9c2bd3ee5a7af2bf7ab194","modified":1592101026691},{"_id":"source/_posts/rl-bandit/4.png","hash":"03fbd816e0da30558fe92403c1701441517c156f","modified":1592104937129},{"_id":"source/_posts/rl-bandit/5.png","hash":"04b713163573340981da8cb2a57f942e8921687d","modified":1592105348248},{"_id":"source/_posts/rl-bandit/7.png","hash":"2ae92891f17acb989e55841c490d1d1fd4e8810c","modified":1592106331473},{"_id":"source/_posts/rl-bandit/6.png","hash":"dfa6a7fa02f4a3c5537d9caaf038f6c6a47df574","modified":1592106252253},{"_id":"source/_posts/rl-bandit/8.png","hash":"836b4a882890e7716e3af2d9cd6ea2d648186db0","modified":1592107385058},{"_id":"source/_posts/rl-mdp/G1.png","hash":"3d0e21a231ed7e2ed08559136d6c9914cd6fe2f7","modified":1592853157330},{"_id":"source/_posts/rl-mdp/G0.png","hash":"5b2ae6d1882b509cf301c858e43c960cdf555611","modified":1592853171529},{"_id":"source/_posts/rl-mdp/mdp1.png","hash":"95e11b1a208370cbf543813eedf5b8614a991271","modified":1592515010212},{"_id":"source/_posts/rl-mdp/mdp0.png","hash":"e1344516fd512ea0975e78afb52b951a5ac2d0a4","modified":1592515261523},{"_id":"source/_posts/rl-policy-valuefunct/actionvalue.png","hash":"70caea98d9d3df40a783f8beadf9f99d8b60220e","modified":1592875369306},{"_id":"source/_posts/rl-policy-valuefunct/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1593485560637},{"_id":"source/_posts/rl-policy-valuefunct/bellman.png","hash":"71ab9e5971d2ed03581e9280156c7040ad97f101","modified":1592880255069},{"_id":"source/_posts/rl-policy-valuefunct/optimal_graphical.png","hash":"cbec917cb0250b3f6f502e8c4661054b36f40bdf","modified":1593485865515},{"_id":"source/_posts/rl-policy-valuefunct/optimalq_givenv.png","hash":"ecd7de9d386fca7025c9f4ca7a09c3b13db8b321","modified":1593485239746},{"_id":"source/_posts/rl-policy-valuefunct/optimalq.png","hash":"803cf3908c07149f2995883d4b9d4c699216ab80","modified":1593485185078},{"_id":"source/_posts/rl-policy-valuefunct/q.png","hash":"8b1cad6fefb618927eba2db8740b62be65fa1c27","modified":1592963887357},{"_id":"source/_posts/rl-policy-valuefunct/statevalue.png","hash":"30423627e983aadb63b751122930908c88113eb4","modified":1592875350170},{"_id":"source/_posts/rl-policy-valuefunct/tree.png","hash":"f3a8c5b84433d9918bf5d3565e598480c9406c20","modified":1592942117296},{"_id":"source/_posts/rl-policy-valuefunct/v.png","hash":"0e8daa229129287743f99ff600beb1645c8189e2","modified":1592963882110},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1592528591685},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1592528591687},{"_id":"themes/next/docs/ru/README.md","hash":"85dd68ed1250897a8e4a444a53a68c1d49eb7e11","modified":1592528591687},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1592528591687},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"5237a368ab99123749d724b6c379415f2c142a96","modified":1592528591687},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1592528591688},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"d3f03be036b75dc71cf3c366cd75aee7c127c874","modified":1592528591688},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1592528591688},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1592528591688},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"8b18f84503a361fc712b0fe4d4568e2f086ca97d","modified":1592528591689},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"b92585d251f1f9ebe401abb5d932cb920f9b8b10","modified":1592528591689},{"_id":"themes/next/docs/zh-CN/README.md","hash":"c038629ff8f3f24e8593c4c8ecf0bef3a35c750d","modified":1592528591689},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"d9ce7331c1236bbe0a551d56cef2405e47e65325","modified":1592528591689},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"9c8dc0b8170679cdc1ee9ee8dbcbaebf3f42897b","modified":1592528591694},{"_id":"themes/next/layout/_macro/post.swig","hash":"090b5a9b6fca8e968178004cbd6cff205b7eba57","modified":1592528591694},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"71655ca21907e9061b6e8ac52d0d8fbf54d0062b","modified":1592528591694},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"34b88784ec120dfdc20fa82aadeb5f64ef614d14","modified":1592528591688},{"_id":"themes/next/layout/_partials/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1597175480375},{"_id":"themes/next/layout/_partials/footer.swig","hash":"e5a2bdf45fde7cea9cb03993f5a0fd960326ed5d","modified":1597175448228},{"_id":"themes/next/layout/_partials/comments.swig","hash":"db6ab5421b5f4b7cb32ac73ad0e053fdf065f83e","modified":1592528591695},{"_id":"themes/next/layout/_partials/languages.swig","hash":"ba9e272f1065b8f0e8848648caa7dea3f02c6be1","modified":1592528591697},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1592528591698},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1592528591700},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1592528591701},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1592528591701},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"4d2c93c66e069852bb0e3ea2e268d213d07bfa3f","modified":1592528591701},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1592528591702},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1592528591703},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"b782eb2e34c0c15440837040b5d65b093ab6ec04","modified":1592528591704},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1592528591706},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"311e5eceec9e949f1ea8d623b083cec0b8700ff2","modified":1592528591707},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"2731e262a6b88eaee2a3ca61e6a3583a7f594702","modified":1592528591708},{"_id":"themes/next/scripts/events/index.js","hash":"5743cde07f3d2aa11532a168a652e52ec28514fd","modified":1592528591717},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1592528591720},{"_id":"themes/next/scripts/filters/locals.js","hash":"b193a936ee63451f09f8886343dcfdca577c0141","modified":1592528591720},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1592528591720},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1592528591721},{"_id":"themes/next/scripts/filters/post.js","hash":"44ba9b1c0bdda57590b53141306bb90adf0678db","modified":1592528591722},{"_id":"themes/next/scripts/helpers/font.js","hash":"40cf00e9f2b7aa6e5f33d412e03ed10304b15fd7","modified":1592528591723},{"_id":"themes/next/scripts/helpers/engine.js","hash":"bdb424c3cc0d145bd0c6015bb1d2443c8a9c6cda","modified":1592528591722},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"5e11f30ddb5093a88a687446617a46b048fa02e5","modified":1592528591726},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"958e86b2bd24e4fdfcbf9ce73e998efe3491a71f","modified":1592528591727},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"94e0bbc7999b359baa42fa3731bdcf89c79ae2b3","modified":1592528591727},{"_id":"themes/next/scripts/tags/button.js","hash":"8c6b45f36e324820c919a822674703769e6da32c","modified":1592528591727},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f1826ade2d135e2f60e2d95cb035383685b3370c","modified":1592528591728},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1592528591728},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1592528591728},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1592528591728},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1592528591728},{"_id":"themes/next/scripts/tags/pdf.js","hash":"8c613b39e7bff735473e35244b5629d02ee20618","modified":1592528591728},{"_id":"themes/next/scripts/tags/tabs.js","hash":"93d8a734a3035c1d3f04933167b500517557ba3e","modified":1592528591729},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1592528591729},{"_id":"themes/next/source/css/_colors.styl","hash":"a8442520f719d3d7a19811cb3b85bcfd4a596e1f","modified":1592528591729},{"_id":"themes/next/source/css/_mixins.styl","hash":"e31a557f8879c2f4d8d5567ee1800b3e03f91f6e","modified":1592528591740},{"_id":"themes/next/source/css/main.styl","hash":"a3a3bbb5a973052f0186b3523911cb2539ff7b88","modified":1592528591746},{"_id":"themes/next/source/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1592528591750},{"_id":"themes/next/source/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1592528591750},{"_id":"themes/next/source/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1592528591750},{"_id":"themes/next/source/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1592528591751},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1592528591751},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1592528591747},{"_id":"themes/next/source/js/utils.js","hash":"2c6e6b4d9a592fbb4bf04689524db2cdfcd94ca7","modified":1592528591754},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1592528591747},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1592528591747},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1592528591748},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1592528591748},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1592528591748},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1592528591748},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1592528591748},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1592528591749},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1592528591749},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1592528591749},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1592528591749},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1592528591749},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1592528591755},{"_id":"themes/next/source/images/favicon.png","hash":"c7ddc4fa3ebddfbe12e7ec7ac2bf32de9de342ab","modified":1596661039146},{"_id":"source/_posts/rl-bandit/3.png","hash":"4113998bb1bec693a53abf0283192ba4daabfb9a","modified":1592103464839},{"_id":"source/_posts/rl-mdp/mdp2.png","hash":"cf4849c5d825b2596de6da3e37aa90aead9ca1f3","modified":1592514990182},{"_id":"source/_posts/rl-policy-valuefunct/computingV1.png","hash":"0e8a0111f9ef2dc4d4a0442f4b2b1256127ad971","modified":1592964911269},{"_id":"source/_posts/rl-policy-valuefunct/computingV2.png","hash":"df0e191bda5f3a34b1130540319fb045b61a2307","modified":1592964943210},{"_id":"source/_posts/rl-policy-valuefunct/optimalv.png","hash":"397f9d44a837e789c5f373da0a2aeac07b60d73d","modified":1593485173188},{"_id":"source/_posts/rl-bandit/2actionvalue.png","hash":"91fb5b769468bfc5ed214dbf527b46122ee51766","modified":1592103383621},{"_id":"source/_posts/agile/Scrum-Diagram-JordanJob.me.png","hash":"12c00e2cf565f7d9bc7b6cae8de4c9fda22f8e01","modified":1595388807301},{"_id":"source/_posts/rl-policy-valuefunct/optimalpolicy.png","hash":"9c7da23153d308f0ee94701ace24f4c70e21bdb0","modified":1593485568462},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"000bad572d76ee95d9c0a78f9ccdc8d97cc7d4b4","modified":1592528591695},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"c70f8e71e026e878a4e9d5ab3bbbf9b0b23c240c","modified":1592528591696},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"7dbe93b8297b746afb89700b4d29289556e85267","modified":1592528591696},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"d31f896680a6c2f2c3f5128b4d4dd46c87ce2130","modified":1592528591697},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1592528591698},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"9b7a66791d7822c52117fe167612265356512477","modified":1592528591698},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"ceba16b9bd3a0c5c8811af7e7e49d0f9dcb2f41e","modified":1592528591699},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"954ad71536b6eb08bd1f30ac6e2f5493b69d1c04","modified":1592528591698},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1592528591699},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1592528591699},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"2b1a73556595c37951e39574df5a3f20b2edeaef","modified":1592528591699},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"48430bd03b8f19c9b8cdb2642005ed67d56c6e0b","modified":1592528591699},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"2be50f9bfb1c56b85b3b6910a7df27f51143632c","modified":1592528591700},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"f48a6a8eba04eb962470ce76dd731e13074d4c45","modified":1592528591700},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"c46849e0af8f8fb78baccd40d2af14df04a074af","modified":1592528591700},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"077b5d66f6309f2e7dcf08645058ff2e03143e6c","modified":1592528591701},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1592528591702},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1592528591702},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1592528591702},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1592528591702},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1592528591703},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1592528591703},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1592528591704},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1592528591704},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"f910618292c63871ca2e6c6e66c491f344fa7b1f","modified":1592528591704},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1592528591704},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1592528591705},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1592528591705},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"82f5b6822aa5ec958aa987b101ef860494c6cf1f","modified":1592528591705},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"d6ceb70648555338a80ae5724b778c8c58d7060d","modified":1592528591705},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1592528591705},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"be0a8eccf1f6dc21154af297fc79555343031277","modified":1592528591706},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1592528591706},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1592528591706},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"ecf751321e799f0fb3bf94d049e535130e2547aa","modified":1592528591707},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"d35a999d67f4c302f76fdf13744ceef3c6506481","modified":1592528591709},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1592528591709},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1592528591713},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"b26ac2bfbe91dd88267f8b96aee6bb222b265b7a","modified":1592528591715},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1592528591715},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1592528591715},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"d56d5af427cdfecc33a0f62ee62c056b4e33d095","modified":1592528591715},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"d30b0e255a8092043bac46441243f943ed6fb09b","modified":1592528591716},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"f3c43664a071ff3c0b28bd7e59b5523446829576","modified":1592528591716},{"_id":"themes/next/scripts/events/lib/config.js","hash":"d34c6040b13649714939f59be5175e137de65ede","modified":1592528591718},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1592528591718},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1592528591718},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"4b1986e43d6abce13450d2b41a736dd6a5620a10","modified":1592528591714},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"a54708fd9309b4357c423a3730eb67f395344a5e","modified":1592528591718},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":1592528591718},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1592528591719},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"4c0c99c7e0f00849003dfce02a131104fb671137","modified":1592528591719},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1592528591719},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1592528591719},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1592528591720},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6cbd85f9433c06bae22225ccf75ac55e04f2d106","modified":1592528591720},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"f4e694e5db81e57442c7e34505a416d818b3044a","modified":1592528591745},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"f70be8e229da7e1715c11dd0e975a2e71e453ac8","modified":1592528591746},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"612ec843372dae709acb17112c1145a53450cc59","modified":1592528591746},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"62df49459d552bbf73841753da8011a1f5e875c8","modified":1592528591746},{"_id":"themes/next/source/css/_variables/base.styl","hash":"818508748b7a62e02035e87fe58e75b603ed56dc","modified":1592528591746},{"_id":"themes/next/source/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1592528591754},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1592528591754},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"810d544019e4a8651b756dd23e5592ee851eda71","modified":1592528591696},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"ae2261bea836581918a1c2b0d1028a78718434e0","modified":1592528591697},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"9440d8a3a181698b80e1fa47f5104f4565d8cdf3","modified":1592528591697},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1592528591759},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1592528591759},{"_id":"source/_posts/agile/diagram.JPG","hash":"00ea15f40cb978cd8e546f58b1c6e39482f37bf7","modified":1595987207514},{"_id":"source/_posts/rl-policy-valuefunct/grid.png","hash":"a8400d9249c20581ae2ba4f94b7ca00ac8492263","modified":1592880744617},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"ca5e70662dcfb261c25191cc5db5084dcf661c76","modified":1592528591729},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"8e7b57a72e757cf95278239641726bb2d5b869d1","modified":1592528591730},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"a47725574e1bee3bc3b63b0ff2039cc982b17eff","modified":1592528591730},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"2e3bf7baf383c9073ec5e67f157d3cb3823c0957","modified":1592528591733},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"681d33e3bc85bdca407d93b134c089264837378c","modified":1592528591735},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"a1690e035b505d28bdef2b4424c13fc6312ab049","modified":1592528591735},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"a2e9e00962e43e98ec2614d6d248ef1773bb9b78","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"0b2c4b78eead410020d7c4ded59c75592a648df8","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"b1f0fab7344a20ed6748b04065b141ad423cf4d9","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1592528591739},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1592528591739},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1592528591740},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"7785bd756e0c4acede3a47fec1ed7b55988385a5","modified":1592528591741},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1592528591739},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"bb7ace23345364eb14983e860a7172e1683a4c94","modified":1592528591741},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"f6516d0f7d89dc7b6c6e143a5af54b926f585d82","modified":1592528591741},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"18ce72d90459c9aa66910ac64eae115f2dde3767","modified":1592528591739},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"6136da4bbb7e70cec99f5c7ae8c7e74f5e7c261a","modified":1592528591742},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1592528591743},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"93db5dafe9294542a6b5f647643cb9deaced8e06","modified":1592528591744},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2b2e7b5cea7783c9c8bb92655e26a67c266886f0","modified":1592528591744},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1592528591744},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1592528591744},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"e282df938bd029f391c466168d0e68389978f120","modified":1592528591744},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"70a4324b70501132855b5e59029acfc5d3da1ebd","modified":1592528591745},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"85da2f3006f4bef9a2199416ecfab4d288f848c4","modified":1592528591745},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"44f47c88c06d89d06f220f102649057118715828","modified":1592528591745},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"e740deadcfc4f29c5cb01e40f9df6277262ba4e3","modified":1592528591745},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1592528591745},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1592528591742},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"f0131db6275ceaecae7e1a6a3798b8f89f6c850d","modified":1592528591743},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"7104b9cef90ca3b140d7a7afcf15540a250218fc","modified":1592528591741},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1592528591757},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1592528591757},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1592528591758},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1592528591757},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"d21d4ac1982c13d02f125a67c065412085a92ff2","modified":1592528591731},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"e771dcb0b4673e063c0f3e2d73e7336ac05bcd57","modified":1592528591731},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"e75693f33dbc92afc55489438267869ae2f3db54","modified":1592528591731},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"ded41fd9d20a5e8db66aaff7cc50f105f5ef2952","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"1e4190c10c9e0c9ce92653b0dbcec21754b0b69d","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"65cb6edb69e94e70e3291e9132408361148d41d5","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f49ca072b5a800f735e8f01fc3518f885951dd8e","modified":1592528591731},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"6a97bcfa635d637dc59005be3b931109e0d1ead5","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"d114b2a531129e739a27ba6271cfe6857aa9a865","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1592528591732},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1592528591733},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1592528591733},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"a760ee83ba6216871a9f14c5e56dc9bd0d9e2103","modified":1592528591733},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"902569a9dea90548bec21a823dd3efd94ff7c133","modified":1592528591731},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1592528591733},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1592528591733},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"9f0b93d109c9aec79450c8a0cf4a4eab717d674d","modified":1592528591734},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1592528591734},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"b49e9fbd3c182b8fc066b8c2caf248e3eb748619","modified":1592528591733},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"454a4aebfabb4469b92a8cbb49f46c49ac9bf165","modified":1592528591734},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e2d606f1ac343e9be4f15dbbaf3464bc4df8bf81","modified":1592528591734},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1592528591735},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"a793cfff86ad4af818faef04c18013077873f8f0","modified":1592528591734},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"e7a9fdb6478b8674b1cdf94de4f8052843fb71d9","modified":1592528591734},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"5f432a6ed9ca80a413c68b00e93d4a411abf280a","modified":1592528591735},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"45a239edca44acecf971d99b04f30a1aafbf6906","modified":1592528591735},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b2fc519828fe89a1f8f03ff7b809ad68cd46f3d7","modified":1592528591735},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1592528591736},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"fa0222197b5eee47e18ac864cdc6eac75678b8fe","modified":1592528591736},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"1f0e7fbe80956f47087c2458ea880acf7a83078b","modified":1592528591736},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"44487d9ab290dc97871fa8dd4487016deb56e123","modified":1592528591736},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1592528591737},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1592528591737},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"a05a4031e799bc864a4536f9ef61fe643cd421af","modified":1592528591737},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1592528591737},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"a9cd93c36bae5af9223e7804963096274e8a4f03","modified":1592528591737},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f71a3e86c05ea668b008cf05a81f67d92b6d65e4","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"35c871a809afa8306c8cde13651010e282548bc6","modified":1592528591738},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1592528591739},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"1d2778ca5aeeeafaa690dc2766b01b352ab76a02","modified":1592528591739},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1592528591739},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"d7fce4b51b5f4b7c31d93a9edb6c6ce740aa0d6b","modified":1592528591740},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"e4d9a77ffe98e851c1202676940097ba28253313","modified":1592528591740},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b49c64f8e9a6ca1c45c0ba98febf1974fdd03616","modified":1592528591740},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"f23670f1d8e749f3e83766d446790d8fd9620278","modified":1592528591740},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1592528591740},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1592528591730},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1592528591730},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1592528591730},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2a47f8a6bb589c2fb635e6c1e4a2563c7f63c407","modified":1592528591738},{"_id":"source/_posts/agile/Scrum-v2.0-GIF-1000px.gif","hash":"ec09136dca2258acfff659f3ef688b54dbdb88d8","modified":1595448418187},{"_id":"source/_posts/rl-bandit/summary.mp4","hash":"de13ff66ff018d234ead2a2bc57dd09716672fa3","modified":1592506925726},{"_id":"source/_posts/rl-bandit/RL.pdf","hash":"e11c618dda22c290315c27c7020b3d9f0a4b55cb","modified":1592506760400},{"_id":"public/about/index.html","hash":"8c425ca53544f3b4e63fd4ccf2c07cdaad558589","modified":1597175666847},{"_id":"public/categories/index.html","hash":"8c88b50ea2d1dfdd6756b4876a76e8430b2cc682","modified":1597175666847},{"_id":"public/2020/08/04/project-ideas/index.html","hash":"eb19eac8f10738d5f32c7abbf605472241737e0a","modified":1597175666847},{"_id":"public/tags/index.html","hash":"88f5140e7b1db3fd0a06d8da02c3245c2ea89dec","modified":1597175666847},{"_id":"public/2020/07/28/agile/index.html","hash":"18cde578d73208a439de5d456b256a9e3f2137dc","modified":1597175666847},{"_id":"public/2020/07/20/online-resources/index.html","hash":"3e0f45f0937b25130d5a5172635fde44c6234a73","modified":1597175666847},{"_id":"public/2020/07/13/interview_prep/index.html","hash":"ae0cdb0b5da1103dec04fa0c86d49ffcdf946b78","modified":1597175666847},{"_id":"public/2020/07/09/rl-env/index.html","hash":"e375c3edd00e43a13dc3ca037e8e821628d913d0","modified":1597175666847},{"_id":"public/2020/07/01/香蕉/index.html","hash":"81b37bf8a1d326cb15254d9606832740a6870f33","modified":1597175666847},{"_id":"public/2020/07/01/neuralnet/index.html","hash":"61759e4ecbe4b594291859c0a8b8971813e11667","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/index.html","hash":"cc3be6752f2fa4c14025159fe75f04851f0af978","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/index.html","hash":"3d46c58b2a7fd45fe7ccdd2f6f7afa061d7b007d","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/index.html","hash":"5ca9c86644eaece1f036945310036a220c49672e","modified":1597175666847},{"_id":"public/2020/05/27/杨绛语录/index.html","hash":"12939381f54cddb0363f1cdb074fe1b489ec3b0a","modified":1597175666847},{"_id":"public/2020/05/23/helloworld/index.html","hash":"77fab96e5486d350a6c18a2b6c87921cfff1837a","modified":1597175666847},{"_id":"public/archives/index.html","hash":"d6fef794819a105ea1c9e745d8faad01290d6337","modified":1597175666847},{"_id":"public/archives/page/2/index.html","hash":"ab03fa2144e6c3571b6fad5a1496b8ae1fd9a9ea","modified":1597175666847},{"_id":"public/archives/2020/index.html","hash":"b7c0728b458cef9768f35c9b4c60f63912a9ffbe","modified":1597175666847},{"_id":"public/archives/2020/page/2/index.html","hash":"1375bba379194e2ca321cb273c4b68284be64e75","modified":1597175666847},{"_id":"public/archives/2020/05/index.html","hash":"5c987734e1b23e9d7f2d22eb9ec97ac6d892a219","modified":1597175666847},{"_id":"public/archives/2020/06/index.html","hash":"d4ceae3edba9b4c073b368ab1f7f4cf112142a26","modified":1597175666847},{"_id":"public/archives/2020/07/index.html","hash":"7cedd5191668ca993f2e1caac3d4bdd5794f4a30","modified":1597175666847},{"_id":"public/archives/2020/08/index.html","hash":"9b94fecd4ca14101651bc1470d286a561c066296","modified":1597175666847},{"_id":"public/categories/Startup/index.html","hash":"ced6d5260ad69c83d59e598a9c69222ecc8eba3a","modified":1597175666847},{"_id":"public/categories/随笔/index.html","hash":"a1fe28ded1bc920a8b3c9e3ccde27d2e85d9d158","modified":1597175666847},{"_id":"public/categories/Data-Science/index.html","hash":"89e7723e72458f6d3f71f2599e22c23acf1d08d3","modified":1597175666847},{"_id":"public/categories/摘录/index.html","hash":"45c8b241a97ce783241c2547acc04194afbf5975","modified":1597175666847},{"_id":"public/index.html","hash":"f30fcafcc2c0d371f0f558d5f90e3f45e38670a4","modified":1597175666847},{"_id":"public/page/2/index.html","hash":"d5ba42b1d3df6a53701853bcebd8869dafbb4b7f","modified":1597175666847},{"_id":"public/tags/Product-management/index.html","hash":"276043980ec4a1fc297d17bbad7177897168a8e5","modified":1597175666847},{"_id":"public/tags/Agile/index.html","hash":"da6e429cd99b5d251ce46d6229382c9cab6d6ef7","modified":1597175666847},{"_id":"public/tags/笔记/index.html","hash":"ef3b8d5d90a37ca5bd50e90c99db9a868de933e2","modified":1597175666847},{"_id":"public/tags/ML/index.html","hash":"5100a4fe0602b78ca9af36a7bedb6e29e65bd71c","modified":1597175666847},{"_id":"public/tags/Neural-Net/index.html","hash":"226a38b318273d0d949f15eb0600909822e00ca9","modified":1597175666847},{"_id":"public/tags/Resources/index.html","hash":"e3beba33848b576d5a7f79180128ef91dde32d17","modified":1597175666847},{"_id":"public/tags/Ideas/index.html","hash":"91ed8644658940f7de79a4f6459a75b92599e153","modified":1597175666847},{"_id":"public/tags/RL/index.html","hash":"de5ebfb0cb966dc37cc59c1af2a6aa6f60289482","modified":1597175666847},{"_id":"public/tags/SQL/index.html","hash":"2a94467f13a480610a7fbaf00847dee78000b52c","modified":1597175666847},{"_id":"public/tags/诗/index.html","hash":"ac7a7ee4428f124b587ddb494b203816f9636d5d","modified":1597175666847},{"_id":"public/tags/DQN/index.html","hash":"1f2a0f78e5b5763525a970938b496f58f797a1ba","modified":1597175666847},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1597175666847},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1597175666847},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1597175666847},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1597175666847},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1597175666847},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1597175666847},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1597175666847},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1597175666847},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1597175666847},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1597175666847},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1597175666847},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1597175666847},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1597175666847},{"_id":"public/2020/07/28/agile/Scrum-Diagram-JordanJob.me.png","hash":"12c00e2cf565f7d9bc7b6cae8de4c9fda22f8e01","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/G0.png","hash":"5b2ae6d1882b509cf301c858e43c960cdf555611","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/G1.png","hash":"3d0e21a231ed7e2ed08559136d6c9914cd6fe2f7","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/mdp0.png","hash":"e1344516fd512ea0975e78afb52b951a5ac2d0a4","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/mdp1.png","hash":"95e11b1a208370cbf543813eedf5b8614a991271","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/1equation.png","hash":"6c5270d865056b0b1e9c2bd3ee5a7af2bf7ab194","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/2actionvalue.png","hash":"91fb5b769468bfc5ed214dbf527b46122ee51766","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/4.png","hash":"03fbd816e0da30558fe92403c1701441517c156f","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/6.png","hash":"dfa6a7fa02f4a3c5537d9caaf038f6c6a47df574","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/5.png","hash":"04b713163573340981da8cb2a57f942e8921687d","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/8.png","hash":"836b4a882890e7716e3af2d9cd6ea2d648186db0","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/7.png","hash":"2ae92891f17acb989e55841c490d1d1fd4e8810c","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/actionvalue.png","hash":"70caea98d9d3df40a783f8beadf9f99d8b60220e","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/bellman.png","hash":"71ab9e5971d2ed03581e9280156c7040ad97f101","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/optimal_graphical.png","hash":"cbec917cb0250b3f6f502e8c4661054b36f40bdf","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/optimalq_givenv.png","hash":"ecd7de9d386fca7025c9f4ca7a09c3b13db8b321","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/optimalq.png","hash":"803cf3908c07149f2995883d4b9d4c699216ab80","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/q.png","hash":"8b1cad6fefb618927eba2db8740b62be65fa1c27","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/statevalue.png","hash":"30423627e983aadb63b751122930908c88113eb4","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/tree.png","hash":"f3a8c5b84433d9918bf5d3565e598480c9406c20","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/v.png","hash":"0e8daa229129287743f99ff600beb1645c8189e2","modified":1597175666847},{"_id":"public/images/favicon.png","hash":"c7ddc4fa3ebddfbe12e7ec7ac2bf32de9de342ab","modified":1597175666847},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1597175666847},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1597175666847},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1597175666847},{"_id":"public/2020/06/18/rl-mdp/mdp2.png","hash":"cf4849c5d825b2596de6da3e37aa90aead9ca1f3","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/3.png","hash":"4113998bb1bec693a53abf0283192ba4daabfb9a","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/computingV1.png","hash":"0e8a0111f9ef2dc4d4a0442f4b2b1256127ad971","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/computingV2.png","hash":"df0e191bda5f3a34b1130540319fb045b61a2307","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/optimalv.png","hash":"397f9d44a837e789c5f373da0a2aeac07b60d73d","modified":1597175666847},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1597175666847},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1597175666847},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1597175666847},{"_id":"public/js/utils.js","hash":"2c6e6b4d9a592fbb4bf04689524db2cdfcd94ca7","modified":1597175666847},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1597175666847},{"_id":"public/css/main.css","hash":"e75ddb3d87213a9714ae4e9a1d7e68a57867239f","modified":1597175666847},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1597175666847},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1597175666847},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1597175666847},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1597175666847},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1597175666847},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1597175666847},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/grid.png","hash":"a8400d9249c20581ae2ba4f94b7ca00ac8492263","modified":1597175666847},{"_id":"public/2020/06/22/rl-policy-valuefunct/optimalpolicy.png","hash":"9c7da23153d308f0ee94701ace24f4c70e21bdb0","modified":1597175666847},{"_id":"public/2020/07/28/agile/diagram.JPG","hash":"00ea15f40cb978cd8e546f58b1c6e39482f37bf7","modified":1597175666847},{"_id":"public/2020/07/28/agile/Scrum-v2.0-GIF-1000px.gif","hash":"ec09136dca2258acfff659f3ef688b54dbdb88d8","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/summary.mp4","hash":"de13ff66ff018d234ead2a2bc57dd09716672fa3","modified":1597175666847},{"_id":"public/2020/06/13/rl-bandit/RL.pdf","hash":"e11c618dda22c290315c27c7020b3d9f0a4b55cb","modified":1597175666847}],"Category":[{"name":"Startup","_id":"ckdqd5kjs0002d64bfjaa1nz3"},{"name":"随笔","_id":"ckdqd5kjy0007d64bf7t98607"},{"name":"Data Science","_id":"ckdqd5kk1000cd64bgqlleo4i"},{"name":"摘录","_id":"ckdqd5klk001jd64b8wt281s6"}],"Data":[{"_id":"styles","data":".archive .collection-title {\n  display: none !important;\n}\n"},{"_id":"footer","data":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}],"Page":[{"title":"","date":"2020-06-19T00:13:07.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle:\ndate: 2020-06-18 20:13:07\ntype: \"categories\"\n---\n","updated":"2020-07-20T17:35:27.254Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckdqd5klc001dd64b5gt75551","content":"","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":""},{"layout":"page","title":"","_content":"\nHello stranger,\n\nWelcome to my space.\nI post a wide range of things,\nfrom notes to reflections to even poems.\n\n","source":"about/index.md","raw":"---\nlayout: page\ntitle: \n---\n\nHello stranger,\n\nWelcome to my space.\nI post a wide range of things,\nfrom notes to reflections to even poems.\n\n","date":"2020-07-20T17:33:56.745Z","updated":"2020-07-20T17:33:56.745Z","path":"about/index.html","comments":1,"_id":"ckdqd5klh001fd64bc2budva5","content":"<p>Hello stranger,</p>\n<p>Welcome to my space.<br>I post a wide range of things,<br>from notes to reflections to even poems.</p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<p>Hello stranger,</p>\n<p>Welcome to my space.<br>I post a wide range of things,<br>from notes to reflections to even poems.</p>\n"},{"layout":"tagcloud","title":"","type":"tags","_content":"","source":"tags/index.md","raw":"---\nlayout: tagcloud\ntitle: \ntype: \"tags\"\n---","date":"2020-07-20T17:34:25.946Z","updated":"2020-07-20T17:34:25.946Z","path":"tags/index.html","comments":1,"_id":"ckdqd5klk001id64bhnvxc9bs","content":"","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":""}],"Post":[{"title":"Startup Case Study --- 'Scrum' Product Management","date":"2020-07-28T19:56:48.000Z","_content":"\nI have worked remotely as a data science intern for a prospering startup for just over two months. During this time, I was exposed to a fascinating product management system called Agile. Hence, I want to note some things down in case they are relevant in the future. \n### What is Agile?\n\nIn early 2001, in Snowbird, Utah, 17 people met to discuss the future of software development. The group’s members shared a frustration about the current state of affairs. The problem, they agreed, was that companies were so focused on excessively planning and documenting their software development cycles that they lost sight of what really mattered—pleasing their customers.\n\nCompanies may have touted corporate values like “excellence” and “integrity,” but these values did little to guide people—especially software developers—toward a better way.\n\nThe Agile Manifesto emerged from this extended weekend at just 68 words, and the short and sweet document went on to change software development forever. [[1]](https://www.atlassian.com/agile/manifesto)\n<!-- more -->\n\n#### The Agile Manifesto:\n\n> **Individuals and interactions** over processes and tools\n**Working software** over comprehensive documentation\n**Customer collaboration** over contract negotiation\n**Responding to change** over following a plan [[2]](https://agilemanifesto.org/)\n\n*That is, while there is value in the items on the right, we value the items on the left more.*\n\n### Agile vs. Scrum vs. Sprint\nThese three terms always seem to appear together and intertwined. Below are their meanings/definitions to clear the confusion.\n\n**Agile** (Agile project management): a project management ***philosophy*** or ***framework*** that takes an iterative approach towards the completion of a project. (Mostly for software development projects)\n\n**Scrum**: an Agile ***methodology*** (Some of the most common Agile methodologies include Scrum, Kanban, Extreme Programming (XP))\n\n**Sprint**: the core element of Scrum methodology, a consistent time period for developing features for a product, the deliverable of a sprint is a \"Done\" product increment.\n\n### Features of Scrum\n\n{% asset_img \"Scrum-v2.0-GIF-1000px.gif\" %}\n\nhttps://jordanjob.me/blog/scrum-diagram/\n\n*Product Backlog* : ordered list of work to be done, maintained by the \"product owner\".\n*Sprint Backlog* : list of articles to address for next sprint.\n*Development Team* : implements articles in sprint backlog.\n*Scrum Master* : someone who coaches the team about how scrum works and sorts out impediments.\n*Daily Scrum* : fixed-time, fixed-place event to synchronize and plan work. What was done today? What needs to be done tomorrow? What is impeding me?\n*Sprint Review* : inspect work done, adapts product backlog if neccessary. \n*Sprint Retrospective* : analyze for improvement.\n\n\n### Implementation by my internship \"startup Y\"\n\n*Product Backlog* : managed by CEO & management team (G-spreadsheet & Slite)\n*Sprint Backlog* : managed by PMs, discussed each sprint (per week)\n*Scrum Master* : this role was missing at startup Y, so I reached out to the CEO to let him know the benefits of having one :)\n*Daily Scrum* : Each day at 3pm, entered into company app (Run on Glideapp.io)\n*Sprint Review* : Weekly monday meeting\n*Sprint Retrospective* : Review session following the monday meeting.\n\nThe Toolbox\n- Glide: For daily scrum and attendance\n- Slite: For project management / planning\n- Discord: Hosting voice meetings and sharing information\n- Google drive & Google groups: Sharing work\n{% asset_img \"diagram.JPG\" %}\n\nSome Notes:\n- A lot of PMs, each subteam has around 2\n- Flexible work schedule\n- 3 mandatory meeting periods per week (Monday, Thursday, Saturday)\n- Communication via discord AND whatsapp, not a good idea... Should only stick to one\n- Development team is not really managed/pushed/monitored, mostly driven by each person's pace & will.","source":"_posts/agile.md","raw":"---\ntitle: \"Startup case study --- 'Scrum' product management\"\ndate: 2020-07-28 15:56:48\ntags:\n- Product management\n- Agile\ncategories: Startup\n\n---\n\nI have worked remotely as a data science intern for a prospering startup for just over two months. During this time, I was exposed to a fascinating product management system called Agile. Hence, I want to note some things down in case they are relevant in the future. \n### What is Agile?\n\nIn early 2001, in Snowbird, Utah, 17 people met to discuss the future of software development. The group’s members shared a frustration about the current state of affairs. The problem, they agreed, was that companies were so focused on excessively planning and documenting their software development cycles that they lost sight of what really mattered—pleasing their customers.\n\nCompanies may have touted corporate values like “excellence” and “integrity,” but these values did little to guide people—especially software developers—toward a better way.\n\nThe Agile Manifesto emerged from this extended weekend at just 68 words, and the short and sweet document went on to change software development forever. [[1]](https://www.atlassian.com/agile/manifesto)\n<!-- more -->\n\n#### The Agile Manifesto:\n\n> **Individuals and interactions** over processes and tools\n**Working software** over comprehensive documentation\n**Customer collaboration** over contract negotiation\n**Responding to change** over following a plan [[2]](https://agilemanifesto.org/)\n\n*That is, while there is value in the items on the right, we value the items on the left more.*\n\n### Agile vs. Scrum vs. Sprint\nThese three terms always seem to appear together and intertwined. Below are their meanings/definitions to clear the confusion.\n\n**Agile** (Agile project management): a project management ***philosophy*** or ***framework*** that takes an iterative approach towards the completion of a project. (Mostly for software development projects)\n\n**Scrum**: an Agile ***methodology*** (Some of the most common Agile methodologies include Scrum, Kanban, Extreme Programming (XP))\n\n**Sprint**: the core element of Scrum methodology, a consistent time period for developing features for a product, the deliverable of a sprint is a \"Done\" product increment.\n\n### Features of Scrum\n\n{% asset_img \"Scrum-v2.0-GIF-1000px.gif\" %}\n\nhttps://jordanjob.me/blog/scrum-diagram/\n\n*Product Backlog* : ordered list of work to be done, maintained by the \"product owner\".\n*Sprint Backlog* : list of articles to address for next sprint.\n*Development Team* : implements articles in sprint backlog.\n*Scrum Master* : someone who coaches the team about how scrum works and sorts out impediments.\n*Daily Scrum* : fixed-time, fixed-place event to synchronize and plan work. What was done today? What needs to be done tomorrow? What is impeding me?\n*Sprint Review* : inspect work done, adapts product backlog if neccessary. \n*Sprint Retrospective* : analyze for improvement.\n\n\n### Implementation by my internship \"startup Y\"\n\n*Product Backlog* : managed by CEO & management team (G-spreadsheet & Slite)\n*Sprint Backlog* : managed by PMs, discussed each sprint (per week)\n*Scrum Master* : this role was missing at startup Y, so I reached out to the CEO to let him know the benefits of having one :)\n*Daily Scrum* : Each day at 3pm, entered into company app (Run on Glideapp.io)\n*Sprint Review* : Weekly monday meeting\n*Sprint Retrospective* : Review session following the monday meeting.\n\nThe Toolbox\n- Glide: For daily scrum and attendance\n- Slite: For project management / planning\n- Discord: Hosting voice meetings and sharing information\n- Google drive & Google groups: Sharing work\n{% asset_img \"diagram.JPG\" %}\n\nSome Notes:\n- A lot of PMs, each subteam has around 2\n- Flexible work schedule\n- 3 mandatory meeting periods per week (Monday, Thursday, Saturday)\n- Communication via discord AND whatsapp, not a good idea... Should only stick to one\n- Development team is not really managed/pushed/monitored, mostly driven by each person's pace & will.","slug":"agile","published":1,"updated":"2020-08-05T14:59:54.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kja0000d64bb5wj7usn","content":"<p>I have worked remotely as a data science intern for a prospering startup for just over two months. During this time, I was exposed to a fascinating product management system called Agile. Hence, I want to note some things down in case they are relevant in the future. </p>\n<h3 id=\"What-is-Agile\"><a href=\"#What-is-Agile\" class=\"headerlink\" title=\"What is Agile?\"></a>What is Agile?</h3><p>In early 2001, in Snowbird, Utah, 17 people met to discuss the future of software development. The group’s members shared a frustration about the current state of affairs. The problem, they agreed, was that companies were so focused on excessively planning and documenting their software development cycles that they lost sight of what really mattered—pleasing their customers.</p>\n<p>Companies may have touted corporate values like “excellence” and “integrity,” but these values did little to guide people—especially software developers—toward a better way.</p>\n<p>The Agile Manifesto emerged from this extended weekend at just 68 words, and the short and sweet document went on to change software development forever. <a href=\"https://www.atlassian.com/agile/manifesto\" target=\"_blank\" rel=\"noopener\">[1]</a></p>\n<a id=\"more\"></a>\n\n<h4 id=\"The-Agile-Manifesto\"><a href=\"#The-Agile-Manifesto\" class=\"headerlink\" title=\"The Agile Manifesto:\"></a>The Agile Manifesto:</h4><blockquote>\n<p><strong>Individuals and interactions</strong> over processes and tools<br><strong>Working software</strong> over comprehensive documentation<br><strong>Customer collaboration</strong> over contract negotiation<br><strong>Responding to change</strong> over following a plan <a href=\"https://agilemanifesto.org/\" target=\"_blank\" rel=\"noopener\">[2]</a></p>\n</blockquote>\n<p><em>That is, while there is value in the items on the right, we value the items on the left more.</em></p>\n<h3 id=\"Agile-vs-Scrum-vs-Sprint\"><a href=\"#Agile-vs-Scrum-vs-Sprint\" class=\"headerlink\" title=\"Agile vs. Scrum vs. Sprint\"></a>Agile vs. Scrum vs. Sprint</h3><p>These three terms always seem to appear together and intertwined. Below are their meanings/definitions to clear the confusion.</p>\n<p><strong>Agile</strong> (Agile project management): a project management <strong><em>philosophy</em></strong> or <strong><em>framework</em></strong> that takes an iterative approach towards the completion of a project. (Mostly for software development projects)</p>\n<p><strong>Scrum</strong>: an Agile <strong><em>methodology</em></strong> (Some of the most common Agile methodologies include Scrum, Kanban, Extreme Programming (XP))</p>\n<p><strong>Sprint</strong>: the core element of Scrum methodology, a consistent time period for developing features for a product, the deliverable of a sprint is a “Done” product increment.</p>\n<h3 id=\"Features-of-Scrum\"><a href=\"#Features-of-Scrum\" class=\"headerlink\" title=\"Features of Scrum\"></a>Features of Scrum</h3><img src=\"/blog/2020/07/28/agile/Scrum-v2.0-GIF-1000px.gif\" class=\"\">\n\n<p><a href=\"https://jordanjob.me/blog/scrum-diagram/\" target=\"_blank\" rel=\"noopener\">https://jordanjob.me/blog/scrum-diagram/</a></p>\n<p><em>Product Backlog</em> : ordered list of work to be done, maintained by the “product owner”.<br><em>Sprint Backlog</em> : list of articles to address for next sprint.<br><em>Development Team</em> : implements articles in sprint backlog.<br><em>Scrum Master</em> : someone who coaches the team about how scrum works and sorts out impediments.<br><em>Daily Scrum</em> : fixed-time, fixed-place event to synchronize and plan work. What was done today? What needs to be done tomorrow? What is impeding me?<br><em>Sprint Review</em> : inspect work done, adapts product backlog if neccessary.<br><em>Sprint Retrospective</em> : analyze for improvement.</p>\n<h3 id=\"Implementation-by-my-internship-“startup-Y”\"><a href=\"#Implementation-by-my-internship-“startup-Y”\" class=\"headerlink\" title=\"Implementation by my internship “startup Y”\"></a>Implementation by my internship “startup Y”</h3><p><em>Product Backlog</em> : managed by CEO &amp; management team (G-spreadsheet &amp; Slite)<br><em>Sprint Backlog</em> : managed by PMs, discussed each sprint (per week)<br><em>Scrum Master</em> : this role was missing at startup Y, so I reached out to the CEO to let him know the benefits of having one :)<br><em>Daily Scrum</em> : Each day at 3pm, entered into company app (Run on Glideapp.io)<br><em>Sprint Review</em> : Weekly monday meeting<br><em>Sprint Retrospective</em> : Review session following the monday meeting.</p>\n<p>The Toolbox</p>\n<ul>\n<li>Glide: For daily scrum and attendance</li>\n<li>Slite: For project management / planning</li>\n<li>Discord: Hosting voice meetings and sharing information</li>\n<li>Google drive &amp; Google groups: Sharing work<img src=\"/blog/2020/07/28/agile/diagram.JPG\" class=\"\">\n\n</li>\n</ul>\n<p>Some Notes:</p>\n<ul>\n<li>A lot of PMs, each subteam has around 2</li>\n<li>Flexible work schedule</li>\n<li>3 mandatory meeting periods per week (Monday, Thursday, Saturday)</li>\n<li>Communication via discord AND whatsapp, not a good idea… Should only stick to one</li>\n<li>Development team is not really managed/pushed/monitored, mostly driven by each person’s pace &amp; will.</li>\n</ul>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<p>I have worked remotely as a data science intern for a prospering startup for just over two months. During this time, I was exposed to a fascinating product management system called Agile. Hence, I want to note some things down in case they are relevant in the future. </p>\n<h3 id=\"What-is-Agile\"><a href=\"#What-is-Agile\" class=\"headerlink\" title=\"What is Agile?\"></a>What is Agile?</h3><p>In early 2001, in Snowbird, Utah, 17 people met to discuss the future of software development. The group’s members shared a frustration about the current state of affairs. The problem, they agreed, was that companies were so focused on excessively planning and documenting their software development cycles that they lost sight of what really mattered—pleasing their customers.</p>\n<p>Companies may have touted corporate values like “excellence” and “integrity,” but these values did little to guide people—especially software developers—toward a better way.</p>\n<p>The Agile Manifesto emerged from this extended weekend at just 68 words, and the short and sweet document went on to change software development forever. <a href=\"https://www.atlassian.com/agile/manifesto\" target=\"_blank\" rel=\"noopener\">[1]</a></p>","more":"<h4 id=\"The-Agile-Manifesto\"><a href=\"#The-Agile-Manifesto\" class=\"headerlink\" title=\"The Agile Manifesto:\"></a>The Agile Manifesto:</h4><blockquote>\n<p><strong>Individuals and interactions</strong> over processes and tools<br><strong>Working software</strong> over comprehensive documentation<br><strong>Customer collaboration</strong> over contract negotiation<br><strong>Responding to change</strong> over following a plan <a href=\"https://agilemanifesto.org/\" target=\"_blank\" rel=\"noopener\">[2]</a></p>\n</blockquote>\n<p><em>That is, while there is value in the items on the right, we value the items on the left more.</em></p>\n<h3 id=\"Agile-vs-Scrum-vs-Sprint\"><a href=\"#Agile-vs-Scrum-vs-Sprint\" class=\"headerlink\" title=\"Agile vs. Scrum vs. Sprint\"></a>Agile vs. Scrum vs. Sprint</h3><p>These three terms always seem to appear together and intertwined. Below are their meanings/definitions to clear the confusion.</p>\n<p><strong>Agile</strong> (Agile project management): a project management <strong><em>philosophy</em></strong> or <strong><em>framework</em></strong> that takes an iterative approach towards the completion of a project. (Mostly for software development projects)</p>\n<p><strong>Scrum</strong>: an Agile <strong><em>methodology</em></strong> (Some of the most common Agile methodologies include Scrum, Kanban, Extreme Programming (XP))</p>\n<p><strong>Sprint</strong>: the core element of Scrum methodology, a consistent time period for developing features for a product, the deliverable of a sprint is a “Done” product increment.</p>\n<h3 id=\"Features-of-Scrum\"><a href=\"#Features-of-Scrum\" class=\"headerlink\" title=\"Features of Scrum\"></a>Features of Scrum</h3><img src=\"/blog/2020/07/28/agile/Scrum-v2.0-GIF-1000px.gif\" class=\"\">\n\n<p><a href=\"https://jordanjob.me/blog/scrum-diagram/\" target=\"_blank\" rel=\"noopener\">https://jordanjob.me/blog/scrum-diagram/</a></p>\n<p><em>Product Backlog</em> : ordered list of work to be done, maintained by the “product owner”.<br><em>Sprint Backlog</em> : list of articles to address for next sprint.<br><em>Development Team</em> : implements articles in sprint backlog.<br><em>Scrum Master</em> : someone who coaches the team about how scrum works and sorts out impediments.<br><em>Daily Scrum</em> : fixed-time, fixed-place event to synchronize and plan work. What was done today? What needs to be done tomorrow? What is impeding me?<br><em>Sprint Review</em> : inspect work done, adapts product backlog if neccessary.<br><em>Sprint Retrospective</em> : analyze for improvement.</p>\n<h3 id=\"Implementation-by-my-internship-“startup-Y”\"><a href=\"#Implementation-by-my-internship-“startup-Y”\" class=\"headerlink\" title=\"Implementation by my internship “startup Y”\"></a>Implementation by my internship “startup Y”</h3><p><em>Product Backlog</em> : managed by CEO &amp; management team (G-spreadsheet &amp; Slite)<br><em>Sprint Backlog</em> : managed by PMs, discussed each sprint (per week)<br><em>Scrum Master</em> : this role was missing at startup Y, so I reached out to the CEO to let him know the benefits of having one :)<br><em>Daily Scrum</em> : Each day at 3pm, entered into company app (Run on Glideapp.io)<br><em>Sprint Review</em> : Weekly monday meeting<br><em>Sprint Retrospective</em> : Review session following the monday meeting.</p>\n<p>The Toolbox</p>\n<ul>\n<li>Glide: For daily scrum and attendance</li>\n<li>Slite: For project management / planning</li>\n<li>Discord: Hosting voice meetings and sharing information</li>\n<li>Google drive &amp; Google groups: Sharing work<img src=\"/blog/2020/07/28/agile/diagram.JPG\" class=\"\">\n\n</li>\n</ul>\n<p>Some Notes:</p>\n<ul>\n<li>A lot of PMs, each subteam has around 2</li>\n<li>Flexible work schedule</li>\n<li>3 mandatory meeting periods per week (Monday, Thursday, Saturday)</li>\n<li>Communication via discord AND whatsapp, not a good idea… Should only stick to one</li>\n<li>Development team is not really managed/pushed/monitored, mostly driven by each person’s pace &amp; will.</li>\n</ul>"},{"title":"Hello World","date":"2020-05-23T22:05:56.000Z","_content":"花了两天搭了个hexo博客，用着顺手简洁，以后就随便写点什么。\n\n五月给伊萨卡带来了阳光与微风，夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和水声，似乎闭目养神就能和自然融为一体。\n\n回看大三，感到可以更有效的去和自己沟通，并更精准的理解自己的思想和心情。这让我能更好的管理时间，计划未来，并预判自己。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全匹不上他想去征服这块大陆的野心。\n\n数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。凡事都不会一蹴而就，让时间去沉淀和积累吧。\n\n未来可期，愿自己能在步入社会的同时不忘初心、牢记原则和信仰。\n\n\n","source":"_posts/helloworld.md","raw":"---\ntitle: hello world\ndate: 2020-05-23 18:05:56\ntags: \ncategories: 随笔\n\n---\n花了两天搭了个hexo博客，用着顺手简洁，以后就随便写点什么。\n\n五月给伊萨卡带来了阳光与微风，夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和水声，似乎闭目养神就能和自然融为一体。\n\n回看大三，感到可以更有效的去和自己沟通，并更精准的理解自己的思想和心情。这让我能更好的管理时间，计划未来，并预判自己。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全匹不上他想去征服这块大陆的野心。\n\n数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。凡事都不会一蹴而就，让时间去沉淀和积累吧。\n\n未来可期，愿自己能在步入社会的同时不忘初心、牢记原则和信仰。\n\n\n","slug":"helloworld","published":1,"updated":"2020-08-05T15:04:45.431Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kjq0001d64b9xmvaonz","content":"<p>花了两天搭了个hexo博客，用着顺手简洁，以后就随便写点什么。</p>\n<p>五月给伊萨卡带来了阳光与微风，夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和水声，似乎闭目养神就能和自然融为一体。</p>\n<p>回看大三，感到可以更有效的去和自己沟通，并更精准的理解自己的思想和心情。这让我能更好的管理时间，计划未来，并预判自己。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全匹不上他想去征服这块大陆的野心。</p>\n<p>数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。凡事都不会一蹴而就，让时间去沉淀和积累吧。</p>\n<p>未来可期，愿自己能在步入社会的同时不忘初心、牢记原则和信仰。</p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<p>花了两天搭了个hexo博客，用着顺手简洁，以后就随便写点什么。</p>\n<p>五月给伊萨卡带来了阳光与微风，夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和水声，似乎闭目养神就能和自然融为一体。</p>\n<p>回看大三，感到可以更有效的去和自己沟通，并更精准的理解自己的思想和心情。这让我能更好的管理时间，计划未来，并预判自己。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全匹不上他想去征服这块大陆的野心。</p>\n<p>数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。凡事都不会一蹴而就，让时间去沉淀和积累吧。</p>\n<p>未来可期，愿自己能在步入社会的同时不忘初心、牢记原则和信仰。</p>\n"},{"title":"Neural Networks & Deep Learning","date":"2020-07-01T19:03:38.000Z","mathjax":true,"_content":"\ncomming soon...\n","source":"_posts/neuralnet.md","raw":"---\ntitle: Neural Networks & Deep Learning\ndate: 2020-07-01 15:03:38\ntags:\n- 笔记\n- ML\n- Neural Net\ncategories: Data Science\nmathjax: true\n---\n\ncomming soon...\n","slug":"neuralnet","published":1,"updated":"2020-07-29T02:27:19.353Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kju0004d64bgze8919s","content":"<p>comming soon…</p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<p>comming soon…</p>\n"},{"title":"Data Science Resources","date":"2020-07-20T19:35:05.000Z","_content":"\n<!-- ## Data Science Resources -->\n\n\n### Data Science: Technical\n[Papers with code](https://paperswithcode.com/sota)\n[Kaggle glossary](https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle)\n\n### Data Science: Community & resources\n[Data Science Central](https://www.datasciencecentral.com/)\n[Continued learning](https://github.com/justmarkham/DAT3/blob/master/resources.md)\n[Data sci guide](https://www.kaggle.com/getting-started/44915)\n<!-- more -->\n\n### Data Science: Interview prep & practice problems\n[100 data sci interview Qs](https://www.datasciencecentral.com/profiles/blogs/100-data-science-interview-questions-and-answers)\n\n### SQL\n[mode.com SQL tutorial](https://mode.com/sql-tutorial/intro-to-advanced-sql/)\n\n### Statistics\n[simplystatistics blog](https://simplystatistics.org/)\n\n### Books\n[PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook/)","source":"_posts/online-resources.md","raw":"---\ntitle: Data Science Resources\ndate: 2020-07-20 15:35:05\ntags: Resources\ncategories: Data Science\n\n---\n\n<!-- ## Data Science Resources -->\n\n\n### Data Science: Technical\n[Papers with code](https://paperswithcode.com/sota)\n[Kaggle glossary](https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle)\n\n### Data Science: Community & resources\n[Data Science Central](https://www.datasciencecentral.com/)\n[Continued learning](https://github.com/justmarkham/DAT3/blob/master/resources.md)\n[Data sci guide](https://www.kaggle.com/getting-started/44915)\n<!-- more -->\n\n### Data Science: Interview prep & practice problems\n[100 data sci interview Qs](https://www.datasciencecentral.com/profiles/blogs/100-data-science-interview-questions-and-answers)\n\n### SQL\n[mode.com SQL tutorial](https://mode.com/sql-tutorial/intro-to-advanced-sql/)\n\n### Statistics\n[simplystatistics blog](https://simplystatistics.org/)\n\n### Books\n[PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook/)","slug":"online-resources","published":1,"updated":"2020-08-05T15:12:03.682Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kjw0005d64b68ra1qzj","content":"<!-- ## Data Science Resources -->\n\n\n<h3 id=\"Data-Science-Technical\"><a href=\"#Data-Science-Technical\" class=\"headerlink\" title=\"Data Science: Technical\"></a>Data Science: Technical</h3><p><a href=\"https://paperswithcode.com/sota\" target=\"_blank\" rel=\"noopener\">Papers with code</a><br><a href=\"https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle\" target=\"_blank\" rel=\"noopener\">Kaggle glossary</a></p>\n<h3 id=\"Data-Science-Community-amp-resources\"><a href=\"#Data-Science-Community-amp-resources\" class=\"headerlink\" title=\"Data Science: Community &amp; resources\"></a>Data Science: Community &amp; resources</h3><p><a href=\"https://www.datasciencecentral.com/\" target=\"_blank\" rel=\"noopener\">Data Science Central</a><br><a href=\"https://github.com/justmarkham/DAT3/blob/master/resources.md\" target=\"_blank\" rel=\"noopener\">Continued learning</a><br><a href=\"https://www.kaggle.com/getting-started/44915\" target=\"_blank\" rel=\"noopener\">Data sci guide</a></p>\n<a id=\"more\"></a>\n\n<h3 id=\"Data-Science-Interview-prep-amp-practice-problems\"><a href=\"#Data-Science-Interview-prep-amp-practice-problems\" class=\"headerlink\" title=\"Data Science: Interview prep &amp; practice problems\"></a>Data Science: Interview prep &amp; practice problems</h3><p><a href=\"https://www.datasciencecentral.com/profiles/blogs/100-data-science-interview-questions-and-answers\" target=\"_blank\" rel=\"noopener\">100 data sci interview Qs</a></p>\n<h3 id=\"SQL\"><a href=\"#SQL\" class=\"headerlink\" title=\"SQL\"></a>SQL</h3><p><a href=\"https://mode.com/sql-tutorial/intro-to-advanced-sql/\" target=\"_blank\" rel=\"noopener\">mode.com SQL tutorial</a></p>\n<h3 id=\"Statistics\"><a href=\"#Statistics\" class=\"headerlink\" title=\"Statistics\"></a>Statistics</h3><p><a href=\"https://simplystatistics.org/\" target=\"_blank\" rel=\"noopener\">simplystatistics blog</a></p>\n<h3 id=\"Books\"><a href=\"#Books\" class=\"headerlink\" title=\"Books\"></a>Books</h3><p><a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/\" target=\"_blank\" rel=\"noopener\">PythonDataScienceHandbook</a></p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<!-- ## Data Science Resources -->\n\n\n<h3 id=\"Data-Science-Technical\"><a href=\"#Data-Science-Technical\" class=\"headerlink\" title=\"Data Science: Technical\"></a>Data Science: Technical</h3><p><a href=\"https://paperswithcode.com/sota\" target=\"_blank\" rel=\"noopener\">Papers with code</a><br><a href=\"https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle\" target=\"_blank\" rel=\"noopener\">Kaggle glossary</a></p>\n<h3 id=\"Data-Science-Community-amp-resources\"><a href=\"#Data-Science-Community-amp-resources\" class=\"headerlink\" title=\"Data Science: Community &amp; resources\"></a>Data Science: Community &amp; resources</h3><p><a href=\"https://www.datasciencecentral.com/\" target=\"_blank\" rel=\"noopener\">Data Science Central</a><br><a href=\"https://github.com/justmarkham/DAT3/blob/master/resources.md\" target=\"_blank\" rel=\"noopener\">Continued learning</a><br><a href=\"https://www.kaggle.com/getting-started/44915\" target=\"_blank\" rel=\"noopener\">Data sci guide</a></p>","more":"<h3 id=\"Data-Science-Interview-prep-amp-practice-problems\"><a href=\"#Data-Science-Interview-prep-amp-practice-problems\" class=\"headerlink\" title=\"Data Science: Interview prep &amp; practice problems\"></a>Data Science: Interview prep &amp; practice problems</h3><p><a href=\"https://www.datasciencecentral.com/profiles/blogs/100-data-science-interview-questions-and-answers\" target=\"_blank\" rel=\"noopener\">100 data sci interview Qs</a></p>\n<h3 id=\"SQL\"><a href=\"#SQL\" class=\"headerlink\" title=\"SQL\"></a>SQL</h3><p><a href=\"https://mode.com/sql-tutorial/intro-to-advanced-sql/\" target=\"_blank\" rel=\"noopener\">mode.com SQL tutorial</a></p>\n<h3 id=\"Statistics\"><a href=\"#Statistics\" class=\"headerlink\" title=\"Statistics\"></a>Statistics</h3><p><a href=\"https://simplystatistics.org/\" target=\"_blank\" rel=\"noopener\">simplystatistics blog</a></p>\n<h3 id=\"Books\"><a href=\"#Books\" class=\"headerlink\" title=\"Books\"></a>Books</h3><p><a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/\" target=\"_blank\" rel=\"noopener\">PythonDataScienceHandbook</a></p>"},{"title":"Project Ideas","date":"2020-08-04T21:16:45.000Z","_content":"\n\n- Tracker for popularity, sentiment, and discussion volumn of a given keyword in a time period.\n    - google trends [api](https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f)\n    - twitter sentiment\n    - twitter retweets / total posts / comments\n    - reddit?\n\n\n- Books/Movies recommender, built upon douban user profile.\n\n- Apple health data, export, identify trends, monitor sleep cycle.","source":"_posts/project-ideas.md","raw":"---\ntitle: Project Ideas\ndate: 2020-08-04 17:16:45\ntags: \n- Ideas\ncategories: Data Science\n---\n\n\n- Tracker for popularity, sentiment, and discussion volumn of a given keyword in a time period.\n    - google trends [api](https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f)\n    - twitter sentiment\n    - twitter retweets / total posts / comments\n    - reddit?\n\n\n- Books/Movies recommender, built upon douban user profile.\n\n- Apple health data, export, identify trends, monitor sleep cycle.","slug":"project-ideas","published":1,"updated":"2020-08-05T14:42:19.354Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kjx0006d64b14zn4cn7","content":"<ul>\n<li>Tracker for popularity, sentiment, and discussion volumn of a given keyword in a time period.<ul>\n<li>google trends <a href=\"https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f\" target=\"_blank\" rel=\"noopener\">api</a></li>\n<li>twitter sentiment</li>\n<li>twitter retweets / total posts / comments</li>\n<li>reddit?</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>Books/Movies recommender, built upon douban user profile.</p>\n</li>\n<li><p>Apple health data, export, identify trends, monitor sleep cycle.</p>\n</li>\n</ul>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<ul>\n<li>Tracker for popularity, sentiment, and discussion volumn of a given keyword in a time period.<ul>\n<li>google trends <a href=\"https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f\" target=\"_blank\" rel=\"noopener\">api</a></li>\n<li>twitter sentiment</li>\n<li>twitter retweets / total posts / comments</li>\n<li>reddit?</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>Books/Movies recommender, built upon douban user profile.</p>\n</li>\n<li><p>Apple health data, export, identify trends, monitor sleep cycle.</p>\n</li>\n</ul>\n"},{"title":"Bandit Problem","date":"2020-06-14T02:06:45.000Z","_content":"\n<!-- ## Reinforcement Learning Notes -->\n\nDesigns an agent that tries to maximize \"reward\" by trial and error.\nNot supervised, but not unsupervised either.\n\n<!-- [Textbook](RL.pdf) -->\n{% asset_link RL.pdf Textbook PDF%}\n\n## The k-armed Bandit Problem\n\nFaced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n\n<!-- more -->\n\n<!-- ![True value of action](1equation.png) -->\n{% asset_img \"1equation.png\" \"True value of action\" %}\n\nValue of an action is the expected value of reward given that action is selected.\n\n- q*(a): TRUE Value of an action (Note known)\n- Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).\n- At: Action selected on time step t\n- Rt: Corresponding Reward on time step t\n\nGoal is to maximize argmax of q*(a)\n\n> Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected\n\n## Action-Value & increment estimation\n\nSample average method: Take the average of action-values so far.\n\n<!-- ![Sample avg (predicted) action value](2actionvalue.png) -->\n{% asset_img \"2actionvalue.png\" \"Sample avg (predicted) action value\" %}\n\n<!-- ![Action value](3.png) -->\n\nAction Selection:\n- Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)\n- Explore: choose other than the largest estimate randomly.\n\n\n\n<mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark>\n\n<!-- ![Increment (R = reward)](4.png) -->\n{% asset_img \"4.png\" \"Increment (R = reward)\" %}\n\nNewEstimate = OldEstimate + StepSize (Target - OldEstimate)\n\n**Nonstationary Problem**\nPerformance changes with time. (e.g. a medicine more effective in winter)\n<!-- ![Constant step size](5.png) -->\n{% asset_img \"5.png\" \"Constant step size\" %}\n\n- alpha is constant, more recent rewards affect more than older rewards.\n- weight decays exponentially according to the exponent on 1- alpha.\n\n## Epsilon Greedy & optimistic initial value\n\nEpsilon = prob of choosing to explore.\nThe smaller, the longer it takes for curve to plateu.\n<!-- ![Epsilon](7.png) -->\n{% asset_img \"7.png\" \"Epsilon\" %}\n\n<!-- ![Optimistic int val](6.png) -->\n{% asset_img \"6.png\" \"Optimistic int val\" %}\n\n- **Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).**\n- The system does a fair amount of exploration even if greedy actions are selected all the time.\n\n## Upper-Conﬁdence-Bound Action Selection\n\nAction selection method that chooses the most \"unexplored\" / \"uncertain\" actions when not choosing to exploit.\n\n<!-- ![UCB](8.png) -->\n{% asset_img \"8.png\" \"UCB\" %}\n\n> N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.\n\n> The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. ***Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.*** On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n\n<!-- [Week 1 Summary](summary.mp4) -->\n{% asset_link summary.mp4 Video: Week 1 Summary %}\n\n\n","source":"_posts/rl-bandit.md","raw":"---\ntitle: Bandit problem\ndate: 2020-06-13 22:06:45\ntags: \n- 笔记\n- RL\ncategories: Data Science\n---\n\n<!-- ## Reinforcement Learning Notes -->\n\nDesigns an agent that tries to maximize \"reward\" by trial and error.\nNot supervised, but not unsupervised either.\n\n<!-- [Textbook](RL.pdf) -->\n{% asset_link RL.pdf Textbook PDF%}\n\n## The k-armed Bandit Problem\n\nFaced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n\n<!-- more -->\n\n<!-- ![True value of action](1equation.png) -->\n{% asset_img \"1equation.png\" \"True value of action\" %}\n\nValue of an action is the expected value of reward given that action is selected.\n\n- q*(a): TRUE Value of an action (Note known)\n- Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).\n- At: Action selected on time step t\n- Rt: Corresponding Reward on time step t\n\nGoal is to maximize argmax of q*(a)\n\n> Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected\n\n## Action-Value & increment estimation\n\nSample average method: Take the average of action-values so far.\n\n<!-- ![Sample avg (predicted) action value](2actionvalue.png) -->\n{% asset_img \"2actionvalue.png\" \"Sample avg (predicted) action value\" %}\n\n<!-- ![Action value](3.png) -->\n\nAction Selection:\n- Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)\n- Explore: choose other than the largest estimate randomly.\n\n\n\n<mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark>\n\n<!-- ![Increment (R = reward)](4.png) -->\n{% asset_img \"4.png\" \"Increment (R = reward)\" %}\n\nNewEstimate = OldEstimate + StepSize (Target - OldEstimate)\n\n**Nonstationary Problem**\nPerformance changes with time. (e.g. a medicine more effective in winter)\n<!-- ![Constant step size](5.png) -->\n{% asset_img \"5.png\" \"Constant step size\" %}\n\n- alpha is constant, more recent rewards affect more than older rewards.\n- weight decays exponentially according to the exponent on 1- alpha.\n\n## Epsilon Greedy & optimistic initial value\n\nEpsilon = prob of choosing to explore.\nThe smaller, the longer it takes for curve to plateu.\n<!-- ![Epsilon](7.png) -->\n{% asset_img \"7.png\" \"Epsilon\" %}\n\n<!-- ![Optimistic int val](6.png) -->\n{% asset_img \"6.png\" \"Optimistic int val\" %}\n\n- **Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).**\n- The system does a fair amount of exploration even if greedy actions are selected all the time.\n\n## Upper-Conﬁdence-Bound Action Selection\n\nAction selection method that chooses the most \"unexplored\" / \"uncertain\" actions when not choosing to exploit.\n\n<!-- ![UCB](8.png) -->\n{% asset_img \"8.png\" \"UCB\" %}\n\n> N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.\n\n> The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. ***Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.*** On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n\n<!-- [Week 1 Summary](summary.mp4) -->\n{% asset_link summary.mp4 Video: Week 1 Summary %}\n\n\n","slug":"rl-bandit","published":1,"updated":"2020-08-05T15:02:51.622Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kjz0009d64b0ll08ahn","content":"<!-- ## Reinforcement Learning Notes -->\n\n<p>Designs an agent that tries to maximize “reward” by trial and error.<br>Not supervised, but not unsupervised either.</p>\n<!-- [Textbook](RL.pdf) -->\n<a href=\"/blog/2020/06/13/rl-bandit/RL.pdf\" title=\"Textbook PDF\">Textbook PDF</a>\n\n<h2 id=\"The-k-armed-Bandit-Problem\"><a href=\"#The-k-armed-Bandit-Problem\" class=\"headerlink\" title=\"The k-armed Bandit Problem\"></a>The k-armed Bandit Problem</h2><p>Faced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>\n<a id=\"more\"></a>\n\n<!-- ![True value of action](1equation.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/1equation.png\" class=\"\" title=\"True value of action\">\n\n<p>Value of an action is the expected value of reward given that action is selected.</p>\n<ul>\n<li>q*(a): TRUE Value of an action (Note known)</li>\n<li>Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).</li>\n<li>At: Action selected on time step t</li>\n<li>Rt: Corresponding Reward on time step t</li>\n</ul>\n<p>Goal is to maximize argmax of q*(a)</p>\n<blockquote>\n<p>Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected</p>\n</blockquote>\n<h2 id=\"Action-Value-amp-increment-estimation\"><a href=\"#Action-Value-amp-increment-estimation\" class=\"headerlink\" title=\"Action-Value &amp; increment estimation\"></a>Action-Value &amp; increment estimation</h2><p>Sample average method: Take the average of action-values so far.</p>\n<!-- ![Sample avg (predicted) action value](2actionvalue.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/2actionvalue.png\" class=\"\" title=\"Sample avg (predicted) action value\">\n\n<!-- ![Action value](3.png) -->\n\n<p>Action Selection:</p>\n<ul>\n<li>Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)</li>\n<li>Explore: choose other than the largest estimate randomly.</li>\n</ul>\n<p><mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark></p>\n<!-- ![Increment (R = reward)](4.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/4.png\" class=\"\" title=\"Increment (R &#x3D; reward)\">\n\n<p>NewEstimate = OldEstimate + StepSize (Target - OldEstimate)</p>\n<p><strong>Nonstationary Problem</strong><br>Performance changes with time. (e.g. a medicine more effective in winter)</p>\n<!-- ![Constant step size](5.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/5.png\" class=\"\" title=\"Constant step size\">\n\n<ul>\n<li>alpha is constant, more recent rewards affect more than older rewards.</li>\n<li>weight decays exponentially according to the exponent on 1- alpha.</li>\n</ul>\n<h2 id=\"Epsilon-Greedy-amp-optimistic-initial-value\"><a href=\"#Epsilon-Greedy-amp-optimistic-initial-value\" class=\"headerlink\" title=\"Epsilon Greedy &amp; optimistic initial value\"></a>Epsilon Greedy &amp; optimistic initial value</h2><p>Epsilon = prob of choosing to explore.<br>The smaller, the longer it takes for curve to plateu.</p>\n<!-- ![Epsilon](7.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/7.png\" class=\"\" title=\"Epsilon\">\n\n<!-- ![Optimistic int val](6.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/6.png\" class=\"\" title=\"Optimistic int val\">\n\n<ul>\n<li><strong>Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).</strong></li>\n<li>The system does a fair amount of exploration even if greedy actions are selected all the time.</li>\n</ul>\n<h2 id=\"Upper-Conﬁdence-Bound-Action-Selection\"><a href=\"#Upper-Conﬁdence-Bound-Action-Selection\" class=\"headerlink\" title=\"Upper-Conﬁdence-Bound Action Selection\"></a>Upper-Conﬁdence-Bound Action Selection</h2><p>Action selection method that chooses the most “unexplored” / “uncertain” actions when not choosing to exploit.</p>\n<!-- ![UCB](8.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/8.png\" class=\"\" title=\"UCB\">\n\n<blockquote>\n<p>N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c &gt; 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.</p>\n</blockquote>\n<blockquote>\n<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. <strong><em>Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.</em></strong> On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>\n</blockquote>\n<!-- [Week 1 Summary](summary.mp4) -->\n<a href=\"/blog/2020/06/13/rl-bandit/summary.mp4\" title=\"Video: Week 1 Summary\">Video: Week 1 Summary</a>\n\n\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<!-- ## Reinforcement Learning Notes -->\n\n<p>Designs an agent that tries to maximize “reward” by trial and error.<br>Not supervised, but not unsupervised either.</p>\n<!-- [Textbook](RL.pdf) -->\n<a href=\"/blog/2020/06/13/rl-bandit/RL.pdf\" title=\"Textbook PDF\">Textbook PDF</a>\n\n<h2 id=\"The-k-armed-Bandit-Problem\"><a href=\"#The-k-armed-Bandit-Problem\" class=\"headerlink\" title=\"The k-armed Bandit Problem\"></a>The k-armed Bandit Problem</h2><p>Faced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>","more":"<!-- ![True value of action](1equation.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/1equation.png\" class=\"\" title=\"True value of action\">\n\n<p>Value of an action is the expected value of reward given that action is selected.</p>\n<ul>\n<li>q*(a): TRUE Value of an action (Note known)</li>\n<li>Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).</li>\n<li>At: Action selected on time step t</li>\n<li>Rt: Corresponding Reward on time step t</li>\n</ul>\n<p>Goal is to maximize argmax of q*(a)</p>\n<blockquote>\n<p>Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected</p>\n</blockquote>\n<h2 id=\"Action-Value-amp-increment-estimation\"><a href=\"#Action-Value-amp-increment-estimation\" class=\"headerlink\" title=\"Action-Value &amp; increment estimation\"></a>Action-Value &amp; increment estimation</h2><p>Sample average method: Take the average of action-values so far.</p>\n<!-- ![Sample avg (predicted) action value](2actionvalue.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/2actionvalue.png\" class=\"\" title=\"Sample avg (predicted) action value\">\n\n<!-- ![Action value](3.png) -->\n\n<p>Action Selection:</p>\n<ul>\n<li>Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)</li>\n<li>Explore: choose other than the largest estimate randomly.</li>\n</ul>\n<p><mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark></p>\n<!-- ![Increment (R = reward)](4.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/4.png\" class=\"\" title=\"Increment (R &#x3D; reward)\">\n\n<p>NewEstimate = OldEstimate + StepSize (Target - OldEstimate)</p>\n<p><strong>Nonstationary Problem</strong><br>Performance changes with time. (e.g. a medicine more effective in winter)</p>\n<!-- ![Constant step size](5.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/5.png\" class=\"\" title=\"Constant step size\">\n\n<ul>\n<li>alpha is constant, more recent rewards affect more than older rewards.</li>\n<li>weight decays exponentially according to the exponent on 1- alpha.</li>\n</ul>\n<h2 id=\"Epsilon-Greedy-amp-optimistic-initial-value\"><a href=\"#Epsilon-Greedy-amp-optimistic-initial-value\" class=\"headerlink\" title=\"Epsilon Greedy &amp; optimistic initial value\"></a>Epsilon Greedy &amp; optimistic initial value</h2><p>Epsilon = prob of choosing to explore.<br>The smaller, the longer it takes for curve to plateu.</p>\n<!-- ![Epsilon](7.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/7.png\" class=\"\" title=\"Epsilon\">\n\n<!-- ![Optimistic int val](6.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/6.png\" class=\"\" title=\"Optimistic int val\">\n\n<ul>\n<li><strong>Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).</strong></li>\n<li>The system does a fair amount of exploration even if greedy actions are selected all the time.</li>\n</ul>\n<h2 id=\"Upper-Conﬁdence-Bound-Action-Selection\"><a href=\"#Upper-Conﬁdence-Bound-Action-Selection\" class=\"headerlink\" title=\"Upper-Conﬁdence-Bound Action Selection\"></a>Upper-Conﬁdence-Bound Action Selection</h2><p>Action selection method that chooses the most “unexplored” / “uncertain” actions when not choosing to exploit.</p>\n<!-- ![UCB](8.png) -->\n<img src=\"/blog/2020/06/13/rl-bandit/8.png\" class=\"\" title=\"UCB\">\n\n<blockquote>\n<p>N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c &gt; 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.</p>\n</blockquote>\n<blockquote>\n<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. <strong><em>Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.</em></strong> On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>\n</blockquote>\n<!-- [Week 1 Summary](summary.mp4) -->\n<a href=\"/blog/2020/06/13/rl-bandit/summary.mp4\" title=\"Video: Week 1 Summary\">Video: Week 1 Summary</a>"},{"title":"Interview Prep","date":"2020-07-14T02:00:31.000Z","_content":"\nA updating collection of notes/resources for interview prep:\n- SQL\n- Programming\n- Stats\n- Behavioural\n<!-- more -->\n\n# SQL\n\n### Basics\n```sql\nSELECT a,\n        b AS \"field b\",\n        c AS field_b,\n        (c + d)/2 AS c_d_avg -- Can do arithmetic operations\n    FROM tablename\n    WHERE g = 1  \n        AND h NOT IN (2,3,4)\n        AND i IS NULL\n        AND (a ILIKE '%caseinsensitive%' OR \"b\" LIKE '%CaseSensitive%')\n\n    LIMIT 100 -- Limits how many results\n    OFFSET 1 -- Skip the first x results\n```\n\n### Logical operators\n`LIKE`\n- match similar values, case-sensitive\n- % : wildcard, any set of characters\n- _ : any individual character\n`IN` \n- specify a list of values you'd like to include\n- WHERE year_rank IN (1, 2, 3)\n`BETWEEN`\n- select only rows within a certain range\n- WHERE year_rank BETWEEN 5 AND 10\n`IS`\n`AND`\n`OR`\n`NOT`\n\n### Aggregate functions\n\n```sql\nSELECT COUNT(a),\n        AVG(b),\n        MIN(c),\n        MAX(d) AS \"MAX of d\",\n        SUM(e)\n    FROM tutorial.aapl_historical_stock_price\n```\n`COUNT` counts total number of **non-null** rows in a particular column.\n`SUM` adds together all the values in a particular column.\n`MIN` and `MAX` return the lowest and highest values in a particular column, respectively.\n`AVG` calculates the average of a group of selected values.\n\n### Others\n`ORDER BY`\n- sorts data, if multiple column names given, sort in order\n- automatically ascending, use DESC to show descending\n\n`GROUP BY` \n- separate data into groups, which can be aggregated independently of one another.\n\n`WHERE`\n- for filtering results\n\n`HAVING`\n- for filtering on aggregate columns\n```sql \nselect Email from Person group by Email having count(Email) > 1;\n```\n\nQuery clause order:\n1. SELECT\n2. FROM\n3. WHERE\n4. GROUP BY\n5. HAVING\n6. ORDER BY\n\n`CASE`\n- SQL's way of handling if/then logic.\n- Goes in the SELECT clause\n- at lease on pair of WHEN and THEN\n- ELSE is optional\n- must end with END\n\n\n`INNER JOIN`\n`LEFT JOIN`\n`RIGHT JOIN`\n`FULL JOIN / FULL OUTER JOIN` Combination of left and right.\n`UNION`\n- Combines tables by stacking/appending. e.g. `SELECT * from T1 UNION SELECT * from T2`\n- Repeated/duplicate rows will not be copied. \n\n`UNION ALL` \n- Repeated/duplicate rows will be copied. \n\n\nSelf Join\n- When a table joins itself to perforom some kind of magic.\n```sql\nSELECT DISTINCT japan_investments.company_name,\n       japan_investments.company_permalink\n  FROM tutorial.crunchbase_investments_part1 japan_investments\n  JOIN tutorial.crunchbase_investments_part1 gb_investments\n    ON japan_investments.company_name = gb_investments.company_name\n   AND gb_investments.investor_country_code = 'GBR'\n   AND gb_investments.funded_at > japan_investments.funded_at\n WHERE japan_investments.investor_country_code = 'JPN'\n ORDER BY 1\n```\n### Examples\n\n```sql\n-- Write a SQL query to get the nth highest salary from the Employee table.\nCREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT\nBEGIN\nDECLARE M INT;\nSET M=N-1;\n  RETURN (\n      SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT M, 1\n  );\nEND\n```\n# Coding\n\n# Stats\n\n# Behavioural\n\n# Resources\n\n<!-- SQL: https://mode.com/sql-tutorial/intro-to-advanced-sql/ -->\n\n","source":"_posts/interview_prep.md","raw":"---\ntitle: Interview Prep\ndate: 2020-07-13 22:00:31\ntags: \n- 笔记\n- SQL\ncategories: Data Science\n\n---\n\nA updating collection of notes/resources for interview prep:\n- SQL\n- Programming\n- Stats\n- Behavioural\n<!-- more -->\n\n# SQL\n\n### Basics\n```sql\nSELECT a,\n        b AS \"field b\",\n        c AS field_b,\n        (c + d)/2 AS c_d_avg -- Can do arithmetic operations\n    FROM tablename\n    WHERE g = 1  \n        AND h NOT IN (2,3,4)\n        AND i IS NULL\n        AND (a ILIKE '%caseinsensitive%' OR \"b\" LIKE '%CaseSensitive%')\n\n    LIMIT 100 -- Limits how many results\n    OFFSET 1 -- Skip the first x results\n```\n\n### Logical operators\n`LIKE`\n- match similar values, case-sensitive\n- % : wildcard, any set of characters\n- _ : any individual character\n`IN` \n- specify a list of values you'd like to include\n- WHERE year_rank IN (1, 2, 3)\n`BETWEEN`\n- select only rows within a certain range\n- WHERE year_rank BETWEEN 5 AND 10\n`IS`\n`AND`\n`OR`\n`NOT`\n\n### Aggregate functions\n\n```sql\nSELECT COUNT(a),\n        AVG(b),\n        MIN(c),\n        MAX(d) AS \"MAX of d\",\n        SUM(e)\n    FROM tutorial.aapl_historical_stock_price\n```\n`COUNT` counts total number of **non-null** rows in a particular column.\n`SUM` adds together all the values in a particular column.\n`MIN` and `MAX` return the lowest and highest values in a particular column, respectively.\n`AVG` calculates the average of a group of selected values.\n\n### Others\n`ORDER BY`\n- sorts data, if multiple column names given, sort in order\n- automatically ascending, use DESC to show descending\n\n`GROUP BY` \n- separate data into groups, which can be aggregated independently of one another.\n\n`WHERE`\n- for filtering results\n\n`HAVING`\n- for filtering on aggregate columns\n```sql \nselect Email from Person group by Email having count(Email) > 1;\n```\n\nQuery clause order:\n1. SELECT\n2. FROM\n3. WHERE\n4. GROUP BY\n5. HAVING\n6. ORDER BY\n\n`CASE`\n- SQL's way of handling if/then logic.\n- Goes in the SELECT clause\n- at lease on pair of WHEN and THEN\n- ELSE is optional\n- must end with END\n\n\n`INNER JOIN`\n`LEFT JOIN`\n`RIGHT JOIN`\n`FULL JOIN / FULL OUTER JOIN` Combination of left and right.\n`UNION`\n- Combines tables by stacking/appending. e.g. `SELECT * from T1 UNION SELECT * from T2`\n- Repeated/duplicate rows will not be copied. \n\n`UNION ALL` \n- Repeated/duplicate rows will be copied. \n\n\nSelf Join\n- When a table joins itself to perforom some kind of magic.\n```sql\nSELECT DISTINCT japan_investments.company_name,\n       japan_investments.company_permalink\n  FROM tutorial.crunchbase_investments_part1 japan_investments\n  JOIN tutorial.crunchbase_investments_part1 gb_investments\n    ON japan_investments.company_name = gb_investments.company_name\n   AND gb_investments.investor_country_code = 'GBR'\n   AND gb_investments.funded_at > japan_investments.funded_at\n WHERE japan_investments.investor_country_code = 'JPN'\n ORDER BY 1\n```\n### Examples\n\n```sql\n-- Write a SQL query to get the nth highest salary from the Employee table.\nCREATE FUNCTION getNthHighestSalary(N INT) RETURNS INT\nBEGIN\nDECLARE M INT;\nSET M=N-1;\n  RETURN (\n      SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT M, 1\n  );\nEND\n```\n# Coding\n\n# Stats\n\n# Behavioural\n\n# Resources\n\n<!-- SQL: https://mode.com/sql-tutorial/intro-to-advanced-sql/ -->\n\n","slug":"interview_prep","published":1,"updated":"2020-08-05T15:01:26.160Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kk0000ad64b1te0af7s","content":"<p>A updating collection of notes/resources for interview prep:</p>\n<ul>\n<li>SQL</li>\n<li>Programming</li>\n<li>Stats</li>\n<li>Behavioural<a id=\"more\"></a>\n\n</li>\n</ul>\n<h1 id=\"SQL\"><a href=\"#SQL\" class=\"headerlink\" title=\"SQL\"></a>SQL</h1><h3 id=\"Basics\"><a href=\"#Basics\" class=\"headerlink\" title=\"Basics\"></a>Basics</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a,</span><br><span class=\"line\">        b <span class=\"keyword\">AS</span> <span class=\"string\">\"field b\"</span>,</span><br><span class=\"line\">        c <span class=\"keyword\">AS</span> field_b,</span><br><span class=\"line\">        (c + d)/<span class=\"number\">2</span> <span class=\"keyword\">AS</span> c_d_avg <span class=\"comment\">-- Can do arithmetic operations</span></span><br><span class=\"line\">    <span class=\"keyword\">FROM</span> tablename</span><br><span class=\"line\">    <span class=\"keyword\">WHERE</span> g = <span class=\"number\">1</span>  </span><br><span class=\"line\">        <span class=\"keyword\">AND</span> h <span class=\"keyword\">NOT</span> <span class=\"keyword\">IN</span> (<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">AND</span> i <span class=\"keyword\">IS</span> <span class=\"literal\">NULL</span></span><br><span class=\"line\">        <span class=\"keyword\">AND</span> (a <span class=\"keyword\">ILIKE</span> <span class=\"string\">'%caseinsensitive%'</span> <span class=\"keyword\">OR</span> <span class=\"string\">\"b\"</span> <span class=\"keyword\">LIKE</span> <span class=\"string\">'%CaseSensitive%'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">LIMIT</span> <span class=\"number\">100</span> <span class=\"comment\">-- Limits how many results</span></span><br><span class=\"line\">    <span class=\"keyword\">OFFSET</span> <span class=\"number\">1</span> <span class=\"comment\">-- Skip the first x results</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Logical-operators\"><a href=\"#Logical-operators\" class=\"headerlink\" title=\"Logical operators\"></a>Logical operators</h3><p><code>LIKE</code></p>\n<ul>\n<li>match similar values, case-sensitive</li>\n<li>% : wildcard, any set of characters</li>\n<li>_ : any individual character<br><code>IN</code> </li>\n<li>specify a list of values you’d like to include</li>\n<li>WHERE year_rank IN (1, 2, 3)<br><code>BETWEEN</code></li>\n<li>select only rows within a certain range</li>\n<li>WHERE year_rank BETWEEN 5 AND 10<br><code>IS</code><br><code>AND</code><br><code>OR</code><br><code>NOT</code></li>\n</ul>\n<h3 id=\"Aggregate-functions\"><a href=\"#Aggregate-functions\" class=\"headerlink\" title=\"Aggregate functions\"></a>Aggregate functions</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">COUNT</span>(a),</span><br><span class=\"line\">        <span class=\"keyword\">AVG</span>(b),</span><br><span class=\"line\">        <span class=\"keyword\">MIN</span>(c),</span><br><span class=\"line\">        <span class=\"keyword\">MAX</span>(d) <span class=\"keyword\">AS</span> <span class=\"string\">\"MAX of d\"</span>,</span><br><span class=\"line\">        <span class=\"keyword\">SUM</span>(e)</span><br><span class=\"line\">    <span class=\"keyword\">FROM</span> tutorial.aapl_historical_stock_price</span><br></pre></td></tr></table></figure>\n<p><code>COUNT</code> counts total number of <strong>non-null</strong> rows in a particular column.<br><code>SUM</code> adds together all the values in a particular column.<br><code>MIN</code> and <code>MAX</code> return the lowest and highest values in a particular column, respectively.<br><code>AVG</code> calculates the average of a group of selected values.</p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><p><code>ORDER BY</code></p>\n<ul>\n<li>sorts data, if multiple column names given, sort in order</li>\n<li>automatically ascending, use DESC to show descending</li>\n</ul>\n<p><code>GROUP BY</code> </p>\n<ul>\n<li>separate data into groups, which can be aggregated independently of one another.</li>\n</ul>\n<p><code>WHERE</code></p>\n<ul>\n<li>for filtering results</li>\n</ul>\n<p><code>HAVING</code></p>\n<ul>\n<li>for filtering on aggregate columns<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> Email <span class=\"keyword\">from</span> Person <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> Email <span class=\"keyword\">having</span> <span class=\"keyword\">count</span>(Email) &gt; <span class=\"number\">1</span>;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>Query clause order:</p>\n<ol>\n<li>SELECT</li>\n<li>FROM</li>\n<li>WHERE</li>\n<li>GROUP BY</li>\n<li>HAVING</li>\n<li>ORDER BY</li>\n</ol>\n<p><code>CASE</code></p>\n<ul>\n<li>SQL’s way of handling if/then logic.</li>\n<li>Goes in the SELECT clause</li>\n<li>at lease on pair of WHEN and THEN</li>\n<li>ELSE is optional</li>\n<li>must end with END</li>\n</ul>\n<p><code>INNER JOIN</code><br><code>LEFT JOIN</code><br><code>RIGHT JOIN</code><br><code>FULL JOIN / FULL OUTER JOIN</code> Combination of left and right.<br><code>UNION</code></p>\n<ul>\n<li>Combines tables by stacking/appending. e.g. <code>SELECT * from T1 UNION SELECT * from T2</code></li>\n<li>Repeated/duplicate rows will not be copied. </li>\n</ul>\n<p><code>UNION ALL</code> </p>\n<ul>\n<li>Repeated/duplicate rows will be copied. </li>\n</ul>\n<p>Self Join</p>\n<ul>\n<li>When a table joins itself to perforom some kind of magic.<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span> japan_investments.company_name,</span><br><span class=\"line\">       japan_investments.company_permalink</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.crunchbase_investments_part1 japan_investments</span><br><span class=\"line\">  <span class=\"keyword\">JOIN</span> tutorial.crunchbase_investments_part1 gb_investments</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> japan_investments.company_name = gb_investments.company_name</span><br><span class=\"line\">   <span class=\"keyword\">AND</span> gb_investments.investor_country_code = <span class=\"string\">'GBR'</span></span><br><span class=\"line\">   <span class=\"keyword\">AND</span> gb_investments.funded_at &gt; japan_investments.funded_at</span><br><span class=\"line\"> <span class=\"keyword\">WHERE</span> japan_investments.investor_country_code = <span class=\"string\">'JPN'</span></span><br><span class=\"line\"> <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h3></li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Write a SQL query to get the nth highest salary from the Employee table.</span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">FUNCTION</span> getNthHighestSalary(N <span class=\"built_in\">INT</span>) <span class=\"keyword\">RETURNS</span> <span class=\"built_in\">INT</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span></span><br><span class=\"line\"><span class=\"keyword\">DECLARE</span> M <span class=\"built_in\">INT</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> M=N<span class=\"number\">-1</span>;</span><br><span class=\"line\">  RETURN (</span><br><span class=\"line\">      <span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span> Salary <span class=\"keyword\">FROM</span> Employee <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> Salary <span class=\"keyword\">DESC</span> <span class=\"keyword\">LIMIT</span> M, <span class=\"number\">1</span></span><br><span class=\"line\">  );</span><br><span class=\"line\"><span class=\"keyword\">END</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"Coding\"><a href=\"#Coding\" class=\"headerlink\" title=\"Coding\"></a>Coding</h1><h1 id=\"Stats\"><a href=\"#Stats\" class=\"headerlink\" title=\"Stats\"></a>Stats</h1><h1 id=\"Behavioural\"><a href=\"#Behavioural\" class=\"headerlink\" title=\"Behavioural\"></a>Behavioural</h1><h1 id=\"Resources\"><a href=\"#Resources\" class=\"headerlink\" title=\"Resources\"></a>Resources</h1><!-- SQL: https://mode.com/sql-tutorial/intro-to-advanced-sql/ -->\n\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<p>A updating collection of notes/resources for interview prep:</p>\n<ul>\n<li>SQL</li>\n<li>Programming</li>\n<li>Stats</li>\n<li>Behavioural","more":"</li>\n</ul>\n<h1 id=\"SQL\"><a href=\"#SQL\" class=\"headerlink\" title=\"SQL\"></a>SQL</h1><h3 id=\"Basics\"><a href=\"#Basics\" class=\"headerlink\" title=\"Basics\"></a>Basics</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> a,</span><br><span class=\"line\">        b <span class=\"keyword\">AS</span> <span class=\"string\">\"field b\"</span>,</span><br><span class=\"line\">        c <span class=\"keyword\">AS</span> field_b,</span><br><span class=\"line\">        (c + d)/<span class=\"number\">2</span> <span class=\"keyword\">AS</span> c_d_avg <span class=\"comment\">-- Can do arithmetic operations</span></span><br><span class=\"line\">    <span class=\"keyword\">FROM</span> tablename</span><br><span class=\"line\">    <span class=\"keyword\">WHERE</span> g = <span class=\"number\">1</span>  </span><br><span class=\"line\">        <span class=\"keyword\">AND</span> h <span class=\"keyword\">NOT</span> <span class=\"keyword\">IN</span> (<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\">        <span class=\"keyword\">AND</span> i <span class=\"keyword\">IS</span> <span class=\"literal\">NULL</span></span><br><span class=\"line\">        <span class=\"keyword\">AND</span> (a <span class=\"keyword\">ILIKE</span> <span class=\"string\">'%caseinsensitive%'</span> <span class=\"keyword\">OR</span> <span class=\"string\">\"b\"</span> <span class=\"keyword\">LIKE</span> <span class=\"string\">'%CaseSensitive%'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">LIMIT</span> <span class=\"number\">100</span> <span class=\"comment\">-- Limits how many results</span></span><br><span class=\"line\">    <span class=\"keyword\">OFFSET</span> <span class=\"number\">1</span> <span class=\"comment\">-- Skip the first x results</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Logical-operators\"><a href=\"#Logical-operators\" class=\"headerlink\" title=\"Logical operators\"></a>Logical operators</h3><p><code>LIKE</code></p>\n<ul>\n<li>match similar values, case-sensitive</li>\n<li>% : wildcard, any set of characters</li>\n<li>_ : any individual character<br><code>IN</code> </li>\n<li>specify a list of values you’d like to include</li>\n<li>WHERE year_rank IN (1, 2, 3)<br><code>BETWEEN</code></li>\n<li>select only rows within a certain range</li>\n<li>WHERE year_rank BETWEEN 5 AND 10<br><code>IS</code><br><code>AND</code><br><code>OR</code><br><code>NOT</code></li>\n</ul>\n<h3 id=\"Aggregate-functions\"><a href=\"#Aggregate-functions\" class=\"headerlink\" title=\"Aggregate functions\"></a>Aggregate functions</h3><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">COUNT</span>(a),</span><br><span class=\"line\">        <span class=\"keyword\">AVG</span>(b),</span><br><span class=\"line\">        <span class=\"keyword\">MIN</span>(c),</span><br><span class=\"line\">        <span class=\"keyword\">MAX</span>(d) <span class=\"keyword\">AS</span> <span class=\"string\">\"MAX of d\"</span>,</span><br><span class=\"line\">        <span class=\"keyword\">SUM</span>(e)</span><br><span class=\"line\">    <span class=\"keyword\">FROM</span> tutorial.aapl_historical_stock_price</span><br></pre></td></tr></table></figure>\n<p><code>COUNT</code> counts total number of <strong>non-null</strong> rows in a particular column.<br><code>SUM</code> adds together all the values in a particular column.<br><code>MIN</code> and <code>MAX</code> return the lowest and highest values in a particular column, respectively.<br><code>AVG</code> calculates the average of a group of selected values.</p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><p><code>ORDER BY</code></p>\n<ul>\n<li>sorts data, if multiple column names given, sort in order</li>\n<li>automatically ascending, use DESC to show descending</li>\n</ul>\n<p><code>GROUP BY</code> </p>\n<ul>\n<li>separate data into groups, which can be aggregated independently of one another.</li>\n</ul>\n<p><code>WHERE</code></p>\n<ul>\n<li>for filtering results</li>\n</ul>\n<p><code>HAVING</code></p>\n<ul>\n<li>for filtering on aggregate columns<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> Email <span class=\"keyword\">from</span> Person <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> Email <span class=\"keyword\">having</span> <span class=\"keyword\">count</span>(Email) &gt; <span class=\"number\">1</span>;</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<p>Query clause order:</p>\n<ol>\n<li>SELECT</li>\n<li>FROM</li>\n<li>WHERE</li>\n<li>GROUP BY</li>\n<li>HAVING</li>\n<li>ORDER BY</li>\n</ol>\n<p><code>CASE</code></p>\n<ul>\n<li>SQL’s way of handling if/then logic.</li>\n<li>Goes in the SELECT clause</li>\n<li>at lease on pair of WHEN and THEN</li>\n<li>ELSE is optional</li>\n<li>must end with END</li>\n</ul>\n<p><code>INNER JOIN</code><br><code>LEFT JOIN</code><br><code>RIGHT JOIN</code><br><code>FULL JOIN / FULL OUTER JOIN</code> Combination of left and right.<br><code>UNION</code></p>\n<ul>\n<li>Combines tables by stacking/appending. e.g. <code>SELECT * from T1 UNION SELECT * from T2</code></li>\n<li>Repeated/duplicate rows will not be copied. </li>\n</ul>\n<p><code>UNION ALL</code> </p>\n<ul>\n<li>Repeated/duplicate rows will be copied. </li>\n</ul>\n<p>Self Join</p>\n<ul>\n<li>When a table joins itself to perforom some kind of magic.<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span> japan_investments.company_name,</span><br><span class=\"line\">       japan_investments.company_permalink</span><br><span class=\"line\">  <span class=\"keyword\">FROM</span> tutorial.crunchbase_investments_part1 japan_investments</span><br><span class=\"line\">  <span class=\"keyword\">JOIN</span> tutorial.crunchbase_investments_part1 gb_investments</span><br><span class=\"line\">    <span class=\"keyword\">ON</span> japan_investments.company_name = gb_investments.company_name</span><br><span class=\"line\">   <span class=\"keyword\">AND</span> gb_investments.investor_country_code = <span class=\"string\">'GBR'</span></span><br><span class=\"line\">   <span class=\"keyword\">AND</span> gb_investments.funded_at &gt; japan_investments.funded_at</span><br><span class=\"line\"> <span class=\"keyword\">WHERE</span> japan_investments.investor_country_code = <span class=\"string\">'JPN'</span></span><br><span class=\"line\"> <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h3></li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Write a SQL query to get the nth highest salary from the Employee table.</span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">FUNCTION</span> getNthHighestSalary(N <span class=\"built_in\">INT</span>) <span class=\"keyword\">RETURNS</span> <span class=\"built_in\">INT</span></span><br><span class=\"line\"><span class=\"keyword\">BEGIN</span></span><br><span class=\"line\"><span class=\"keyword\">DECLARE</span> M <span class=\"built_in\">INT</span>;</span><br><span class=\"line\"><span class=\"keyword\">SET</span> M=N<span class=\"number\">-1</span>;</span><br><span class=\"line\">  RETURN (</span><br><span class=\"line\">      <span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span> Salary <span class=\"keyword\">FROM</span> Employee <span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> Salary <span class=\"keyword\">DESC</span> <span class=\"keyword\">LIMIT</span> M, <span class=\"number\">1</span></span><br><span class=\"line\">  );</span><br><span class=\"line\"><span class=\"keyword\">END</span></span><br></pre></td></tr></table></figure>\n<h1 id=\"Coding\"><a href=\"#Coding\" class=\"headerlink\" title=\"Coding\"></a>Coding</h1><h1 id=\"Stats\"><a href=\"#Stats\" class=\"headerlink\" title=\"Stats\"></a>Stats</h1><h1 id=\"Behavioural\"><a href=\"#Behavioural\" class=\"headerlink\" title=\"Behavioural\"></a>Behavioural</h1><h1 id=\"Resources\"><a href=\"#Resources\" class=\"headerlink\" title=\"Resources\"></a>Resources</h1><!-- SQL: https://mode.com/sql-tutorial/intro-to-advanced-sql/ -->"},{"title":"Finite Markov Decision Processes & Rewards","date":"2020-06-18T19:00:13.000Z","_content":"\nMDPs are a classical formalization of sequential decision making, where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. \n\nThus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value q ⇤ (a) of each action a, in MDPs we estimate the value q ⇤ (s, a) of each action a in each state s, or we estimate the value v ⇤ (s) of each state given optimal action selections. \n\n{% asset_img mdp0.png cycle %}\n\nFor a problem, as long as you can identify States, Actions, and Rewards. It can be framed using RL.\n<!-- more -->\n\n## *dynamics* of MDP\n{% asset_img mdp1.png dynamics of the MDP %}\n\n## Recycling Robot example\n\n{% asset_img mdp2.png MDP graph %}\n\n## Rewards & Goals\n\nThe reward signal is your way of communicating to the robot ***what*** you want it to achieve, not ***how*** you want it achieved\n\n**Episodic** tasks: Tasks that have a set terminal state. e.g. a chess game.\n\n**Continuing** tasks: Tasks that can continue forever. e.g. balance a pole.\n**Discounting**: Decrease future rewards by discounting factor. Infinite sum of rewards remain bounded.\n\n{% asset_img G0.png Return denoted as G %}\n{% asset_img G1.png Infinite sum when gamma < 1 and >0 %}\n","source":"_posts/rl-mdp.md","raw":"---\ntitle: Finite Markov Decision Processes & Rewards\ndate: 2020-06-18 15:00:13\ntags: \n- 笔记\n- RL\ncategories: Data Science\n\n---\n\nMDPs are a classical formalization of sequential decision making, where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. \n\nThus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value q ⇤ (a) of each action a, in MDPs we estimate the value q ⇤ (s, a) of each action a in each state s, or we estimate the value v ⇤ (s) of each state given optimal action selections. \n\n{% asset_img mdp0.png cycle %}\n\nFor a problem, as long as you can identify States, Actions, and Rewards. It can be framed using RL.\n<!-- more -->\n\n## *dynamics* of MDP\n{% asset_img mdp1.png dynamics of the MDP %}\n\n## Recycling Robot example\n\n{% asset_img mdp2.png MDP graph %}\n\n## Rewards & Goals\n\nThe reward signal is your way of communicating to the robot ***what*** you want it to achieve, not ***how*** you want it achieved\n\n**Episodic** tasks: Tasks that have a set terminal state. e.g. a chess game.\n\n**Continuing** tasks: Tasks that can continue forever. e.g. balance a pole.\n**Discounting**: Decrease future rewards by discounting factor. Infinite sum of rewards remain bounded.\n\n{% asset_img G0.png Return denoted as G %}\n{% asset_img G1.png Infinite sum when gamma < 1 and >0 %}\n","slug":"rl-mdp","published":1,"updated":"2020-08-05T15:03:01.676Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kk1000ed64b3bi46qy4","content":"<p>MDPs are a classical formalization of sequential decision making, where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. </p>\n<p>Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value q ⇤ (a) of each action a, in MDPs we estimate the value q ⇤ (s, a) of each action a in each state s, or we estimate the value v ⇤ (s) of each state given optimal action selections. </p>\n<img src=\"/blog/2020/06/18/rl-mdp/mdp0.png\" class=\"\" title=\"cycle\">\n\n<p>For a problem, as long as you can identify States, Actions, and Rewards. It can be framed using RL.</p>\n<a id=\"more\"></a>\n\n<h2 id=\"dynamics-of-MDP\"><a href=\"#dynamics-of-MDP\" class=\"headerlink\" title=\"dynamics of MDP\"></a><em>dynamics</em> of MDP</h2><img src=\"/blog/2020/06/18/rl-mdp/mdp1.png\" class=\"\" title=\"dynamics of the MDP\">\n\n<h2 id=\"Recycling-Robot-example\"><a href=\"#Recycling-Robot-example\" class=\"headerlink\" title=\"Recycling Robot example\"></a>Recycling Robot example</h2><img src=\"/blog/2020/06/18/rl-mdp/mdp2.png\" class=\"\" title=\"MDP graph\">\n\n<h2 id=\"Rewards-amp-Goals\"><a href=\"#Rewards-amp-Goals\" class=\"headerlink\" title=\"Rewards &amp; Goals\"></a>Rewards &amp; Goals</h2><p>The reward signal is your way of communicating to the robot <strong><em>what</em></strong> you want it to achieve, not <strong><em>how</em></strong> you want it achieved</p>\n<p><strong>Episodic</strong> tasks: Tasks that have a set terminal state. e.g. a chess game.</p>\n<p><strong>Continuing</strong> tasks: Tasks that can continue forever. e.g. balance a pole.<br><strong>Discounting</strong>: Decrease future rewards by discounting factor. Infinite sum of rewards remain bounded.</p>\n<img src=\"/blog/2020/06/18/rl-mdp/G0.png\" class=\"\" title=\"Return denoted as G\">\n<img src=\"/blog/2020/06/18/rl-mdp/G1.png\" class=\"\" title=\"Infinite sum when gamma &lt; 1 and &gt;0\">\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<p>MDPs are a classical formalization of sequential decision making, where actions inﬂuence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. </p>\n<p>Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward. Whereas in bandit problems we estimated the value q ⇤ (a) of each action a, in MDPs we estimate the value q ⇤ (s, a) of each action a in each state s, or we estimate the value v ⇤ (s) of each state given optimal action selections. </p>\n<img src=\"/blog/2020/06/18/rl-mdp/mdp0.png\" class=\"\" title=\"cycle\">\n\n<p>For a problem, as long as you can identify States, Actions, and Rewards. It can be framed using RL.</p>","more":"<h2 id=\"dynamics-of-MDP\"><a href=\"#dynamics-of-MDP\" class=\"headerlink\" title=\"dynamics of MDP\"></a><em>dynamics</em> of MDP</h2><img src=\"/blog/2020/06/18/rl-mdp/mdp1.png\" class=\"\" title=\"dynamics of the MDP\">\n\n<h2 id=\"Recycling-Robot-example\"><a href=\"#Recycling-Robot-example\" class=\"headerlink\" title=\"Recycling Robot example\"></a>Recycling Robot example</h2><img src=\"/blog/2020/06/18/rl-mdp/mdp2.png\" class=\"\" title=\"MDP graph\">\n\n<h2 id=\"Rewards-amp-Goals\"><a href=\"#Rewards-amp-Goals\" class=\"headerlink\" title=\"Rewards &amp; Goals\"></a>Rewards &amp; Goals</h2><p>The reward signal is your way of communicating to the robot <strong><em>what</em></strong> you want it to achieve, not <strong><em>how</em></strong> you want it achieved</p>\n<p><strong>Episodic</strong> tasks: Tasks that have a set terminal state. e.g. a chess game.</p>\n<p><strong>Continuing</strong> tasks: Tasks that can continue forever. e.g. balance a pole.<br><strong>Discounting</strong>: Decrease future rewards by discounting factor. Infinite sum of rewards remain bounded.</p>\n<img src=\"/blog/2020/06/18/rl-mdp/G0.png\" class=\"\" title=\"Return denoted as G\">\n<img src=\"/blog/2020/06/18/rl-mdp/G1.png\" class=\"\" title=\"Infinite sum when gamma &lt; 1 and &gt;0\">"},{"title":"《香蕉》","date":"2020-07-02T02:01:56.000Z","_content":"## 《香蕉》\n\n我在房间剥香蕉，\n谁知力道刚刚好，\n香蕉对半折了腰，\n恰好落进垃圾篓，\n留我凌乱在心头。\n\n此蕉与彼蕉，\n失于刹那间，\n似不可兼得！\n若大小忧乐皆无常，\n以何决策机良与否？","source":"_posts/香蕉.md","raw":"---\ntitle: 《香蕉》\ndate: 2020-07-01 22:01:56\ntags: \n- 诗\ncategories: 随笔\n---\n## 《香蕉》\n\n我在房间剥香蕉，\n谁知力道刚刚好，\n香蕉对半折了腰，\n恰好落进垃圾篓，\n留我凌乱在心头。\n\n此蕉与彼蕉，\n失于刹那间，\n似不可兼得！\n若大小忧乐皆无常，\n以何决策机良与否？","slug":"香蕉","published":1,"updated":"2020-08-05T15:04:58.294Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5klc001cd64b7torh44g","content":"<h2 id=\"《香蕉》\"><a href=\"#《香蕉》\" class=\"headerlink\" title=\"《香蕉》\"></a>《香蕉》</h2><p>我在房间剥香蕉，<br>谁知力道刚刚好，<br>香蕉对半折了腰，<br>恰好落进垃圾篓，<br>留我凌乱在心头。</p>\n<p>此蕉与彼蕉，<br>失于刹那间，<br>似不可兼得！<br>若大小忧乐皆无常，<br>以何决策机良与否？</p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<h2 id=\"《香蕉》\"><a href=\"#《香蕉》\" class=\"headerlink\" title=\"《香蕉》\"></a>《香蕉》</h2><p>我在房间剥香蕉，<br>谁知力道刚刚好，<br>香蕉对半折了腰，<br>恰好落进垃圾篓，<br>留我凌乱在心头。</p>\n<p>此蕉与彼蕉，<br>失于刹那间，<br>似不可兼得！<br>若大小忧乐皆无常，<br>以何决策机良与否？</p>\n"},{"title":"杨绛语录","date":"2020-05-28T00:26:07.000Z","_content":"\n> 1. 你的问题主要在于读书不多而想得太多。\n2. 如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。\n3. 我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！\n4. 有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。\n5. 少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。\n6. 在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。\n7. 惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。\n8. 上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。\n9. 世间好物不坚牢，彩云易散琉璃脆。\n10. 我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。\n11. 月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。\n12. 我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。\n","source":"_posts/杨绛语录.md","raw":"---\ntitle: 杨绛语录\ndate: 2020-05-27 20:26:07\ntags: \ncategories: 摘录\n\n---\n\n> 1. 你的问题主要在于读书不多而想得太多。\n2. 如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。\n3. 我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！\n4. 有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。\n5. 少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。\n6. 在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。\n7. 惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。\n8. 上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。\n9. 世间好物不坚牢，彩云易散琉璃脆。\n10. 我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。\n11. 月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。\n12. 我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。\n","slug":"杨绛语录","published":1,"updated":"2020-08-05T15:04:51.545Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kle001ed64bg3yb2c7p","content":"<blockquote>\n<ol>\n<li>你的问题主要在于读书不多而想得太多。</li>\n<li>如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。</li>\n<li>我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！</li>\n<li>有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。</li>\n<li>少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。</li>\n<li>在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。</li>\n<li>惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。</li>\n<li>上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。</li>\n<li>世间好物不坚牢，彩云易散琉璃脆。</li>\n<li>我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。</li>\n<li>月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。</li>\n<li>我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。</li>\n</ol>\n</blockquote>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<blockquote>\n<ol>\n<li>你的问题主要在于读书不多而想得太多。</li>\n<li>如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。</li>\n<li>我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！</li>\n<li>有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。</li>\n<li>少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。</li>\n<li>在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。</li>\n<li>惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。</li>\n<li>上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。</li>\n<li>世间好物不坚牢，彩云易散琉璃脆。</li>\n<li>我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。</li>\n<li>月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。</li>\n<li>我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。</li>\n</ol>\n</blockquote>\n"},{"title":"RL Environment Setup","date":"2020-07-09T12:54:09.000Z","_content":"\n\n````python\nimport numpy as np\n\nclass healtius_env(object):\n    def __init__(self, symptomsQuestions, diseases):\n        \"\"\"\n        symptomsQuestions: \n            dictionary of questions mapped to symptoms.\n            e.g. { \"asthma\" : \"Have you been diagnosed with asthma?\", \"sweating_more\" : \"Are you sweating more than usual?\"}\n        diseases: \n            list of terminal states / diagnosis\n            e.g. [diabetes, fever, ...]\n        \"\"\"\n        self.symptomsQuestions = symptomsQuestions\n        self.diseases = diseases\n\n        self.possibleActions = list(self.symptomsQuestions.keys()) + diseases\n        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))\n        self.takenActions = []\n\n    def reset(self):\n        \"\"\"\n        Resets the state of the environment and returns an initial observation.\n        Returns:\n            observation (object): the initial observation.\n        \"\"\"\n        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))\n        return self.symptomState\n    \n    \n    def step(self, action):\n        \"\"\"\n        Run one timestep of the environment's dynamics. When end of\n        episode is reached, you are responsible for calling `reset()`\n        to reset this environment's state.\n        Accepts an action and returns a tuple (observation, reward, done, info).\n        Args:\n            action (object): an action provided by the agent\n        Returns:\n            observation (object): agent's observation of the current environment (state)\n            reward (float) : amount of reward returned after previous action\n            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n        \"\"\"\n\n        # Update State\n        answer = self.getAnswer(action)\n        index = list(self.symptomsQuestions.keys()).index(action)\n        self.symptomState[index] = answer\n\n        # Calculate Reward\n        if action in self.takenActions:\n            reward = -1\n        elif action in self.diseases:\n            reward = 1\n        else: reward = 0\n\n        # Is Terminal State\n        done = action in self.diseases\n\n        # Add Action\n        self.takenActions.append(action)\n\n        \n        return self.symptomState, reward, done, None\n\n    def getAnswer(self, action):\n        \"\"\"\n        Asks action to user, get response from user\n        Assumption:\n            - user response is an integer\n        \"\"\"\n        answer = input(self.symptomsQuestions[action])\n        return answer\n\n\n````","source":"_posts/rl-env.md","raw":"---\ntitle: RL environment setup\ndate: 2020-07-09 08:54:09\ntags:\n- 笔记\n- RL\n- DQN\ncategories: Data Science\n---\n\n\n````python\nimport numpy as np\n\nclass healtius_env(object):\n    def __init__(self, symptomsQuestions, diseases):\n        \"\"\"\n        symptomsQuestions: \n            dictionary of questions mapped to symptoms.\n            e.g. { \"asthma\" : \"Have you been diagnosed with asthma?\", \"sweating_more\" : \"Are you sweating more than usual?\"}\n        diseases: \n            list of terminal states / diagnosis\n            e.g. [diabetes, fever, ...]\n        \"\"\"\n        self.symptomsQuestions = symptomsQuestions\n        self.diseases = diseases\n\n        self.possibleActions = list(self.symptomsQuestions.keys()) + diseases\n        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))\n        self.takenActions = []\n\n    def reset(self):\n        \"\"\"\n        Resets the state of the environment and returns an initial observation.\n        Returns:\n            observation (object): the initial observation.\n        \"\"\"\n        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))\n        return self.symptomState\n    \n    \n    def step(self, action):\n        \"\"\"\n        Run one timestep of the environment's dynamics. When end of\n        episode is reached, you are responsible for calling `reset()`\n        to reset this environment's state.\n        Accepts an action and returns a tuple (observation, reward, done, info).\n        Args:\n            action (object): an action provided by the agent\n        Returns:\n            observation (object): agent's observation of the current environment (state)\n            reward (float) : amount of reward returned after previous action\n            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n        \"\"\"\n\n        # Update State\n        answer = self.getAnswer(action)\n        index = list(self.symptomsQuestions.keys()).index(action)\n        self.symptomState[index] = answer\n\n        # Calculate Reward\n        if action in self.takenActions:\n            reward = -1\n        elif action in self.diseases:\n            reward = 1\n        else: reward = 0\n\n        # Is Terminal State\n        done = action in self.diseases\n\n        # Add Action\n        self.takenActions.append(action)\n\n        \n        return self.symptomState, reward, done, None\n\n    def getAnswer(self, action):\n        \"\"\"\n        Asks action to user, get response from user\n        Assumption:\n            - user response is an integer\n        \"\"\"\n        answer = input(self.symptomsQuestions[action])\n        return answer\n\n\n````","slug":"rl-env","published":1,"updated":"2020-07-09T20:04:21.469Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5klj001hd64b7zy4bpvh","content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">healtius_env</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, symptomsQuestions, diseases)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        symptomsQuestions: </span></span><br><span class=\"line\"><span class=\"string\">            dictionary of questions mapped to symptoms.</span></span><br><span class=\"line\"><span class=\"string\">            e.g. &#123; \"asthma\" : \"Have you been diagnosed with asthma?\", \"sweating_more\" : \"Are you sweating more than usual?\"&#125;</span></span><br><span class=\"line\"><span class=\"string\">        diseases: </span></span><br><span class=\"line\"><span class=\"string\">            list of terminal states / diagnosis</span></span><br><span class=\"line\"><span class=\"string\">            e.g. [diabetes, fever, ...]</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        self.symptomsQuestions = symptomsQuestions</span><br><span class=\"line\">        self.diseases = diseases</span><br><span class=\"line\"></span><br><span class=\"line\">        self.possibleActions = list(self.symptomsQuestions.keys()) + diseases</span><br><span class=\"line\">        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))</span><br><span class=\"line\">        self.takenActions = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Resets the state of the environment and returns an initial observation.</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            observation (object): the initial observation.</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.symptomState</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">step</span><span class=\"params\">(self, action)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Run one timestep of the environment's dynamics. When end of</span></span><br><span class=\"line\"><span class=\"string\">        episode is reached, you are responsible for calling `reset()`</span></span><br><span class=\"line\"><span class=\"string\">        to reset this environment's state.</span></span><br><span class=\"line\"><span class=\"string\">        Accepts an action and returns a tuple (observation, reward, done, info).</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            action (object): an action provided by the agent</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            observation (object): agent's observation of the current environment (state)</span></span><br><span class=\"line\"><span class=\"string\">            reward (float) : amount of reward returned after previous action</span></span><br><span class=\"line\"><span class=\"string\">            done (bool): whether the episode has ended, in which case further step() calls will return undefined results</span></span><br><span class=\"line\"><span class=\"string\">            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update State</span></span><br><span class=\"line\">        answer = self.getAnswer(action)</span><br><span class=\"line\">        index = list(self.symptomsQuestions.keys()).index(action)</span><br><span class=\"line\">        self.symptomState[index] = answer</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate Reward</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> action <span class=\"keyword\">in</span> self.takenActions:</span><br><span class=\"line\">            reward = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> action <span class=\"keyword\">in</span> self.diseases:</span><br><span class=\"line\">            reward = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: reward = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Is Terminal State</span></span><br><span class=\"line\">        done = action <span class=\"keyword\">in</span> self.diseases</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Add Action</span></span><br><span class=\"line\">        self.takenActions.append(action)</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.symptomState, reward, done, <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAnswer</span><span class=\"params\">(self, action)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Asks action to user, get response from user</span></span><br><span class=\"line\"><span class=\"string\">        Assumption:</span></span><br><span class=\"line\"><span class=\"string\">            - user response is an integer</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        answer = input(self.symptomsQuestions[action])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> answer</span><br></pre></td></tr></table></figure>","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">healtius_env</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, symptomsQuestions, diseases)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        symptomsQuestions: </span></span><br><span class=\"line\"><span class=\"string\">            dictionary of questions mapped to symptoms.</span></span><br><span class=\"line\"><span class=\"string\">            e.g. &#123; \"asthma\" : \"Have you been diagnosed with asthma?\", \"sweating_more\" : \"Are you sweating more than usual?\"&#125;</span></span><br><span class=\"line\"><span class=\"string\">        diseases: </span></span><br><span class=\"line\"><span class=\"string\">            list of terminal states / diagnosis</span></span><br><span class=\"line\"><span class=\"string\">            e.g. [diabetes, fever, ...]</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        self.symptomsQuestions = symptomsQuestions</span><br><span class=\"line\">        self.diseases = diseases</span><br><span class=\"line\"></span><br><span class=\"line\">        self.possibleActions = list(self.symptomsQuestions.keys()) + diseases</span><br><span class=\"line\">        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))</span><br><span class=\"line\">        self.takenActions = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reset</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Resets the state of the environment and returns an initial observation.</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            observation (object): the initial observation.</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        self.symptomState = np.zeros(len(self.symptomsQuestions.keys()))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.symptomState</span><br><span class=\"line\">    </span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">step</span><span class=\"params\">(self, action)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Run one timestep of the environment's dynamics. When end of</span></span><br><span class=\"line\"><span class=\"string\">        episode is reached, you are responsible for calling `reset()`</span></span><br><span class=\"line\"><span class=\"string\">        to reset this environment's state.</span></span><br><span class=\"line\"><span class=\"string\">        Accepts an action and returns a tuple (observation, reward, done, info).</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            action (object): an action provided by the agent</span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">            observation (object): agent's observation of the current environment (state)</span></span><br><span class=\"line\"><span class=\"string\">            reward (float) : amount of reward returned after previous action</span></span><br><span class=\"line\"><span class=\"string\">            done (bool): whether the episode has ended, in which case further step() calls will return undefined results</span></span><br><span class=\"line\"><span class=\"string\">            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Update State</span></span><br><span class=\"line\">        answer = self.getAnswer(action)</span><br><span class=\"line\">        index = list(self.symptomsQuestions.keys()).index(action)</span><br><span class=\"line\">        self.symptomState[index] = answer</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Calculate Reward</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> action <span class=\"keyword\">in</span> self.takenActions:</span><br><span class=\"line\">            reward = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> action <span class=\"keyword\">in</span> self.diseases:</span><br><span class=\"line\">            reward = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: reward = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Is Terminal State</span></span><br><span class=\"line\">        done = action <span class=\"keyword\">in</span> self.diseases</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># Add Action</span></span><br><span class=\"line\">        self.takenActions.append(action)</span><br><span class=\"line\"></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.symptomState, reward, done, <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getAnswer</span><span class=\"params\">(self, action)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Asks action to user, get response from user</span></span><br><span class=\"line\"><span class=\"string\">        Assumption:</span></span><br><span class=\"line\"><span class=\"string\">            - user response is an integer</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        answer = input(self.symptomsQuestions[action])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> answer</span><br></pre></td></tr></table></figure>"},{"title":"Policies, Value Functions, Optimality","date":"2020-06-22T19:31:56.000Z","mathjax":true,"_content":"\n## 1. Policies\n\nFormally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a | s)$ is the probability that $A_t=a$ if $S_t=s$.\n\n**Deterministic** policy: A policy that maps each state to a single action.\n\n**Stochastic** policy: A policy where multiple actions may be selected with non-zero probability. The probabilities are non-negative and sum to 1.\n\n***Policies tell an agent how to behave in their environment.***\n\n***Policy can depend only on the current state, and not other things like time or previous states.***\n\n<!-- more -->\n\n## 2. Value functions\nValue functions estimate future returns under a specific policy.\n\n### **State-value function**\nThe expected return when starting in s and following policy $\\pi$ thereafter.\n{% asset_img statevalue.png (G: return) %}\n{% asset_img v.png  %}\n\n\n### **Action-value function**\nThe value of taking action a in state s and following policy $\\pi$ thereafter.\n{% asset_img actionvalue.png  %}\n{% asset_img q.png  %}\n\nValue functions allow an agent to query the quality of its current situation instead of waiting to observe the long-term outcome. \n\nBenefits:\n- The return is not immediately available.\n- The return may be random due to stochasticity in both the policy and environment dynamics. \n\nThe value function summarizes all the possible futures by averaging over returns. Ultimately, we care most about ***learning a good policy***. Value function enable us to judge the quality of different policies.\n\n### Bellman equation for $v_\\pi$\n\nExpresses a relationship between the ***value of a state*** and the ***values of its successor states.***\n\n<!-- It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\n(Refer to previous definitions of v) -->\n\nThe Bellman equation for the state value function gives the value of the current state as a sum over the values of all the successor states, and immediate rewards. \n\n{% asset_img bellman.png Bellman Equation %}\n\n\n$\\sum$ (Action $a$'s probability under current state $s$ $\\times \\sum$  (probability of next state *s'* given $a$ is taken under $s$) $\\times$ (corresponding reward $r$ + discounted value of state *s'* ))\n\n\n{% asset_img tree.png Explains 3.14 graphically %}\n\n\n{% asset_img grid.png Example of finite MDP %}\n\n\n### Using Bellman equation and computing $v$\n\n{% asset_img computingV1.png \"Simple case: only one future state for each action\" %}\n{% asset_img computingV2.png Now solve linear equations %}\n\n[Excercises Answers](https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions)\n\n## 3. Optimality\n\n\nThere is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi *$.\n\n### Optimal state value function ($v_*$)\n\n{% asset_img optimalv.png %}\n\n\n### Optimal action value function ($q_*$)\n{% asset_img optimalq.png %}\n{% asset_img optimal_graphical.png %}\n\n### Optimal policy\n{% asset_img optimalpolicy.png %}\n\n","source":"_posts/rl-policy-valuefunct.md","raw":"---\ntitle: Policies, Value functions, Optimality\ndate: 2020-06-22 15:31:56\ntags:\n- 笔记\n- RL\ncategories: Data Science\nmathjax: true\n---\n\n## 1. Policies\n\nFormally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a | s)$ is the probability that $A_t=a$ if $S_t=s$.\n\n**Deterministic** policy: A policy that maps each state to a single action.\n\n**Stochastic** policy: A policy where multiple actions may be selected with non-zero probability. The probabilities are non-negative and sum to 1.\n\n***Policies tell an agent how to behave in their environment.***\n\n***Policy can depend only on the current state, and not other things like time or previous states.***\n\n<!-- more -->\n\n## 2. Value functions\nValue functions estimate future returns under a specific policy.\n\n### **State-value function**\nThe expected return when starting in s and following policy $\\pi$ thereafter.\n{% asset_img statevalue.png (G: return) %}\n{% asset_img v.png  %}\n\n\n### **Action-value function**\nThe value of taking action a in state s and following policy $\\pi$ thereafter.\n{% asset_img actionvalue.png  %}\n{% asset_img q.png  %}\n\nValue functions allow an agent to query the quality of its current situation instead of waiting to observe the long-term outcome. \n\nBenefits:\n- The return is not immediately available.\n- The return may be random due to stochasticity in both the policy and environment dynamics. \n\nThe value function summarizes all the possible futures by averaging over returns. Ultimately, we care most about ***learning a good policy***. Value function enable us to judge the quality of different policies.\n\n### Bellman equation for $v_\\pi$\n\nExpresses a relationship between the ***value of a state*** and the ***values of its successor states.***\n\n<!-- It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\n(Refer to previous definitions of v) -->\n\nThe Bellman equation for the state value function gives the value of the current state as a sum over the values of all the successor states, and immediate rewards. \n\n{% asset_img bellman.png Bellman Equation %}\n\n\n$\\sum$ (Action $a$'s probability under current state $s$ $\\times \\sum$  (probability of next state *s'* given $a$ is taken under $s$) $\\times$ (corresponding reward $r$ + discounted value of state *s'* ))\n\n\n{% asset_img tree.png Explains 3.14 graphically %}\n\n\n{% asset_img grid.png Example of finite MDP %}\n\n\n### Using Bellman equation and computing $v$\n\n{% asset_img computingV1.png \"Simple case: only one future state for each action\" %}\n{% asset_img computingV2.png Now solve linear equations %}\n\n[Excercises Answers](https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions)\n\n## 3. Optimality\n\n\nThere is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi *$.\n\n### Optimal state value function ($v_*$)\n\n{% asset_img optimalv.png %}\n\n\n### Optimal action value function ($q_*$)\n{% asset_img optimalq.png %}\n{% asset_img optimal_graphical.png %}\n\n### Optimal policy\n{% asset_img optimalpolicy.png %}\n\n","slug":"rl-policy-valuefunct","published":1,"updated":"2020-08-05T15:03:18.458Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckdqd5kll001kd64b0lhhaovm","content":"<h2 id=\"1-Policies\"><a href=\"#1-Policies\" class=\"headerlink\" title=\"1. Policies\"></a>1. Policies</h2><p>Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a | s)$ is the probability that $A_t=a$ if $S_t=s$.</p>\n<p><strong>Deterministic</strong> policy: A policy that maps each state to a single action.</p>\n<p><strong>Stochastic</strong> policy: A policy where multiple actions may be selected with non-zero probability. The probabilities are non-negative and sum to 1.</p>\n<p><strong><em>Policies tell an agent how to behave in their environment.</em></strong></p>\n<p><strong><em>Policy can depend only on the current state, and not other things like time or previous states.</em></strong></p>\n<a id=\"more\"></a>\n\n<h2 id=\"2-Value-functions\"><a href=\"#2-Value-functions\" class=\"headerlink\" title=\"2. Value functions\"></a>2. Value functions</h2><p>Value functions estimate future returns under a specific policy.</p>\n<h3 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State-value function\"></a><strong>State-value function</strong></h3><p>The expected return when starting in s and following policy $\\pi$ thereafter.</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/statevalue.png\" class=\"\" title=\"(G: return)\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/v.png\" class=\"\">\n\n\n<h3 id=\"Action-value-function\"><a href=\"#Action-value-function\" class=\"headerlink\" title=\"Action-value function\"></a><strong>Action-value function</strong></h3><p>The value of taking action a in state s and following policy $\\pi$ thereafter.</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/actionvalue.png\" class=\"\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/q.png\" class=\"\">\n\n<p>Value functions allow an agent to query the quality of its current situation instead of waiting to observe the long-term outcome. </p>\n<p>Benefits:</p>\n<ul>\n<li>The return is not immediately available.</li>\n<li>The return may be random due to stochasticity in both the policy and environment dynamics. </li>\n</ul>\n<p>The value function summarizes all the possible futures by averaging over returns. Ultimately, we care most about <strong><em>learning a good policy</em></strong>. Value function enable us to judge the quality of different policies.</p>\n<h3 id=\"Bellman-equation-for-v-pi\"><a href=\"#Bellman-equation-for-v-pi\" class=\"headerlink\" title=\"Bellman equation for $v_\\pi$\"></a>Bellman equation for $v_\\pi$</h3><p>Expresses a relationship between the <strong><em>value of a state</em></strong> and the <strong><em>values of its successor states.</em></strong></p>\n<!-- It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\n(Refer to previous definitions of v) -->\n\n<p>The Bellman equation for the state value function gives the value of the current state as a sum over the values of all the successor states, and immediate rewards. </p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/bellman.png\" class=\"\" title=\"Bellman Equation\">\n\n\n<p>$\\sum$ (Action $a$’s probability under current state $s$ $\\times \\sum$  (probability of next state <em>s’</em> given $a$ is taken under $s$) $\\times$ (corresponding reward $r$ + discounted value of state <em>s’</em> ))</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/tree.png\" class=\"\" title=\"Explains 3.14 graphically\">\n\n\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/grid.png\" class=\"\" title=\"Example of finite MDP\">\n\n\n<h3 id=\"Using-Bellman-equation-and-computing-v\"><a href=\"#Using-Bellman-equation-and-computing-v\" class=\"headerlink\" title=\"Using Bellman equation and computing $v$\"></a>Using Bellman equation and computing $v$</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/computingV1.png\" class=\"\" title=\"Simple case: only one future state for each action\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/computingV2.png\" class=\"\" title=\"Now solve linear equations\">\n\n<p><a href=\"https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions\" target=\"_blank\" rel=\"noopener\">Excercises Answers</a></p>\n<h2 id=\"3-Optimality\"><a href=\"#3-Optimality\" class=\"headerlink\" title=\"3. Optimality\"></a>3. Optimality</h2><p>There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi *$.</p>\n<h3 id=\"Optimal-state-value-function-v\"><a href=\"#Optimal-state-value-function-v\" class=\"headerlink\" title=\"Optimal state value function ($v_*$)\"></a>Optimal state value function ($v_*$)</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalv.png\" class=\"\">\n\n\n<h3 id=\"Optimal-action-value-function-q\"><a href=\"#Optimal-action-value-function-q\" class=\"headerlink\" title=\"Optimal action value function ($q_*$)\"></a>Optimal action value function ($q_*$)</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalq.png\" class=\"\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimal_graphical.png\" class=\"\">\n\n<h3 id=\"Optimal-policy\"><a href=\"#Optimal-policy\" class=\"headerlink\" title=\"Optimal policy\"></a>Optimal policy</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalpolicy.png\" class=\"\">\n\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"<h2 id=\"1-Policies\"><a href=\"#1-Policies\" class=\"headerlink\" title=\"1. Policies\"></a>1. Policies</h2><p>Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$ at time $t$, then $\\pi(a | s)$ is the probability that $A_t=a$ if $S_t=s$.</p>\n<p><strong>Deterministic</strong> policy: A policy that maps each state to a single action.</p>\n<p><strong>Stochastic</strong> policy: A policy where multiple actions may be selected with non-zero probability. The probabilities are non-negative and sum to 1.</p>\n<p><strong><em>Policies tell an agent how to behave in their environment.</em></strong></p>\n<p><strong><em>Policy can depend only on the current state, and not other things like time or previous states.</em></strong></p>","more":"<h2 id=\"2-Value-functions\"><a href=\"#2-Value-functions\" class=\"headerlink\" title=\"2. Value functions\"></a>2. Value functions</h2><p>Value functions estimate future returns under a specific policy.</p>\n<h3 id=\"State-value-function\"><a href=\"#State-value-function\" class=\"headerlink\" title=\"State-value function\"></a><strong>State-value function</strong></h3><p>The expected return when starting in s and following policy $\\pi$ thereafter.</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/statevalue.png\" class=\"\" title=\"(G: return)\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/v.png\" class=\"\">\n\n\n<h3 id=\"Action-value-function\"><a href=\"#Action-value-function\" class=\"headerlink\" title=\"Action-value function\"></a><strong>Action-value function</strong></h3><p>The value of taking action a in state s and following policy $\\pi$ thereafter.</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/actionvalue.png\" class=\"\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/q.png\" class=\"\">\n\n<p>Value functions allow an agent to query the quality of its current situation instead of waiting to observe the long-term outcome. </p>\n<p>Benefits:</p>\n<ul>\n<li>The return is not immediately available.</li>\n<li>The return may be random due to stochasticity in both the policy and environment dynamics. </li>\n</ul>\n<p>The value function summarizes all the possible futures by averaging over returns. Ultimately, we care most about <strong><em>learning a good policy</em></strong>. Value function enable us to judge the quality of different policies.</p>\n<h3 id=\"Bellman-equation-for-v-pi\"><a href=\"#Bellman-equation-for-v-pi\" class=\"headerlink\" title=\"Bellman equation for $v_\\pi$\"></a>Bellman equation for $v_\\pi$</h3><p>Expresses a relationship between the <strong><em>value of a state</em></strong> and the <strong><em>values of its successor states.</em></strong></p>\n<!-- It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\n(Refer to previous definitions of v) -->\n\n<p>The Bellman equation for the state value function gives the value of the current state as a sum over the values of all the successor states, and immediate rewards. </p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/bellman.png\" class=\"\" title=\"Bellman Equation\">\n\n\n<p>$\\sum$ (Action $a$’s probability under current state $s$ $\\times \\sum$  (probability of next state <em>s’</em> given $a$ is taken under $s$) $\\times$ (corresponding reward $r$ + discounted value of state <em>s’</em> ))</p>\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/tree.png\" class=\"\" title=\"Explains 3.14 graphically\">\n\n\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/grid.png\" class=\"\" title=\"Example of finite MDP\">\n\n\n<h3 id=\"Using-Bellman-equation-and-computing-v\"><a href=\"#Using-Bellman-equation-and-computing-v\" class=\"headerlink\" title=\"Using Bellman equation and computing $v$\"></a>Using Bellman equation and computing $v$</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/computingV1.png\" class=\"\" title=\"Simple case: only one future state for each action\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/computingV2.png\" class=\"\" title=\"Now solve linear equations\">\n\n<p><a href=\"https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions\" target=\"_blank\" rel=\"noopener\">Excercises Answers</a></p>\n<h2 id=\"3-Optimality\"><a href=\"#3-Optimality\" class=\"headerlink\" title=\"3. Optimality\"></a>3. Optimality</h2><p>There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\\pi *$.</p>\n<h3 id=\"Optimal-state-value-function-v\"><a href=\"#Optimal-state-value-function-v\" class=\"headerlink\" title=\"Optimal state value function ($v_*$)\"></a>Optimal state value function ($v_*$)</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalv.png\" class=\"\">\n\n\n<h3 id=\"Optimal-action-value-function-q\"><a href=\"#Optimal-action-value-function-q\" class=\"headerlink\" title=\"Optimal action value function ($q_*$)\"></a>Optimal action value function ($q_*$)</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalq.png\" class=\"\">\n<img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimal_graphical.png\" class=\"\">\n\n<h3 id=\"Optimal-policy\"><a href=\"#Optimal-policy\" class=\"headerlink\" title=\"Optimal policy\"></a>Optimal policy</h3><img src=\"/blog/2020/06/22/rl-policy-valuefunct/optimalpolicy.png\" class=\"\">"},{"title":"Test","date":"2040-05-23T22:05:56.000Z","_content":"hi\n\n","source":"_posts/helloworld2.md","raw":"---\ntitle: test\ndate: 2040-05-23 18:05:56\ntags: \ncategories: 随笔\n\n---\nhi\n\n","slug":"helloworld2","published":1,"updated":"2020-08-12T21:17:32.943Z","_id":"ckdrvj1990002ki4bboba3tjl","comments":1,"layout":"post","photos":[],"link":"","content":"<p>hi</p>\n","site":{"data":{"styles":".archive .collection-title {\n  display: none !important;\n}\n","footer":"<br>\n<a class=\"author\" href=\"https://xiaoxiang-ma.github.io\">Return to main page</a>\n"}},"excerpt":"","more":"<p>hi</p>\n"}],"PostAsset":[{"_id":"source/_posts/agile/diagram.JPG","slug":"diagram.JPG","post":"ckdqd5kja0000d64bb5wj7usn","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/grid.png","slug":"grid.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/agile/Scrum-Diagram-JordanJob.me.png","slug":"Scrum-Diagram-JordanJob.me.png","post":"ckdqd5kja0000d64bb5wj7usn","modified":0,"renderable":0},{"_id":"source/_posts/agile/Scrum-v2.0-GIF-1000px.gif","slug":"Scrum-v2.0-GIF-1000px.gif","post":"ckdqd5kja0000d64bb5wj7usn","modified":0,"renderable":0},{"_id":"source/_posts/rl-mdp/G0.png","slug":"G0.png","post":"ckdqd5kk1000ed64b3bi46qy4","modified":0,"renderable":0},{"_id":"source/_posts/rl-mdp/G1.png","slug":"G1.png","post":"ckdqd5kk1000ed64b3bi46qy4","modified":0,"renderable":0},{"_id":"source/_posts/rl-mdp/mdp0.png","slug":"mdp0.png","post":"ckdqd5kk1000ed64b3bi46qy4","modified":0,"renderable":0},{"_id":"source/_posts/rl-mdp/mdp1.png","slug":"mdp1.png","post":"ckdqd5kk1000ed64b3bi46qy4","modified":0,"renderable":0},{"_id":"source/_posts/rl-mdp/mdp2.png","slug":"mdp2.png","post":"ckdqd5kk1000ed64b3bi46qy4","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/1equation.png","slug":"1equation.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/2actionvalue.png","slug":"2actionvalue.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/3.png","slug":"3.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/4.png","slug":"4.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/5.png","slug":"5.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/6.png","slug":"6.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/7.png","slug":"7.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/8.png","slug":"8.png","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/RL.pdf","slug":"RL.pdf","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-bandit/summary.mp4","slug":"summary.mp4","post":"ckdqd5kjz0009d64b0ll08ahn","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/actionvalue.png","slug":"actionvalue.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/bellman.png","slug":"bellman.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/computingV1.png","slug":"computingV1.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/computingV2.png","slug":"computingV2.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/optimal_graphical.png","slug":"optimal_graphical.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/optimalpolicy.png","slug":"optimalpolicy.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/optimalq.png","slug":"optimalq.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/optimalq_givenv.png","slug":"optimalq_givenv.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/optimalv.png","slug":"optimalv.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/q.png","slug":"q.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/statevalue.png","slug":"statevalue.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/tree.png","slug":"tree.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0},{"_id":"source/_posts/rl-policy-valuefunct/v.png","slug":"v.png","post":"ckdqd5kll001kd64b0lhhaovm","modified":0,"renderable":0}],"PostCategory":[{"post_id":"ckdqd5kja0000d64bb5wj7usn","category_id":"ckdqd5kjs0002d64bfjaa1nz3","_id":"ckdqd5kk1000bd64b4wul4khe"},{"post_id":"ckdqd5kjq0001d64b9xmvaonz","category_id":"ckdqd5kjy0007d64bf7t98607","_id":"ckdqd5kk3000hd64bf1tc2sdh"},{"post_id":"ckdqd5kk1000ed64b3bi46qy4","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kk5000kd64b7q8i3jms"},{"post_id":"ckdqd5kju0004d64bgze8919s","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kk6000nd64bb1a79jxr"},{"post_id":"ckdqd5kjw0005d64b68ra1qzj","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kk7000pd64b9yb64h27"},{"post_id":"ckdqd5kjx0006d64b14zn4cn7","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kk8000td64bh3wxc2kg"},{"post_id":"ckdqd5kjz0009d64b0ll08ahn","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kk9000xd64b3n4ba9w4"},{"post_id":"ckdqd5kk0000ad64b1te0af7s","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5kka0011d64b5skjcctp"},{"post_id":"ckdqd5klc001cd64b7torh44g","category_id":"ckdqd5kjy0007d64bf7t98607","_id":"ckdqd5kll001ld64b82t74koy"},{"post_id":"ckdqd5klj001hd64b7zy4bpvh","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5klm001od64bbu3r2utv"},{"post_id":"ckdqd5kll001kd64b0lhhaovm","category_id":"ckdqd5kk1000cd64bgqlleo4i","_id":"ckdqd5klm001qd64b2a5f0y1s"},{"post_id":"ckdqd5kle001ed64bg3yb2c7p","category_id":"ckdqd5klk001jd64b8wt281s6","_id":"ckdqd5kln001sd64bakpgd5su"},{"post_id":"ckdrvj1990002ki4bboba3tjl","category_id":"ckdqd5kjy0007d64bf7t98607","_id":"ckdrvj19a0003ki4b10kx7384"}],"PostTag":[{"post_id":"ckdqd5kja0000d64bb5wj7usn","tag_id":"ckdqd5kjt0003d64b5l9a3mlr","_id":"ckdqd5kk2000fd64b9qtphjl5"},{"post_id":"ckdqd5kja0000d64bb5wj7usn","tag_id":"ckdqd5kjy0008d64b0bhaglup","_id":"ckdqd5kk2000gd64bci2s6967"},{"post_id":"ckdqd5kju0004d64bgze8919s","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5kk8000rd64bchwh6f4v"},{"post_id":"ckdqd5kju0004d64bgze8919s","tag_id":"ckdqd5kk3000jd64bafif1gvg","_id":"ckdqd5kk8000ud64b814x3qkc"},{"post_id":"ckdqd5kju0004d64bgze8919s","tag_id":"ckdqd5kk6000md64b5r3ufxbr","_id":"ckdqd5kk9000wd64b5ynd2ba7"},{"post_id":"ckdqd5kjw0005d64b68ra1qzj","tag_id":"ckdqd5kk7000qd64b39yy010s","_id":"ckdqd5kk9000yd64b2qibd083"},{"post_id":"ckdqd5kjx0006d64b14zn4cn7","tag_id":"ckdqd5kk9000vd64bfjbx7xic","_id":"ckdqd5kka0010d64b4jbyhno9"},{"post_id":"ckdqd5kjz0009d64b0ll08ahn","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5kkb0014d64b5rlff59m"},{"post_id":"ckdqd5kjz0009d64b0ll08ahn","tag_id":"ckdqd5kka0012d64bfgg92ucy","_id":"ckdqd5kkb0015d64b2na45tns"},{"post_id":"ckdqd5kk0000ad64b1te0af7s","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5kkc0018d64b8a0me4fw"},{"post_id":"ckdqd5kk0000ad64b1te0af7s","tag_id":"ckdqd5kkb0016d64bd6jtg2v4","_id":"ckdqd5kkc0019d64b0g768dw4"},{"post_id":"ckdqd5kk1000ed64b3bi46qy4","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5kkd001ad64bera33urx"},{"post_id":"ckdqd5kk1000ed64b3bi46qy4","tag_id":"ckdqd5kka0012d64bfgg92ucy","_id":"ckdqd5kkd001bd64b0wqo22b8"},{"post_id":"ckdqd5kll001kd64b0lhhaovm","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5klm001nd64b5bnp0p1n"},{"post_id":"ckdqd5kll001kd64b0lhhaovm","tag_id":"ckdqd5kka0012d64bfgg92ucy","_id":"ckdqd5klm001pd64b2rzgewai"},{"post_id":"ckdqd5klc001cd64b7torh44g","tag_id":"ckdqd5klj001gd64bc05q5p44","_id":"ckdqd5kln001rd64b7npyg6nv"},{"post_id":"ckdqd5klj001hd64b7zy4bpvh","tag_id":"ckdqd5kk1000dd64bf41mbw2m","_id":"ckdqd5kln001td64ba8b8cwc4"},{"post_id":"ckdqd5klj001hd64b7zy4bpvh","tag_id":"ckdqd5kka0012d64bfgg92ucy","_id":"ckdqd5kln001ud64b7vaz8unb"},{"post_id":"ckdqd5klj001hd64b7zy4bpvh","tag_id":"ckdqd5klm001md64b2anh7yjp","_id":"ckdqd5kln001vd64b63v5772c"}],"Tag":[{"name":"Product management","_id":"ckdqd5kjt0003d64b5l9a3mlr"},{"name":"Agile","_id":"ckdqd5kjy0008d64b0bhaglup"},{"name":"笔记","_id":"ckdqd5kk1000dd64bf41mbw2m"},{"name":"ML","_id":"ckdqd5kk3000jd64bafif1gvg"},{"name":"Neural Net","_id":"ckdqd5kk6000md64b5r3ufxbr"},{"name":"Resources","_id":"ckdqd5kk7000qd64b39yy010s"},{"name":"Ideas","_id":"ckdqd5kk9000vd64bfjbx7xic"},{"name":"RL","_id":"ckdqd5kka0012d64bfgg92ucy"},{"name":"SQL","_id":"ckdqd5kkb0016d64bd6jtg2v4"},{"name":"诗","_id":"ckdqd5klj001gd64bc05q5p44"},{"name":"DQN","_id":"ckdqd5klm001md64b2anh7yjp"}]}}