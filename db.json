{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/maupassant/source/css/copyright.css","path":"css/copyright.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/copycode.css","path":"css/copycode.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/dark.css","path":"css/dark.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/donate.css","path":"css/donate.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/search.css","path":"css/search.css","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/alipay.svg","path":"img/alipay.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/bitcoin.svg","path":"img/bitcoin.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/like.svg","path":"img/like.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/css/style.scss","path":"css/style.scss","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/wechat.svg","path":"img/wechat.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/paypal.svg","path":"img/paypal.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","path":"js/codeblock-resizer.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/copycode.js","path":"js/copycode.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/img/github.svg","path":"img/github.svg","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/donate.js","path":"js/donate.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/copyright.js","path":"js/copyright.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/love.js","path":"js/love.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/share.js","path":"js/share.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/fancybox.js","path":"js/fancybox.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/smartresize.js","path":"js/smartresize.js","modified":1,"renderable":1},{"_id":"themes/maupassant/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/maupassant/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1590271761526},{"_id":"themes/maupassant/index.md","hash":"ae3583a95b0763c4a85ce0acfbdac4a5fa13c18e","modified":1590263545263},{"_id":"themes/maupassant/_config.yml","hash":"c5e87cf2066bb78728df6d869f58cdcf068c2733","modified":1590626198559},{"_id":"themes/maupassant/package.json","hash":"f092433469eb87362e831326425a6a5c3c9fea0d","modified":1590245224556},{"_id":"themes/maupassant/LICENSE","hash":"f0ac2f92770650c9835183f79010c0d307b34acd","modified":1590245224550},{"_id":"source/.DS_Store","hash":"d3966c8334cdef640ca71805dea602008cc345ad","modified":1592102398403},{"_id":"themes/maupassant/languages/de-DE.yml","hash":"5d3556a885e355a8c2da65ef3e7b3ee36a628bfa","modified":1590245224551},{"_id":"themes/maupassant/languages/en.yml","hash":"9c979a2f107536399bbe2be572c2d0bebcdd9d95","modified":1590245224551},{"_id":"themes/maupassant/languages/es-ES.yml","hash":"58e1d04bcd1834fa9d2960e18e027abbbccbedc9","modified":1590245224551},{"_id":"themes/maupassant/languages/fr-FR.yml","hash":"b47906ec0abf867fb3e3360bc046b7afb68aee25","modified":1590245224551},{"_id":"themes/maupassant/languages/zh-CN.yml","hash":"78cc1794a3ce3e186c462c1a70f097d0c05cd210","modified":1590245224551},{"_id":"themes/maupassant/layout/base-without-sidebar.pug","hash":"16c4d1079450f801b5ac079d3cc101856d8f387c","modified":1590245224555},{"_id":"themes/maupassant/layout/archive.pug","hash":"2fe2cff144c9b7b509b7b6cb1b3acba27aa1d132","modified":1590245224555},{"_id":"themes/maupassant/languages/ru.yml","hash":"2476a631f4d3c668de04af85a6c2c97ba2a57e96","modified":1590245224551},{"_id":"themes/maupassant/languages/ko.yml","hash":"909a33e0befa6978e8e72157c6b415b48551ee31","modified":1590245224551},{"_id":"themes/maupassant/languages/zh-TW.yml","hash":"e9747f9b3ec1314a3cae44a9a90e7649af739633","modified":1590245224552},{"_id":"themes/maupassant/layout/page.pug","hash":"a285a76950bb1b91f482ef9c473472bd0ede601d","modified":1590245224555},{"_id":"themes/maupassant/layout/index.pug","hash":"0435a4e5f5c6976e05b3079d335453c246f5ba6e","modified":1590245224555},{"_id":"themes/maupassant/layout/tagcloud.pug","hash":"4aa57b41e919a12b6d7691cd4a36d3a531b55fcb","modified":1590245224555},{"_id":"themes/maupassant/layout/timeline.pug","hash":"84fbfc92ccdf291b491140d89557553141a5d3f9","modified":1590245224556},{"_id":"themes/maupassant/layout/base.pug","hash":"a4e32bcb580b76af9ad0582d9d3f0107e34509ed","modified":1590245224555},{"_id":"themes/maupassant/layout/post.pug","hash":"7dcb8cffb29e519abe794ddd7e11ed7b3acb6287","modified":1590245224555},{"_id":"themes/maupassant/layout/single-column.pug","hash":"0593f261dc208bb0b5c4232eb41eff597a291bd9","modified":1590245224555},{"_id":"source/about/index.md","hash":"2f5fd56519c1b3a86e0fc4a8590a28833b035a2b","modified":1592421442943},{"_id":"source/_posts/reinforcement.md","hash":"48cdb1867093b4d8b990d5cf12143929edbee8e6","modified":1592108445497},{"_id":"source/_posts/workschedule.md","hash":"e7910d807f4fc6c872d3d89e3cb50666781d1a4c","modified":1592421698996},{"_id":"source/_posts/大学年三.md","hash":"1cacc4e2fdf2dffa2e1b3adc9fb4a9f33918b406","modified":1590628157875},{"_id":"source/_posts/杨绛语录.md","hash":"46618c54415a12a2443e66916c570bccb9845a14","modified":1592417032164},{"_id":"themes/maupassant/layout/_widget/category.pug","hash":"7c6aed762934ca51aa2669b886254da24b77bc14","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/copyright.pug","hash":"32701dcba9c52d4bc73badaf8afe91af3f87b6a0","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/donate.pug","hash":"50855ce17af4298ce0f15236e2308c4a1760e7e3","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/links.pug","hash":"c45aa7ec00158579e58f1f8dfd890447bb5e5e54","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/recent_comments.pug","hash":"4102d446f13b02ff617f055c2a8f726bca12744a","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/recent_posts.pug","hash":"19431336d724d2118e46da43683bce9063176541","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/search.pug","hash":"6e8e4123cca38840c4607c1a056205972b82bb7b","modified":1590245224554},{"_id":"themes/maupassant/layout/_widget/tag.pug","hash":"132f049ce677d0e38f50073174c4ee4b825d4a06","modified":1590245224554},{"_id":"themes/maupassant/layout/_partial/after_footer.pug","hash":"450f704907ed2ee5ff055ad60e85116ab98d4633","modified":1590245224552},{"_id":"themes/maupassant/layout/_partial/comments.pug","hash":"1254f863e076957ef282a561d4f2855e67a898e0","modified":1590245224552},{"_id":"themes/maupassant/layout/_partial/footer.pug","hash":"650781b5bc8c632658ad6880ba663b1e3bfb5798","modified":1590245224552},{"_id":"themes/maupassant/layout/_partial/head.pug","hash":"51881cde4653f00e9d0de68fb9c201b52b100b9e","modified":1590245224552},{"_id":"themes/maupassant/layout/_partial/helpers.pug","hash":"acdf9e2d52ee86c831fa15ce1570930c5779bc78","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/mathjax.pug","hash":"b54b56faff9e47ab3ca3cdd55056c73e60776f3c","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/mathjax2.pug","hash":"d6ac5dc4e9c7a1b866f1f92d88988cfb35aded4c","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/paginator.pug","hash":"53f9cb77448e84a98da5eb688e2e12b173c555bb","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/post_nav.pug","hash":"a2d698c84bb6da08195fe870dbd7215f65388d3f","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/tag.pug","hash":"53d721ea4c93564cb0bdde065572ff6128574d36","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/totop.pug","hash":"8225bbc3cdb9648bc2e6872e5c616a9a1e4def4f","modified":1590245224553},{"_id":"themes/maupassant/layout/_partial/wordcount.pug","hash":"b3c846a2e0ac79933e32c343029b769f2865a27f","modified":1590245224553},{"_id":"themes/maupassant/source/css/copyright.css","hash":"a418da11a88d1feb14500df42ed97a64da6a7611","modified":1590245224556},{"_id":"themes/maupassant/source/css/copycode.css","hash":"e2463b8dacf629e180a1b6cd80667ca8044292eb","modified":1590245224556},{"_id":"themes/maupassant/source/css/dark.css","hash":"0faf42a84e243032b736c5f06ddbb95ac69e779c","modified":1590245224556},{"_id":"themes/maupassant/source/css/donate.css","hash":"95b2fd65042afecc0b5530847c369bcc11d74bd0","modified":1590245224556},{"_id":"themes/maupassant/source/css/search.css","hash":"9406e138d7bb6a9ef4a067eba1dafa627519c8a7","modified":1590245224556},{"_id":"themes/maupassant/source/img/alipay.svg","hash":"3d94d2f9b09e352802628c9225578e1086f5fef3","modified":1590245224557},{"_id":"source/_drafts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1590269503016},{"_id":"source/tags/index.md","hash":"3bc021f7d89086d423eccf8c3b68ce08ab2b5a59","modified":1592421278487},{"_id":"source/_posts/.DS_Store","hash":"6f67e6eda911edccd80613dac023523d5d107a17","modified":1592103400183},{"_id":"themes/maupassant/source/img/bitcoin.svg","hash":"590b6b6462896168d08b30dfe2de5f08950d5553","modified":1590245224557},{"_id":"themes/maupassant/source/img/like.svg","hash":"e6e4bd1af076be9358316cac89efdc22ef4a5064","modified":1590245224558},{"_id":"themes/maupassant/source/css/style.scss","hash":"c43117f52095d68e776c62165becbbbad082d368","modified":1592421440939},{"_id":"themes/maupassant/source/img/wechat.svg","hash":"19c1f68ec8c0b8e9f7855e7a6e78077f70a1aedc","modified":1590245224558},{"_id":"themes/maupassant/source/img/paypal.svg","hash":"09786c983a10bc570dcd05b87cec601e9d01eb00","modified":1590245224558},{"_id":"themes/maupassant/source/js/codeblock-resizer.js","hash":"5d0b786d60bf225d9eabcc9cece2719ff4d9b6cd","modified":1590245224558},{"_id":"themes/maupassant/source/js/copycode.js","hash":"fde1f153bab1f00ae8930668094c00aa9ff3c7a3","modified":1590245224558},{"_id":"themes/maupassant/source/img/github.svg","hash":"277798d16cb6106e45ef74f6b9972011fa43401b","modified":1590245224557},{"_id":"themes/maupassant/source/js/donate.js","hash":"bdddd8d9847462d020f7a511e7e12c046223195d","modified":1590245224559},{"_id":"themes/maupassant/source/js/copyright.js","hash":"7b1bd775ea22abf33d57f78628f436bf656e439a","modified":1590245224558},{"_id":"themes/maupassant/source/js/search.js","hash":"0c0630e2ef213701d393b041f10572e951a27985","modified":1590245224559},{"_id":"themes/maupassant/source/js/love.js","hash":"5cf89f622bf891cf1986845eb92eadef6f358eb7","modified":1590245224559},{"_id":"themes/maupassant/source/js/share.js","hash":"a2f9de374523dc7f2ddb90ed5f24b668c20d9272","modified":1590245224559},{"_id":"themes/maupassant/source/js/fancybox.js","hash":"13c4781570339f4fba76a3d7f202e442817dd605","modified":1590245224559},{"_id":"source/_posts/reinforcement/1equation.png","hash":"6c5270d865056b0b1e9c2bd3ee5a7af2bf7ab194","modified":1592101026691},{"_id":"themes/maupassant/source/js/smartresize.js","hash":"3ef157fd877167e3290f42c67a624ea375a46c24","modified":1590245224559},{"_id":"themes/maupassant/source/js/totop.js","hash":"7dbf8fcf582a4fb6eb9b2c60d6de9f9c2091ec4c","modified":1590245224559},{"_id":"source/_posts/reinforcement/4.png","hash":"03fbd816e0da30558fe92403c1701441517c156f","modified":1592104937129},{"_id":"source/_posts/reinforcement/2actionvalue.png","hash":"91fb5b769468bfc5ed214dbf527b46122ee51766","modified":1592103383621},{"_id":"source/_posts/reinforcement/5.png","hash":"04b713163573340981da8cb2a57f942e8921687d","modified":1592105348248},{"_id":"source/_posts/reinforcement/8.png","hash":"836b4a882890e7716e3af2d9cd6ea2d648186db0","modified":1592107385058},{"_id":"source/_posts/reinforcement/3.png","hash":"4113998bb1bec693a53abf0283192ba4daabfb9a","modified":1592103464839},{"_id":"source/_posts/reinforcement/6.png","hash":"dfa6a7fa02f4a3c5537d9caaf038f6c6a47df574","modified":1592106252253},{"_id":"source/_posts/reinforcement/7.png","hash":"2ae92891f17acb989e55841c490d1d1fd4e8810c","modified":1592106331473},{"_id":"public/about/index.html","hash":"82e1a85a7b91eea4d117553a1bad9457fa9d11ab","modified":1592421783442},{"_id":"public/tags/index.html","hash":"4274c32f2272a8e2570e4307c3d5dc608d95d68c","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/index.html","hash":"9198e83b16a8ab75496f7df12871448090e2ce84","modified":1592421783442},{"_id":"public/2020/06/17/workschedule/index.html","hash":"45759df0b442ddd9e75a846a4c56939d0988cb9b","modified":1592421783442},{"_id":"public/2020/05/27/杨绛语录/index.html","hash":"52c199d739aec52b01c576217087a02d39e24016","modified":1592421783442},{"_id":"public/2020/05/23/大学年三/index.html","hash":"3f8285b91638c2688b839149008bca66412b6b80","modified":1592421783442},{"_id":"public/tags/心情/index.html","hash":"a631033da5ef211419a2fabbd1bed80592c9176b","modified":1592421783442},{"_id":"public/tags/笔记/index.html","hash":"4e66bf0d1d4971ba1cec3aa3326daee779f94fd5","modified":1592421783442},{"_id":"public/index.html","hash":"07a2997de75cc9a8f63dd399ab25964ba26dc913","modified":1592421783442},{"_id":"public/archives/index.html","hash":"29b807aa7df07866818b7c5393705a89dac7a044","modified":1592421783442},{"_id":"public/archives/2020/index.html","hash":"29b807aa7df07866818b7c5393705a89dac7a044","modified":1592421783442},{"_id":"public/archives/2020/05/index.html","hash":"f88e336e694411f5fcc5952c753b9986bd6a30fa","modified":1592421783442},{"_id":"public/archives/2020/06/index.html","hash":"a9e09ea5a04a2b5485ca0a8ec6ac66874f282bae","modified":1592421783442},{"_id":"public/img/alipay.svg","hash":"3d94d2f9b09e352802628c9225578e1086f5fef3","modified":1592421783442},{"_id":"public/img/bitcoin.svg","hash":"590b6b6462896168d08b30dfe2de5f08950d5553","modified":1592421783442},{"_id":"public/img/wechat.svg","hash":"19c1f68ec8c0b8e9f7855e7a6e78077f70a1aedc","modified":1592421783442},{"_id":"public/img/paypal.svg","hash":"09786c983a10bc570dcd05b87cec601e9d01eb00","modified":1592421783442},{"_id":"public/img/github.svg","hash":"277798d16cb6106e45ef74f6b9972011fa43401b","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/1equation.png","hash":"6c5270d865056b0b1e9c2bd3ee5a7af2bf7ab194","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/2actionvalue.png","hash":"91fb5b769468bfc5ed214dbf527b46122ee51766","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/4.png","hash":"03fbd816e0da30558fe92403c1701441517c156f","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/5.png","hash":"04b713163573340981da8cb2a57f942e8921687d","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/6.png","hash":"dfa6a7fa02f4a3c5537d9caaf038f6c6a47df574","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/7.png","hash":"2ae92891f17acb989e55841c490d1d1fd4e8810c","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/8.png","hash":"836b4a882890e7716e3af2d9cd6ea2d648186db0","modified":1592421783442},{"_id":"public/img/like.svg","hash":"e6e4bd1af076be9358316cac89efdc22ef4a5064","modified":1592421783442},{"_id":"public/2020/06/13/reinforcement/3.png","hash":"4113998bb1bec693a53abf0283192ba4daabfb9a","modified":1592421783442},{"_id":"public/css/dark.css","hash":"0faf42a84e243032b736c5f06ddbb95ac69e779c","modified":1592421783442},{"_id":"public/css/donate.css","hash":"95b2fd65042afecc0b5530847c369bcc11d74bd0","modified":1592421783442},{"_id":"public/css/search.css","hash":"9406e138d7bb6a9ef4a067eba1dafa627519c8a7","modified":1592421783442},{"_id":"public/css/copycode.css","hash":"e2463b8dacf629e180a1b6cd80667ca8044292eb","modified":1592421783442},{"_id":"public/css/copyright.css","hash":"a418da11a88d1feb14500df42ed97a64da6a7611","modified":1592421783442},{"_id":"public/css/style.css","hash":"18e013592e9f9f185a96e0dddf3aa94035b1bc9f","modified":1592421783442},{"_id":"public/js/copycode.js","hash":"fde1f153bab1f00ae8930668094c00aa9ff3c7a3","modified":1592421783442},{"_id":"public/js/codeblock-resizer.js","hash":"5d0b786d60bf225d9eabcc9cece2719ff4d9b6cd","modified":1592421783442},{"_id":"public/js/love.js","hash":"5cf89f622bf891cf1986845eb92eadef6f358eb7","modified":1592421783442},{"_id":"public/js/donate.js","hash":"bdddd8d9847462d020f7a511e7e12c046223195d","modified":1592421783442},{"_id":"public/js/copyright.js","hash":"7b1bd775ea22abf33d57f78628f436bf656e439a","modified":1592421783442},{"_id":"public/js/share.js","hash":"a2f9de374523dc7f2ddb90ed5f24b668c20d9272","modified":1592421783442},{"_id":"public/js/smartresize.js","hash":"3ef157fd877167e3290f42c67a624ea375a46c24","modified":1592421783442},{"_id":"public/js/fancybox.js","hash":"13c4781570339f4fba76a3d7f202e442817dd605","modified":1592421783442},{"_id":"public/js/totop.js","hash":"7dbf8fcf582a4fb6eb9b2c60d6de9f9c2091ec4c","modified":1592421783442},{"_id":"public/js/search.js","hash":"0c0630e2ef213701d393b041f10572e951a27985","modified":1592421783442}],"Category":[],"Data":[],"Page":[{"layout":"page","title":"About","_content":"\ncomming soon...","source":"about/index.md","raw":"---\nlayout: page\ntitle: About\n---\n\ncomming soon...","date":"2020-06-17T19:17:22.943Z","updated":"2020-06-17T19:17:22.943Z","path":"about/index.html","comments":1,"_id":"ckbjqtd300000m94bg3b7b4v6","content":"<p>comming soon…</p>\n","site":{"data":{}},"excerpt":"","more":"<p>comming soon…</p>\n"},{"layout":"tagcloud","title":"Tags","_content":"","source":"tags/index.md","raw":"---\nlayout: tagcloud\ntitle: Tags\n---","date":"2020-06-17T19:14:38.487Z","updated":"2020-06-17T19:14:38.487Z","path":"tags/index.html","comments":1,"_id":"ckbjqtd3n000bm94b8as5esmz","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Reinforcement Learning Notes","date":"2020-06-14T02:06:45.000Z","_content":"\n<!-- ## Reinforcement Learning Notes -->\n\nDesigns an agent that tries to maximize \"reward\" by trial and error.\nNot supervised, but not unsupervised either.\n\n[Textbook](https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1592265600&Signature=Oi0sGTGZL~k36Xj6gpkVVf0kn3imrSJQ1JUi6POtlUgX8DQkX2-598rCevrFd4aMG3hiRqvucoRH6evQiB9DdReQl3sREKsxK9VauTXD-18ticSatVReQRvEcLc9d-aBjyYreIBTJ4M2~iDrOIkbrY2ir9PaOYiW~xN68wgrsFM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)\n\n## The k-armed Bandit Problem\n\nFaced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n\n\n\n![True value of action](1equation.png)\n\nValue of an action is the expected value of reward given that action is selected.\n\n- q*(a): TRUE Value of an action (Note known)\n- Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).\n- At: Action selected on time step t\n- Rt: Corresponding Reward on time step t\n\nGoal is to maximize argmax of q*(a)\n\n> Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected\n\n## Action-Value & increment estimation\n\nSample average method: Take the average of action-values so far.\n\n![Sample avg (predicted) action value](2actionvalue.png)\n<!-- ![Action value](3.png) -->\n\nAction Selection:\n- Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)\n- Explore: choose other than the largest estimate randomly.\n\n\n\n<mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark>\n\n![Increment (R = reward)](4.png)\n\nNewEstimate = OldEstimate + StepSize (Target - OldEstimate)\n\n**Nonstationary Problem**\nPerformance changes with time. (e.g. a medicine more effective in winter)\n![Constant step size](5.png)\n\n- alpha is constant, more recent rewards affect more than older rewards.\n- weight decays exponentially according to the exponent on 1- alpha.\n\n## Epsilon Greedy & optimistic initial value\n\nEpsilon = prob of choosing to explore.\nThe smaller, the longer it takes for curve to plateu.\n![Epsilon](7.png)\n\n![Optimistic int val](6.png)\n\n- **Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).**\n- The system does a fair amount of exploration even if greedy actions are selected all the time.\n\n## Upper-Conﬁdence-Bound Action Selection\n\nAction selection method that chooses the most \"unexplored\" / \"uncertain\" actions when not choosing to exploit.\n\n![UCB](8.png)\n\n> N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.\n\n> The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. ***Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.*** On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n\n[Week 1 Summary](https://d18ky98rnyall9.cloudfront.net/k5ZG8p3IEemEawpeY3OQmg.processed/full/360p/index.webm?Expires=1592265600&Signature=IxfR~bsRqAL3g-K2cRDS48LcMiaK~Q8HYfeZGaOQwNKQ80qJ-zuTQUhca75nHvfR58VNOGjqStnexBRUTejaN1FGIMPgRsx2N7WI8E3PIc65k8d-PbY0PThBCHGgv8uIlzP8dQAVcSRbFwicygHYHTRYFsVVEbgijR5NvgPNF0Q_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)\n\n\n","source":"_posts/reinforcement.md","raw":"---\ntitle: Reinforcement Learning Notes\ndate: 2020-06-13 22:06:45\ntags: 笔记\n---\n\n<!-- ## Reinforcement Learning Notes -->\n\nDesigns an agent that tries to maximize \"reward\" by trial and error.\nNot supervised, but not unsupervised either.\n\n[Textbook](https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1592265600&Signature=Oi0sGTGZL~k36Xj6gpkVVf0kn3imrSJQ1JUi6POtlUgX8DQkX2-598rCevrFd4aMG3hiRqvucoRH6evQiB9DdReQl3sREKsxK9VauTXD-18ticSatVReQRvEcLc9d-aBjyYreIBTJ4M2~iDrOIkbrY2ir9PaOYiW~xN68wgrsFM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)\n\n## The k-armed Bandit Problem\n\nFaced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n\n\n\n![True value of action](1equation.png)\n\nValue of an action is the expected value of reward given that action is selected.\n\n- q*(a): TRUE Value of an action (Note known)\n- Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).\n- At: Action selected on time step t\n- Rt: Corresponding Reward on time step t\n\nGoal is to maximize argmax of q*(a)\n\n> Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected\n\n## Action-Value & increment estimation\n\nSample average method: Take the average of action-values so far.\n\n![Sample avg (predicted) action value](2actionvalue.png)\n<!-- ![Action value](3.png) -->\n\nAction Selection:\n- Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)\n- Explore: choose other than the largest estimate randomly.\n\n\n\n<mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark>\n\n![Increment (R = reward)](4.png)\n\nNewEstimate = OldEstimate + StepSize (Target - OldEstimate)\n\n**Nonstationary Problem**\nPerformance changes with time. (e.g. a medicine more effective in winter)\n![Constant step size](5.png)\n\n- alpha is constant, more recent rewards affect more than older rewards.\n- weight decays exponentially according to the exponent on 1- alpha.\n\n## Epsilon Greedy & optimistic initial value\n\nEpsilon = prob of choosing to explore.\nThe smaller, the longer it takes for curve to plateu.\n![Epsilon](7.png)\n\n![Optimistic int val](6.png)\n\n- **Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).**\n- The system does a fair amount of exploration even if greedy actions are selected all the time.\n\n## Upper-Conﬁdence-Bound Action Selection\n\nAction selection method that chooses the most \"unexplored\" / \"uncertain\" actions when not choosing to exploit.\n\n![UCB](8.png)\n\n> N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.\n\n> The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. ***Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.*** On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n\n[Week 1 Summary](https://d18ky98rnyall9.cloudfront.net/k5ZG8p3IEemEawpeY3OQmg.processed/full/360p/index.webm?Expires=1592265600&Signature=IxfR~bsRqAL3g-K2cRDS48LcMiaK~Q8HYfeZGaOQwNKQ80qJ-zuTQUhca75nHvfR58VNOGjqStnexBRUTejaN1FGIMPgRsx2N7WI8E3PIc65k8d-PbY0PThBCHGgv8uIlzP8dQAVcSRbFwicygHYHTRYFsVVEbgijR5NvgPNF0Q_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)\n\n\n","slug":"reinforcement","published":1,"updated":"2020-06-14T04:20:45.497Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckbjqtd3b0001m94baq2uevrb","content":"<!-- ## Reinforcement Learning Notes -->\n\n<p>Designs an agent that tries to maximize “reward” by trial and error.<br>Not supervised, but not unsupervised either.</p>\n<p><a href=\"https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1592265600&Signature=Oi0sGTGZL~k36Xj6gpkVVf0kn3imrSJQ1JUi6POtlUgX8DQkX2-598rCevrFd4aMG3hiRqvucoRH6evQiB9DdReQl3sREKsxK9VauTXD-18ticSatVReQRvEcLc9d-aBjyYreIBTJ4M2~iDrOIkbrY2ir9PaOYiW~xN68wgrsFM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A\" target=\"_blank\" rel=\"noopener\">Textbook</a></p>\n<h2 id=\"The-k-armed-Bandit-Problem\"><a href=\"#The-k-armed-Bandit-Problem\" class=\"headerlink\" title=\"The k-armed Bandit Problem\"></a>The k-armed Bandit Problem</h2><p>Faced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>\n<p><img src=\"1equation.png\" alt=\"True value of action\"></p>\n<p>Value of an action is the expected value of reward given that action is selected.</p>\n<ul>\n<li>q*(a): TRUE Value of an action (Note known)</li>\n<li>Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).</li>\n<li>At: Action selected on time step t</li>\n<li>Rt: Corresponding Reward on time step t</li>\n</ul>\n<p>Goal is to maximize argmax of q*(a)</p>\n<blockquote>\n<p>Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected</p>\n</blockquote>\n<h2 id=\"Action-Value-amp-increment-estimation\"><a href=\"#Action-Value-amp-increment-estimation\" class=\"headerlink\" title=\"Action-Value &amp; increment estimation\"></a>Action-Value &amp; increment estimation</h2><p>Sample average method: Take the average of action-values so far.</p>\n<p><img src=\"2actionvalue.png\" alt=\"Sample avg (predicted) action value\"></p>\n<!-- ![Action value](3.png) -->\n\n<p>Action Selection:</p>\n<ul>\n<li>Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)</li>\n<li>Explore: choose other than the largest estimate randomly.</li>\n</ul>\n<p><mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark></p>\n<p><img src=\"4.png\" alt=\"Increment (R = reward)\"></p>\n<p>NewEstimate = OldEstimate + StepSize (Target - OldEstimate)</p>\n<p><strong>Nonstationary Problem</strong><br>Performance changes with time. (e.g. a medicine more effective in winter)<br><img src=\"5.png\" alt=\"Constant step size\"></p>\n<ul>\n<li>alpha is constant, more recent rewards affect more than older rewards.</li>\n<li>weight decays exponentially according to the exponent on 1- alpha.</li>\n</ul>\n<h2 id=\"Epsilon-Greedy-amp-optimistic-initial-value\"><a href=\"#Epsilon-Greedy-amp-optimistic-initial-value\" class=\"headerlink\" title=\"Epsilon Greedy &amp; optimistic initial value\"></a>Epsilon Greedy &amp; optimistic initial value</h2><p>Epsilon = prob of choosing to explore.<br>The smaller, the longer it takes for curve to plateu.<br><img src=\"7.png\" alt=\"Epsilon\"></p>\n<p><img src=\"6.png\" alt=\"Optimistic int val\"></p>\n<ul>\n<li><strong>Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).</strong></li>\n<li>The system does a fair amount of exploration even if greedy actions are selected all the time.</li>\n</ul>\n<h2 id=\"Upper-Conﬁdence-Bound-Action-Selection\"><a href=\"#Upper-Conﬁdence-Bound-Action-Selection\" class=\"headerlink\" title=\"Upper-Conﬁdence-Bound Action Selection\"></a>Upper-Conﬁdence-Bound Action Selection</h2><p>Action selection method that chooses the most “unexplored” / “uncertain” actions when not choosing to exploit.</p>\n<p><img src=\"8.png\" alt=\"UCB\"></p>\n<blockquote>\n<p>N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c &gt; 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.</p>\n</blockquote>\n<blockquote>\n<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. <strong><em>Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.</em></strong> On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>\n</blockquote>\n<p><a href=\"https://d18ky98rnyall9.cloudfront.net/k5ZG8p3IEemEawpeY3OQmg.processed/full/360p/index.webm?Expires=1592265600&Signature=IxfR~bsRqAL3g-K2cRDS48LcMiaK~Q8HYfeZGaOQwNKQ80qJ-zuTQUhca75nHvfR58VNOGjqStnexBRUTejaN1FGIMPgRsx2N7WI8E3PIc65k8d-PbY0PThBCHGgv8uIlzP8dQAVcSRbFwicygHYHTRYFsVVEbgijR5NvgPNF0Q_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A\" target=\"_blank\" rel=\"noopener\">Week 1 Summary</a></p>\n","site":{"data":{}},"excerpt":"","more":"<!-- ## Reinforcement Learning Notes -->\n\n<p>Designs an agent that tries to maximize “reward” by trial and error.<br>Not supervised, but not unsupervised either.</p>\n<p><a href=\"https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1592265600&Signature=Oi0sGTGZL~k36Xj6gpkVVf0kn3imrSJQ1JUi6POtlUgX8DQkX2-598rCevrFd4aMG3hiRqvucoRH6evQiB9DdReQl3sREKsxK9VauTXD-18ticSatVReQRvEcLc9d-aBjyYreIBTJ4M2~iDrOIkbrY2ir9PaOYiW~xN68wgrsFM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A\" target=\"_blank\" rel=\"noopener\">Textbook</a></p>\n<h2 id=\"The-k-armed-Bandit-Problem\"><a href=\"#The-k-armed-Bandit-Problem\" class=\"headerlink\" title=\"The k-armed Bandit Problem\"></a>The k-armed Bandit Problem</h2><p>Faced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>\n<p><img src=\"1equation.png\" alt=\"True value of action\"></p>\n<p>Value of an action is the expected value of reward given that action is selected.</p>\n<ul>\n<li>q*(a): TRUE Value of an action (Note known)</li>\n<li>Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).</li>\n<li>At: Action selected on time step t</li>\n<li>Rt: Corresponding Reward on time step t</li>\n</ul>\n<p>Goal is to maximize argmax of q*(a)</p>\n<blockquote>\n<p>Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected</p>\n</blockquote>\n<h2 id=\"Action-Value-amp-increment-estimation\"><a href=\"#Action-Value-amp-increment-estimation\" class=\"headerlink\" title=\"Action-Value &amp; increment estimation\"></a>Action-Value &amp; increment estimation</h2><p>Sample average method: Take the average of action-values so far.</p>\n<p><img src=\"2actionvalue.png\" alt=\"Sample avg (predicted) action value\"></p>\n<!-- ![Action value](3.png) -->\n\n<p>Action Selection:</p>\n<ul>\n<li>Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)</li>\n<li>Explore: choose other than the largest estimate randomly.</li>\n</ul>\n<p><mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark></p>\n<p><img src=\"4.png\" alt=\"Increment (R = reward)\"></p>\n<p>NewEstimate = OldEstimate + StepSize (Target - OldEstimate)</p>\n<p><strong>Nonstationary Problem</strong><br>Performance changes with time. (e.g. a medicine more effective in winter)<br><img src=\"5.png\" alt=\"Constant step size\"></p>\n<ul>\n<li>alpha is constant, more recent rewards affect more than older rewards.</li>\n<li>weight decays exponentially according to the exponent on 1- alpha.</li>\n</ul>\n<h2 id=\"Epsilon-Greedy-amp-optimistic-initial-value\"><a href=\"#Epsilon-Greedy-amp-optimistic-initial-value\" class=\"headerlink\" title=\"Epsilon Greedy &amp; optimistic initial value\"></a>Epsilon Greedy &amp; optimistic initial value</h2><p>Epsilon = prob of choosing to explore.<br>The smaller, the longer it takes for curve to plateu.<br><img src=\"7.png\" alt=\"Epsilon\"></p>\n<p><img src=\"6.png\" alt=\"Optimistic int val\"></p>\n<ul>\n<li><strong>Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).</strong></li>\n<li>The system does a fair amount of exploration even if greedy actions are selected all the time.</li>\n</ul>\n<h2 id=\"Upper-Conﬁdence-Bound-Action-Selection\"><a href=\"#Upper-Conﬁdence-Bound-Action-Selection\" class=\"headerlink\" title=\"Upper-Conﬁdence-Bound Action Selection\"></a>Upper-Conﬁdence-Bound Action Selection</h2><p>Action selection method that chooses the most “unexplored” / “uncertain” actions when not choosing to exploit.</p>\n<p><img src=\"8.png\" alt=\"UCB\"></p>\n<blockquote>\n<p>N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c &gt; 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.</p>\n</blockquote>\n<blockquote>\n<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. <strong><em>Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.</em></strong> On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>\n</blockquote>\n<p><a href=\"https://d18ky98rnyall9.cloudfront.net/k5ZG8p3IEemEawpeY3OQmg.processed/full/360p/index.webm?Expires=1592265600&Signature=IxfR~bsRqAL3g-K2cRDS48LcMiaK~Q8HYfeZGaOQwNKQ80qJ-zuTQUhca75nHvfR58VNOGjqStnexBRUTejaN1FGIMPgRsx2N7WI8E3PIc65k8d-PbY0PThBCHGgv8uIlzP8dQAVcSRbFwicygHYHTRYFsVVEbgijR5NvgPNF0Q_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A\" target=\"_blank\" rel=\"noopener\">Week 1 Summary</a></p>\n"},{"title":"Work & Life & Summer","date":"2020-06-17T18:00:42.000Z","_content":"\n\nWork:\n- 9am - 5pm: Work time. No social media, gaming, or videos\n- YS main, Marhub alt, free time on loan prediction project/personal projects.\n\nSummer:\n- Meng application\n- Update resume & apply for jobs\n- 2+ Projects\n","source":"_posts/workschedule.md","raw":"---\ntitle: Work & Life & Summer\ndate: 2020-06-17 14:00:42\ntags:\n---\n\n\nWork:\n- 9am - 5pm: Work time. No social media, gaming, or videos\n- YS main, Marhub alt, free time on loan prediction project/personal projects.\n\nSummer:\n- Meng application\n- Update resume & apply for jobs\n- 2+ Projects\n","slug":"workschedule","published":1,"updated":"2020-06-17T19:21:38.996Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckbjqtd3e0002m94b3ah75az9","content":"<p>Work:</p>\n<ul>\n<li>9am - 5pm: Work time. No social media, gaming, or videos</li>\n<li>YS main, Marhub alt, free time on loan prediction project/personal projects.</li>\n</ul>\n<p>Summer:</p>\n<ul>\n<li>Meng application</li>\n<li>Update resume &amp; apply for jobs</li>\n<li>2+ Projects</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Work:</p>\n<ul>\n<li>9am - 5pm: Work time. No social media, gaming, or videos</li>\n<li>YS main, Marhub alt, free time on loan prediction project/personal projects.</li>\n</ul>\n<p>Summer:</p>\n<ul>\n<li>Meng application</li>\n<li>Update resume &amp; apply for jobs</li>\n<li>2+ Projects</li>\n</ul>\n"},{"title":"杨绛语录","date":"2020-05-28T00:26:07.000Z","_content":"\n> 1. 你的问题主要在于读书不多而想得太多。\n2. 如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。\n3. 我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！\n4. 有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。\n5. 少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。\n6. 在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。\n7. 惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。\n8. 上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。\n9. 世间好物不坚牢，彩云易散琉璃脆。\n10. 我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。\n11. 月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。\n12. 我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。\n","source":"_posts/杨绛语录.md","raw":"---\ntitle: 杨绛语录\ndate: 2020-05-27 20:26:07\ntags: 心情\n---\n\n> 1. 你的问题主要在于读书不多而想得太多。\n2. 如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。\n3. 我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！\n4. 有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。\n5. 少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。\n6. 在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。\n7. 惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。\n8. 上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。\n9. 世间好物不坚牢，彩云易散琉璃脆。\n10. 我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。\n11. 月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。\n12. 我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。\n","slug":"杨绛语录","published":1,"updated":"2020-06-17T18:03:52.164Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckbjqtd3h0004m94b0hii4qet","content":"<blockquote>\n<ol>\n<li>你的问题主要在于读书不多而想得太多。</li>\n<li>如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。</li>\n<li>我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！</li>\n<li>有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。</li>\n<li>少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。</li>\n<li>在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。</li>\n<li>惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。</li>\n<li>上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。</li>\n<li>世间好物不坚牢，彩云易散琉璃脆。</li>\n<li>我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。</li>\n<li>月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。</li>\n<li>我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。</li>\n</ol>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<ol>\n<li>你的问题主要在于读书不多而想得太多。</li>\n<li>如要锻炼一个能做大事的人，必定要叫他吃苦受累，百不称心，才能养成坚忍的性格。一个人经过不同程度的锻炼，就获得不同程度的修养，不同程度的效益。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。</li>\n<li>我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容……我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系！</li>\n<li>有些人之所以不断成长，就绝对是有一种坚持下去的力量。好读书，肯下功夫，不仅读，还做笔记。人要成长，必有原因，背后的努力与积累一定数倍于普通人。所以，关键还在于自己。</li>\n<li>少年贪玩，青年迷恋爱情，壮年汲汲于成名成家，暮年自安于自欺欺人。人寿几何，顽铁能炼成的精金，能有多少？但不同程度的锻炼，必有不同程度的成绩；不同程度的纵欲放肆，必积下不同程度的顽劣。</li>\n<li>在这物欲横流的人世间，人生一世实在是够苦。你存心做一个与世无争的老实人吧，人家就利用你欺侮你。你稍有才德品貌，人家就嫉妒你排挤你。你大度退让，人家就侵犯你损害你。你要不与人争，就得与世无求，同时还要维持实力准备斗争。你要和别人和平共处，就先得和他们周旋，还得准备随时吃亏。</li>\n<li>惟有身处卑微的人，最有机缘看到世态人情的真相。一个人不想攀高就不怕下跌，也不用倾轧排挤，可以保其天真，成其自然，潜心一志完成自己能做的事。</li>\n<li>上苍不会让所有幸福集中到某个人身上，得到爱情未必拥有金钱；拥有金钱未必得到快乐；得到快乐未必拥有健康；拥有健康未必一切都会如愿以偿。保持知足常乐的心态才是淬炼心智、净化心灵的最佳途径。一切快乐的享受都属于精神，这种快乐把忍受变为享受，是精神对于物质的胜利，这便是人生哲学。</li>\n<li>世间好物不坚牢，彩云易散琉璃脆。</li>\n<li>我是一位老人，净说些老话。对于时代，我是落伍者，没有什么良言贡献给现代婚姻。只是在物质至上的时代潮流下，想提醒年轻的朋友，男女结合最最重要的是感情，双方互相理解的程度。理解深才能互相欣赏、吸引、支持和鼓励，两情相悦。门当户对及其他，并不重要。</li>\n<li>月盈则亏，水满则溢，爱情到这里就可以了，我不要它溢出来。</li>\n<li>我和谁都不争，和谁争我都不屑。简朴的生活、高贵的灵魂是人生的至高境界。</li>\n</ol>\n</blockquote>\n"},{"title":"大学年三","date":"2020-05-23T22:05:56.000Z","_content":"五月给伊萨卡带来了阳光与微风，让夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和微弱的水声，恬静得似乎闭目养神就能和自然融为一体。\n\n大学的第三年也随着五月的到来缓缓结束了。这一年里，我感到可以更有效的去和自己沟通，并更精准的的理解自己行为背后的思想和心情。这让我能更好的管理时间，计划未来，并预判自己的行为。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全跟不上他想去征服这块大陆的决心。\n\n单说我的专业，数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。这是一件值得庆幸的好事。\n\n总的来说，这一年成长了不少，也为未来几年勾勒出了大致的轮廓。对未来充满期待和向往，愿自己能在步入社会的同时不忘初心，牢记我的原则和信仰。\n\n\n","source":"_posts/大学年三.md","raw":"---\ntitle: 大学年三\ndate: 2020-05-23 18:05:56\ntags: \n- 心情\n---\n五月给伊萨卡带来了阳光与微风，让夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和微弱的水声，恬静得似乎闭目养神就能和自然融为一体。\n\n大学的第三年也随着五月的到来缓缓结束了。这一年里，我感到可以更有效的去和自己沟通，并更精准的的理解自己行为背后的思想和心情。这让我能更好的管理时间，计划未来，并预判自己的行为。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全跟不上他想去征服这块大陆的决心。\n\n单说我的专业，数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。这是一件值得庆幸的好事。\n\n总的来说，这一年成长了不少，也为未来几年勾勒出了大致的轮廓。对未来充满期待和向往，愿自己能在步入社会的同时不忘初心，牢记我的原则和信仰。\n\n\n","slug":"大学年三","published":1,"updated":"2020-05-28T01:09:17.875Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckbjqtd3i0005m94b72fg2cnz","content":"<p>五月给伊萨卡带来了阳光与微风，让夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和微弱的水声，恬静得似乎闭目养神就能和自然融为一体。</p>\n<p>大学的第三年也随着五月的到来缓缓结束了。这一年里，我感到可以更有效的去和自己沟通，并更精准的的理解自己行为背后的思想和心情。这让我能更好的管理时间，计划未来，并预判自己的行为。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全跟不上他想去征服这块大陆的决心。</p>\n<p>单说我的专业，数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。这是一件值得庆幸的好事。</p>\n<p>总的来说，这一年成长了不少，也为未来几年勾勒出了大致的轮廓。对未来充满期待和向往，愿自己能在步入社会的同时不忘初心，牢记我的原则和信仰。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>五月给伊萨卡带来了阳光与微风，让夏日的美好充斥鼻尖。buttermilk公园的原始森林穿插着鸟叫和微弱的水声，恬静得似乎闭目养神就能和自然融为一体。</p>\n<p>大学的第三年也随着五月的到来缓缓结束了。这一年里，我感到可以更有效的去和自己沟通，并更精准的的理解自己行为背后的思想和心情。这让我能更好的管理时间，计划未来，并预判自己的行为。同时，我也了解到了太多未知的领域，学习和消化新知识的速度远远跟不上发现未知的速度。就好比一只蜗牛发现了新大陆，随他激动也好，信心满满也好，他的移动速度完全跟不上他想去征服这块大陆的决心。</p>\n<p>单说我的专业，数据科学的求知路尽管坎坷，但非常充实。虽然我认为自己对知识的消化的算是较慢的，但这一年的积累也让我的思维方式完全蜕变。这是一件值得庆幸的好事。</p>\n<p>总的来说，这一年成长了不少，也为未来几年勾勒出了大致的轮廓。对未来充满期待和向往，愿自己能在步入社会的同时不忘初心，牢记我的原则和信仰。</p>\n"}],"PostAsset":[{"_id":"source/_posts/reinforcement/1equation.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"1equation.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/2actionvalue.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"2actionvalue.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/3.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"3.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/4.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"4.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/5.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"5.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/6.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"6.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/7.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/reinforcement/8.png","post":"ckbjqtd3b0001m94baq2uevrb","slug":"8.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[{"post_id":"ckbjqtd3b0001m94baq2uevrb","tag_id":"ckbjqtd3g0003m94bem8k2kgu","_id":"ckbjqtd3k0007m94bh9p31eor"},{"post_id":"ckbjqtd3h0004m94b0hii4qet","tag_id":"ckbjqtd3j0006m94b9n8thoqn","_id":"ckbjqtd3m0009m94bbihm4s26"},{"post_id":"ckbjqtd3i0005m94b72fg2cnz","tag_id":"ckbjqtd3j0006m94b9n8thoqn","_id":"ckbjqtd3m000am94bdvcs9idx"}],"Tag":[{"name":"笔记","_id":"ckbjqtd3g0003m94bem8k2kgu"},{"name":"心情","_id":"ckbjqtd3j0006m94b9n8thoqn"}]}}