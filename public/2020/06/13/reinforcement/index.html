<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>Reinforcement Learning Notes | (▀̿Ĺ̯▀̿ ̿)</title><link rel="stylesheet" type="text/css" href="/blog/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/blog/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/blog/favicon.ico"><link rel="apple-touch-icon" href="/blog/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/blog/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Reinforcement Learning Notes</h1><a id="logo" href="/blog/.">(▀̿Ĺ̯▀̿ ̿)</a><p class="description">Ma blog</p></div><div id="nav-menu"><a class="current" href="/blog/."><i class="fa fa-home"> 首页</i></a><a href="/blog/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/blog/tags/"><i class="fa fa-tag"> 标签</i></a><a href="/blog/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Reinforcement Learning Notes</h1><div class="post-meta">2020-06-13</div><div class="post-content"><!-- ## Reinforcement Learning Notes -->

<p>Designs an agent that tries to maximize “reward” by trial and error.<br>Not supervised, but not unsupervised either.</p>
<p><a href="https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1592265600&Signature=Oi0sGTGZL~k36Xj6gpkVVf0kn3imrSJQ1JUi6POtlUgX8DQkX2-598rCevrFd4aMG3hiRqvucoRH6evQiB9DdReQl3sREKsxK9VauTXD-18ticSatVReQRvEcLc9d-aBjyYreIBTJ4M2~iDrOIkbrY2ir9PaOYiW~xN68wgrsFM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" target="_blank" rel="noopener">Textbook</a></p>
<h2 id="The-k-armed-Bandit-Problem"><a href="#The-k-armed-Bandit-Problem" class="headerlink" title="The k-armed Bandit Problem"></a>The k-armed Bandit Problem</h2><p>Faced repeatedly with a choice among k different options. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. The objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
<p><img src="1equation.png" alt="True value of action"></p>
<p>Value of an action is the expected value of reward given that action is selected.</p>
<ul>
<li>q*(a): TRUE Value of an action (Note known)</li>
<li>Qt(a): estimated value of action a at time step t, We would like Q t (a) to be close to q*(a).</li>
<li>At: Action selected on time step t</li>
<li>Rt: Corresponding Reward on time step t</li>
</ul>
<p>Goal is to maximize argmax of q*(a)</p>
<blockquote>
<p>Each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as A t , and the corresponding reward as R t . The value then of an arbitrary action a, denoted q ⇤ (a), is the expected reward given that a is selected</p>
</blockquote>
<h2 id="Action-Value-amp-increment-estimation"><a href="#Action-Value-amp-increment-estimation" class="headerlink" title="Action-Value &amp; increment estimation"></a>Action-Value &amp; increment estimation</h2><p>Sample average method: Take the average of action-values so far.</p>
<p><img src="2actionvalue.png" alt="Sample avg (predicted) action value"></p>
<!-- ![Action value](3.png) -->

<p>Action Selection:</p>
<ul>
<li>Exploit / Greedy: argmax Qt(a) (choose the current largest estimate action / most reward right now)</li>
<li>Explore: choose other than the largest estimate randomly.</li>
</ul>
<p><mark>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</mark></p>
<p><img src="4.png" alt="Increment (R = reward)"></p>
<p>NewEstimate = OldEstimate + StepSize (Target - OldEstimate)</p>
<p><strong>Nonstationary Problem</strong><br>Performance changes with time. (e.g. a medicine more effective in winter)<br><img src="5.png" alt="Constant step size"></p>
<ul>
<li>alpha is constant, more recent rewards affect more than older rewards.</li>
<li>weight decays exponentially according to the exponent on 1- alpha.</li>
</ul>
<h2 id="Epsilon-Greedy-amp-optimistic-initial-value"><a href="#Epsilon-Greedy-amp-optimistic-initial-value" class="headerlink" title="Epsilon Greedy &amp; optimistic initial value"></a>Epsilon Greedy &amp; optimistic initial value</h2><p>Epsilon = prob of choosing to explore.<br>The smaller, the longer it takes for curve to plateu.<br><img src="7.png" alt="Epsilon"></p>
<p><img src="6.png" alt="Optimistic int val"></p>
<ul>
<li><strong>Worse at first due to more exploration, exploration decreases with time (not goot for nonstationary problems).</strong></li>
<li>The system does a fair amount of exploration even if greedy actions are selected all the time.</li>
</ul>
<h2 id="Upper-Conﬁdence-Bound-Action-Selection"><a href="#Upper-Conﬁdence-Bound-Action-Selection" class="headerlink" title="Upper-Conﬁdence-Bound Action Selection"></a>Upper-Conﬁdence-Bound Action Selection</h2><p>Action selection method that chooses the most “unexplored” / “uncertain” actions when not choosing to exploit.</p>
<p><img src="8.png" alt="UCB"></p>
<blockquote>
<p>N t(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c &gt; 0 controls the degree of exploration. If N t (a) = 0, then a is considered to be a maximizing action.</p>
</blockquote>
<blockquote>
<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the conﬁdence level. <strong><em>Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases.</em></strong> On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>
</blockquote>
<p><a href="https://d18ky98rnyall9.cloudfront.net/k5ZG8p3IEemEawpeY3OQmg.processed/full/360p/index.webm?Expires=1592265600&Signature=IxfR~bsRqAL3g-K2cRDS48LcMiaK~Q8HYfeZGaOQwNKQ80qJ-zuTQUhca75nHvfR58VNOGjqStnexBRUTejaN1FGIMPgRsx2N7WI8E3PIc65k8d-PbY0PThBCHGgv8uIlzP8dQAVcSRbFwicygHYHTRYFsVVEbgijR5NvgPNF0Q_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" target="_blank" rel="noopener">Week 1 Summary</a></p>
</div><div class="tags"><a href="/blog/tags/%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>笔记</a></div><div class="post-nav"><a class="pre" href="/blog/2020/06/17/workschedule/">Work &amp; Life &amp; Summer</a><a class="next" href="/blog/2020/05/27/%E6%9D%A8%E7%BB%9B%E8%AF%AD%E5%BD%95/">杨绛语录</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/blog/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">笔记</a> <a href="/blog/tags/%E5%BF%83%E6%83%85/" style="font-size: 15px;">心情</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/blog/2020/06/17/workschedule/">Work & Life & Summer</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2020/06/13/reinforcement/">Reinforcement Learning Notes</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2020/05/27/%E6%9D%A8%E7%BB%9B%E8%AF%AD%E5%BD%95/">杨绛语录</a></li><li class="post-list-item"><a class="post-list-link" href="/blog/2020/05/23/%E5%A4%A7%E5%AD%A6%E5%B9%B4%E4%B8%89/">大学年三</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/blog/." rel="nofollow">(▀̿Ĺ̯▀̿ ̿).</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/blog/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/blog/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/blog/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/blog/js/smartresize.js?v=0.0.0"></script></div></body></html>